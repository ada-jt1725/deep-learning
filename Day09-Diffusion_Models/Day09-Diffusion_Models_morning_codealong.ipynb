{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YehS8enAmDn"
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1cXtXdAcwedVDbapmz1pj_hULsQrhEcff\" width=\"500\"/>\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__g36CjU8CUi"
   },
   "source": [
    "# **Diffusion models**\n",
    "\n",
    "#### **Morning contents/agenda**\n",
    "\n",
    "1. *Why* use diffusion models?\n",
    "\n",
    "2. *What* are diffusion models?\n",
    "\n",
    "3. Score-based models\n",
    "\n",
    "#### **Learning outcomes**\n",
    "\n",
    "1. Understand how Diffusion models differ from VAEs and GANs\n",
    "\n",
    "2. Understand how the forward and reverse diffusion processes work\n",
    "\n",
    "3. Understand how we can sample from Diffusion models and the importance of noise in their training\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **Afternoon contents/agenda**\n",
    "\n",
    "1. Implement cosine variance schedule\n",
    "\n",
    "2. Improve the U-Net architecture\n",
    "\n",
    "#### **Learning outcomes**\n",
    "\n",
    "1. Understand the impact of the variance schedule on Diffusion models\n",
    "\n",
    "2. Understand some of the design choices that make Diffusion models feasible in practice\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5CluZ9CZtgV"
   },
   "source": [
    "## 1. *Why* use diffusion models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ilh7RBFnnvm"
   },
   "source": [
    "### **Concept and motivation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJAt134jaL7L"
   },
   "source": [
    "\n",
    "Many generative modelling techniques, such as [variational autoencoders](https://arxiv.org/abs/1312.6114) (VAEs) and [generative adversarial networks](https://arxiv.org/abs/1406.2661) (GANs), attempt to go directly from a random latent-space sample to a generated output in one forward pass of a network. This is a *very* difficult thing to do for an artificial neural network. It is also a *very* difficult and unnatural thing for a biological neural network to do. For instance, when an author writes a novel, or an artist creates a painting, they do so by gradually refining it over many iterations. Rarely will an author or artist produce their final creation with one train of thought! This observation is the core inspiration behind diffusion models.\n",
    "\n",
    "Many of the important limitations of VAEs and GANs can be considered a consequence of the aforementioned difficulty in generating data samples with a single network evaluation. VAEs optimise a surrogate loss that is only an *approximation* of maximum likelihood training. This has practical negative consequences on the quality of the generated outputs. Meanwhile, the adversarial training scheme used to optimise GANs is [notoriously unstable](https://arxiv.org/abs/1606.03498). It is also often unfaithful to the true data distribution ([mode collapse](https://arxiv.org/abs/1611.02163)). In truth, the generator doesn't (directly) care about learning a representative model of the data distribution, so long as it can fool the discriminator.\n",
    "\n",
    "Diffusion models have the potential to circumvent these issues. Diffusion models were first introduced in a [2015 seminal paper](https://proceedings.mlr.press/v37/sohl-dickstein15.html), but they truly became popular in 2019 when [another paper](https://proceedings.neurips.cc/paper_files/paper/2019/hash/3001ef257407d5a371a96dcd947c7d93-Abstract.html) showed that these models could rival GANs and VAEs. They benefit from a stable training scheme, and can generate very high quality samples that are competitive with (and now often surpass) those produced using GANs. This is not to say diffusion models are a perfect silver bullet solution, and they certainly have important limitations of their own.\n",
    "\n",
    "<br/>\n",
    "\n",
    ">*All models are wrong, but some are useful* - George E. P. Box.\n",
    "\n",
    "<br/>\n",
    "\n",
    "A single pass through a diffusion model will likely produce samples that are *‚Äúwrong‚Äù* and typically much worse than that produced by a VAE or GAN. However, diffusion models are designed to be *‚Äúuseful‚Äù* over many hundreds or thousands of iterations (and network evaluations). A negative consequence of this is that using diffusion models for inference is considerably more computationally expensive than a VAE or GAN, which only require a single pass through the network.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://drive.google.com/uc?id=1t1tgNGszgsaDH076GfF2MC-gS8JW43pD\" width=\"1000\"/>\n",
    "</center>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5kMlxhPmQ9Q"
   },
   "source": [
    "### **The results speak for themselves**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23Gs6zlwna6l"
   },
   "source": [
    "**Imagen**\n",
    "\n",
    "Developed by Google Research, Brain Team. See paper [here](https://arxiv.org/abs/2205.11487)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1760967942139,
     "user": {
      "displayName": "Carlos Cueto Mond√©jar",
      "userId": "10605816413378977833"
     },
     "user_tz": -60
    },
    "id": "PoyVhYIcmYQt",
    "outputId": "5c57d961-c0ce-4fb1-99c7-fca4228cfb91"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"600\"\n",
       "            src=\"https://imagen.research.google/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7de5a985b650>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "display.IFrame('https://imagen.research.google/', width=1000, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgjB_77Lue9l"
   },
   "source": [
    "**Stable diffusion**\n",
    "\n",
    "Open sourced model and codes [here](https://github.com/CompVis/stable-diffusion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1760967942172,
     "user": {
      "displayName": "Carlos Cueto Mond√©jar",
      "userId": "10605816413378977833"
     },
     "user_tz": -60
    },
    "id": "_nU7rCdiuisf",
    "outputId": "40d0ecf3-8a33-432a-dc39-45a2f181b6db"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"600\"\n",
       "            src=\"https://stablediffusionweb.com/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7de5a98b40b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display.IFrame('https://stablediffusionweb.com/', width=1000, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OQGGPpg3C_a"
   },
   "source": [
    "**Video Diffusion Models**\n",
    "\n",
    "Developed by Ho et al. 2022. See paper [here](https://arxiv.org/abs/2204.03458)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1760967942196,
     "user": {
      "displayName": "Carlos Cueto Mond√©jar",
      "userId": "10605816413378977833"
     },
     "user_tz": -60
    },
    "id": "P5ZA0sOr22Wb",
    "outputId": "9fa04c89-7590-41be-84c6-91d9791fc73e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"600\"\n",
       "            src=\"https://video-diffusion.github.io/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7de5a98b42f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display.IFrame('https://video-diffusion.github.io/', width=1000, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Open AI's Sora**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.IFrame(\"https://openai.com/sora/\", width=1000, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAUdRrgMDOAP"
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Dx6KzNryPDI"
   },
   "source": [
    "## 2. *What* are diffusion models?\n",
    "\n",
    ">*Creating noise from data is easy; creating data from noise is generative modeling* -Song et al., 2020\n",
    "\n",
    "Diffusion models are inspired by non-equilibrium thermodynamics. They define a chain of diffusion steps that slowly add random noise to the data and then learn to reverse the diffusion process in order to generate data samples from noise.\n",
    "\n",
    "Diffusion models are broadly divided into two complementary steps: forward and reverse diffusion.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://drive.google.com/thumbnail?id=1hYXh0UtXi5Ovv6QjYY97EMksU6LZkvr8&sz=w1500\" width=\"800\"/>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "**Training a Diffusion Model**:\n",
    "- Forward process: we corrupt our data in a controlled environment (i.e. we know how much noise has been added to the data at each point) - no training is involved\n",
    "\n",
    "- Training: we train our model to predict the noise contained in a corrupted image at different corruption levels in $[0, T]$\n",
    "\n",
    "- Reverse process (or sampling): we use our trained network to generate images by **iteratively denoising from pure noise**\n",
    "\n",
    "<br>\n",
    "\n",
    "Easy, right? Let's have a look at one step at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcKbIn6BkSl3"
   },
   "source": [
    "\n",
    "### **Forward diffusion**\n",
    "\n",
    "**NOTE:** *There are a number of different ways to implement Diffusion models, so for the next section we will be using the notation introduced in the [de-noising diffusion probabilistic models (DDPM)](https://arxiv.org/pdf/2006.11239). This is a good baseline from which to understand other types of implementations.*\n",
    "\n",
    "<br/>\n",
    "\n",
    "The forward diffusion process begins with a data sample $x(0)$ from our dataset (for example, an image of a dog) and progressively adds noise to it in a step-wise fashion until the sample, $x(T)$, is completely corrupted by noise at step T.\n",
    "\n",
    "The noise that we add to the data samples is generally sampled from a Gaussian distribution, with a variance schedule that increases progressively from $t = 0$ to $t = T$. The variance schedule is generally designated by $\\{ \\beta_t \\in (0,1) \\}_{t=1}^T$.\n",
    "\n",
    "Given an initial data sample $x(0) = x_0$, we would like to know how to calculate every step in the forward diffusion $x(t) = x_t$ until $x(T) = x_T$. Turns out that, because we are using Gaussian noise, we can use some tricks to arrive at the following expression:\n",
    "\n",
    "$$\n",
    "x_t = \\sqrt{\\bar\\alpha_t} x_0 + \\sqrt{1 - \\bar\\alpha_t} \\epsilon\n",
    "$$\n",
    "\n",
    "Where $\\epsilon \\sim N(0, 1)$ is a random sample from the normal distribution with zero mean and unit variance, $\\alpha_t = 1 - \\beta_t$ and $\\bar\\alpha_t = \\prod_{i=1}^t \\alpha_i$. This expression will let us easily calculate a noise-corrupted data sample at any step $t$ of the diffusion process.\n",
    "\n",
    "Let's start implementing this forward diffusion process using the MNIST dataset.\n",
    "\n",
    "<br/>\n",
    "\n",
    "We can start by importing some packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_kYki018eTFJ"
   },
   "outputs": [],
   "source": [
    "!pip install pycm livelossplot torchinfo -q\n",
    "%pylab inline\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchinfo import summary\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYIknCA5zny4"
   },
   "source": [
    "And setting up the device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LI8sNA9feT3H"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
    "    torch.backends.cudnn.enabled   = False\n",
    "\n",
    "    return True\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
    "    print(\"Cuda installed! Running on GPU!\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"No GPU available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hwu-U9UVztKn"
   },
   "source": [
    "Now, we can load the MNIST dataset and apply some initial processing to the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTcq3KIYXorp"
   },
   "outputs": [],
   "source": [
    "# Define torch transformations\n",
    "x_dim = 32\n",
    "batch_size = 64\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    ##,                             # Convert to a tensor and normalise to range [0,1]\n",
    "    ##,                             # Resize images to 32x32. We will later use this in\n",
    "                                    #     a U-Net architecture, and inputs with powers of 2\n",
    "                                    #     are more suitable.\n",
    "    ##])                            # Normalise to the range [-1,1]\n",
    "\n",
    "# Download and define MNIST as a torch dataset, using the above transformations\n",
    "train_dataset = ##\n",
    "\n",
    "# Define a torch dataloader with a batch size of 64\n",
    "train_loader = ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyJuoWlZ0SEy"
   },
   "source": [
    "We can plot a few samples from the dataset to check that everything is as we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97THri7LzLcW"
   },
   "outputs": [],
   "source": [
    "# Grab the first 16 images\n",
    "x0 = ##\n",
    "\n",
    "# Define a plotting function to visualise the batch as a grid image\n",
    "def plot_batch(x):\n",
    "    \"\"\"plot batch x as grid image\"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(torchvision.utils.make_grid(x, pad_value=0, padding=2).permute(1, 2, 0))\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "    plt.show()\n",
    "\n",
    "#¬†Visualise the batch\n",
    "plot_batch(x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONS0U5uJzLcX"
   },
   "source": [
    "As we have mentioned, the forward diffusion process is defined by the repeated addition of Gaussian noise to a data sample, $x_0$, according to a pre-defined variance schedule $\\beta_{1},\\dots ,\\beta_{T}$.\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's define these variables using the hyperparameters from the original DDPM paper\n",
    "- $T$ number of timesteps: ``1000``\n",
    "- $\\beta_0$ = ``1e-4``\n",
    "- $\\beta_T$ = ``0.02``\n",
    "- Linear variance schedule\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmiyltjKzLcX"
   },
   "outputs": [],
   "source": [
    "# Number of variance timesteps\n",
    "T = ##\n",
    "\n",
    "# Define a linear variance schedule\n",
    "beta_start = ##\n",
    "beta_end = ##\n",
    "betas = ## # These are the noise variances added to the sample for every t in [0, T]\n",
    "\n",
    "betas.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXLk6PTX0xMn"
   },
   "source": [
    "From this schedule, we can calculate other relevant quantities $\\alpha_t$ and $\\bar\\alpha_t$ so we can implement the forward equation defined by:\n",
    "\n",
    "$$\n",
    "x_t = \\sqrt{\\bar\\alpha_t} x_0 + \\sqrt{1 - \\bar\\alpha_t} \\epsilon\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$\\alpha_t = 1 - \\beta_t$,\n",
    "\n",
    "\n",
    "$\\bar\\alpha_t = \\prod_{i=1}^t \\alpha_i$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H96xb9RYzLcX"
   },
   "outputs": [],
   "source": [
    "# Pre-calculate other helpful terms that derive from the variance schedule\n",
    "alphas = ##\n",
    "alphas_bar = ##\n",
    "\n",
    "alphas.shape, alphas_bar.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Tb5d_m2bUCa"
   },
   "source": [
    "Now we can corrupt the data with noise, given the noise schedule, following the forward diffusion process. Let's have a look for a single sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TAp5K1OKbTjL"
   },
   "outputs": [],
   "source": [
    "# Grab a single sample for testing\n",
    "x = ##\n",
    "\n",
    "# What does the image look like when corrupted at timestep 't=90' for a noise vector 'e'?\n",
    "t = ##\n",
    "e = ##\n",
    "\n",
    "# Forward process\n",
    "xt = ##\n",
    "\n",
    "# Plots\n",
    "plt.imshow(##, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(##, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmNPnsgPaTXT"
   },
   "source": [
    "What if we want to implement these for an entire batch, each with a different timestep schedule?\n",
    "\n",
    "Let's write a helper function that retrieves from any scheduling array (betas, alphas, alphas_bar, etc..) the values corresponding to a batch of timesteps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9_INfZpaTAY"
   },
   "outputs": [],
   "source": [
    "# Define a helper function that samples any scheduling array (betas, alphas, alphas_bar, etc..)\n",
    "# for a batch of timesteps t, and expands its dimensionality to that of the data\n",
    "def fix_batch(sched, t, device):\n",
    "    \"\"\"extract scheds for a batch of timesteps t, and expand to batch dimensionality\"\"\"\n",
    "    return sched.to(device)[t.to(device)].view(-1, 1, 1, 1)\n",
    "\n",
    "# Each sample from the batch will have a different level of noise perturbation so that\n",
    "# the network does not memorise any individual timestep\n",
    "\n",
    "# Let's check and understand these dimensions:\n",
    "t = ##\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ro6TppaCzLcX"
   },
   "source": [
    "Let's now implement the forward diffusion process, where we take a data sample and progressively corrupt it by adding a sequence of Gaussian noise controlled by the variance schedule:\n",
    "\n",
    "$$\n",
    "x_t = \\sqrt{\\bar\\alpha_t} x_0 + \\sqrt{1 - \\bar\\alpha_t} \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0yK_ph68zLcX"
   },
   "outputs": [],
   "source": [
    "# let's define a function that implements the forward diffusion process\n",
    "def forward_diffusion(x0, t, e):\n",
    "    \"\"\"run the forward diffusion process on x up to time t using noise e\"\"\"\n",
    "    sqrt_alphas_bar = fix_batch(##, t, x0.device)\n",
    "    sqrt_alphas_bar_minus_one = fix_batch(##, t, x0.device)\n",
    "\n",
    "    return ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8vHLM4-zLcX"
   },
   "source": [
    "Let's test the forward diffusion process out on our current batch $x_{0}$.\n",
    "\n",
    "**This will be one of the inputs to our network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QyQUeKrvdz_C"
   },
   "outputs": [],
   "source": [
    "x0 = ##\n",
    "\n",
    "# Sample 16 random values for t\n",
    "t = ##\n",
    "\n",
    "# Draw a random sample from a normal distribution with unit variance (noise)\n",
    "e = ##\n",
    "\n",
    "# Calculate the forward diffusion process for the batch, each corrupted in a level\n",
    "# defined by t\n",
    "x_t = ##\n",
    "\n",
    "# Plot\n",
    "print(t)\n",
    "plot_batch(x_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plyzwwlxeQbt"
   },
   "source": [
    "To better understand the forward degradation, we can also check how **one single** image is degraded for a progressively increasing $t$.\n",
    "\n",
    "**Note that this is for visualisation only, we will not input a single image or an even timestep degradaton into the network at once to avoid it learning any unwanted patterns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grBWZEO4zLcX"
   },
   "outputs": [],
   "source": [
    "# Sample 16 values for t that are EVENLY SPACED from t=0 to t=T\n",
    "t = ##\n",
    "\n",
    "# draw a random sample from a normal distribution with unit variance\n",
    "e = ##\n",
    "\n",
    "# calculate the forward diffusion process for a single image from batch x0\n",
    "x_t = ##\n",
    "\n",
    "# Plot\n",
    "print(t)\n",
    "plot_batch(x_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqmSyYpmfJhG"
   },
   "source": [
    "Another useful visualisation is the histogram of each of these images at the different timesteps, showing that the forward process slowly degradates the data towards pure Gaussian noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebcx-VkgzLcX"
   },
   "outputs": [],
   "source": [
    "# Display histograms of the pixel values throughout the forward diffusion\n",
    "plt.figure(figsize=(15, 4))\n",
    "for i in range(len(t)):\n",
    "    plt.subplot(2, len(t)//2, 1+i)\n",
    "    plt.hist(x_t[i].flatten(), bins=np.arange(x_t[-1].min(), x_t[-1].max() + 0.5, 0.5), edgecolor='black', linewidth=1.2)\n",
    "    plt.xlim(x_t[-1].min(), x_t[-1].max())\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "    plt.title(f'$t={t[i]}$')\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCFcYlURyt9N"
   },
   "source": [
    "<br><br><br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://drive.google.com/thumbnail?id=1hYXh0UtXi5Ovv6QjYY97EMksU6LZkvr8&sz=w1500\" width=\"800\"/>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Training the Diffusion Model**\n",
    "\n",
    "The reverse diffusion process involves estimating the specific noise patterns that have been introduced at each step and progressively denoising the data from $x(T)$ back to $x(0)$.\n",
    "\n",
    "The goal of our trained network is to learn how to achieve this denoising process. But, how do we do this? Once again, there are multiple ways of doing this depending on the specific approach used. For now, let's start by considering the [DDPM approach](https://arxiv.org/pdf/2006.11239).\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "We know that the forward diffusion process using Gaussian noise follows this formula:\n",
    "\n",
    "$$\n",
    "x_t = \\sqrt{\\bar\\alpha_t} x_0 + \\sqrt{1 - \\bar\\alpha_t} \\epsilon\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Where $\\epsilon \\sim N(0, 1)$ is a random sample from the normal distribution with zero mean and unit variance. The DDPM approach then is to design a neural network that approximates $\\epsilon$ at any given step $t$: $\\epsilon_\\theta(x_t, t)$. We can *easily* train this network by using the MSE loss:\n",
    "\n",
    "$$\n",
    "\\theta = argmin_\\theta ||\\epsilon - \\epsilon_\\theta(x_t, t)||^2\n",
    "$$\n",
    "\n",
    "<br/>\n",
    "\n",
    "It turns out that minimsing this function is also equivalent to minimising the ELBO. For those interested, [this](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) post has a great dissection of the DDPM loss function. (NOT ASSESSED)\n",
    "\n",
    "<br/>\n",
    "\n",
    "--> **Inputs to the network are $x_t$ and $t$, the corrupted image and corresponding timestep, respectively**\n",
    "\n",
    "--> **Output of the network is the prediction of $\\epsilon_{\\theta} \\approx \\epsilon$**\n",
    "\n",
    "Let's create a network and train it on our MNIST dataset then!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAkhA0iGE1EN"
   },
   "source": [
    "<br>\n",
    "\n",
    "We will use a modified [U-Net](https://arxiv.org/abs/1505.04597) architecture to approximate $\\epsilon_{\\theta}(x_{t}, t)$.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://drive.google.com/thumbnail?id=1wc5eldR_3yCbjWMwiHQksfiSAtZfhvM5&sz=w1500\" width=\"800\"/>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "A U-Net is composed of the following:\n",
    "- Convolutional blocks that preserve the size of the images\n",
    "\n",
    "- Encoding blocks composed of **a few convolutional blocks + a downsampling layer**\n",
    "\n",
    "- Decoding blocks composed of **an upsampling layer + a few convolutional blocks**, mirroring the encoding blocks\n",
    "\n",
    "- Residual connections (also known as skip connections) that **transfer information from the encoding blocks to the decoding blocks**\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Crucially, the U-Net must be conditioned on time $t$, so that it is able to predict the noise $\\epsilon$ from a forward diffused image $x_{t}$ at any time from $t=0$ to $t=T$. We will do this by using a concept called **sinusoidal position embeddings**. We will see this concept in more detail in the Transformers lecture.\n",
    "\n",
    "For now, we only need to understand these embeddings as an **\"intelligent\" way of representing a single-integer timestep $t$ for the model**. The sinusoidal position embedding takes $t$ as an input, and produces an embedding or vector that represents that particular value of $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5riA04iE1EO"
   },
   "outputs": [],
   "source": [
    "# Define the sinusoidal position embeddings\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    \"\"\"sinusoidal position embedding, https://arxiv.org/abs/1706.03762\"\"\"\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        # Given a batch of timesteps, create a batch of representative vectors\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = np.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-UBI1cmE1EP"
   },
   "source": [
    "This embedding is often passed through a small multi-layered perceptron (or fully connected network), and is then added to each convolutional block throughout the U-Net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VN0Ys_3JE1EP"
   },
   "outputs": [],
   "source": [
    "# Define a convolutional block with time-embedding\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"time-conditioned convolutional block\"\"\"\n",
    "\n",
    "    def __init__(self, in_c, out_c, embed_dim):\n",
    "        super().__init__()\n",
    "        self.conv = ##\n",
    "        self.dense = ## # reshapes the time embedding length to the number of channels\n",
    "        self.bn = ##\n",
    "        self.act = ##\n",
    "\n",
    "    def forward(self, x, t_embed):\n",
    "        x = ##\n",
    "        x += ## # add the output of the time embedding dense layer\n",
    "        x = ##\n",
    "        x = ##\n",
    "        return x\n",
    "\n",
    "# define an encoder block of the U-Net with time-embedding\n",
    "class EncBlock(nn.Module):\n",
    "    \"\"\"time-conditioned U-Net encoder block\"\"\"\n",
    "\n",
    "    def __init__(self, in_c, out_c, embed_dim):\n",
    "        super().__init__()\n",
    "        self.conv_block1 = ##\n",
    "        self.conv_block2 = ##\n",
    "        self.pool = ##\n",
    "\n",
    "    def forward(self, x, t_embed):\n",
    "        h = ##\n",
    "        h = ##\n",
    "        p = ##\n",
    "        return h, p\n",
    "\n",
    "# define an decoder block of the U-Net with time-embedding\n",
    "class DecBlock(nn.Module):\n",
    "    \"\"\"time-conditioned U-Net decoder block\"\"\"\n",
    "\n",
    "    def __init__(self, in_c, out_c, embed_dim):\n",
    "        super().__init__()\n",
    "        self.up = ##\n",
    "        self.conv_block1 = ##\n",
    "        self.conv_block2 = ##\n",
    "\n",
    "    def forward(self, x, s, t_embed):\n",
    "        h = ##\n",
    "        h = ## # concatenate x with U-Net skip connection from encoder\n",
    "        h = ##\n",
    "        h = ##\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgjVbBsjE1EP"
   },
   "source": [
    "Putting it all together, we can now define our time-conditioned U-Net!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZdVzGr11E1EP"
   },
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    \"\"\"DDPM U-Net, https://arxiv.org/abs/2006.11239 and https://arxiv.org/abs/1505.04597\"\"\"\n",
    "\n",
    "    def __init__(self, n_channels, embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # time positional embedding MLP\n",
    "        self.embed = nn.Sequential(SinusoidalPositionEmbeddings(embed_dim),\n",
    "                                   nn.Linear(embed_dim, embed_dim),\n",
    "                                   nn.GELU(),\n",
    "                                   nn.Linear(embed_dim, embed_dim))\n",
    "\n",
    "        # encoder\n",
    "        self.e1 = EncBlock(n_channels, 64, embed_dim)\n",
    "        self.e2 = EncBlock(64, 128, embed_dim)\n",
    "        self.e3 = EncBlock(128, 256, embed_dim)\n",
    "        self.e4 = EncBlock(256, 512, embed_dim)\n",
    "\n",
    "        # bottleneck\n",
    "        self.b1 = ConvBlock(512, 1024, embed_dim)\n",
    "        self.b2 = ConvBlock(1024, 1024, embed_dim)\n",
    "\n",
    "        # decoder\n",
    "        self.d1 = DecBlock(1024, 512, embed_dim)\n",
    "        self.d2 = DecBlock(512, 256, embed_dim)\n",
    "        self.d3 = DecBlock(256, 128, embed_dim)\n",
    "        self.d4 = DecBlock(128, 64, embed_dim)\n",
    "\n",
    "        #¬†output layer\n",
    "        self.output = ##\n",
    "\n",
    "    def forward(self, x, t):\n",
    "\n",
    "        t_embed = ##\n",
    "\n",
    "        # encoder\n",
    "        s1, x = ##\n",
    "        s2, x = ##\n",
    "        s3, x = ##\n",
    "        s4, x = ##\n",
    "\n",
    "        # bottleneck\n",
    "        x = ##\n",
    "        x = ##\n",
    "\n",
    "        # decoder\n",
    "        x = ##\n",
    "        x = ##\n",
    "        x = ##\n",
    "        x = ##\n",
    "\n",
    "        # output\n",
    "        output = ##\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUwk7c_RE1EQ"
   },
   "source": [
    "To train the network, we will use the following procedure:\n",
    "\n",
    "1. Randomly select a sample from the training data $x_{0}$\n",
    "\n",
    "2. Randomly sample $t$ from a uniform distribution over the range $[1,T]$\n",
    "\n",
    "3. Randomly sample $\\epsilon$ from a zero-mean normal distribution with a variance of 1\n",
    "\n",
    "4. Apply the forward diffusion process on $x_{0}$ using $\\epsilon$ and $t$\n",
    "\n",
    "5. Predict the noise by evaluating $\\epsilon_{\\theta}(x_{t}, t)$\n",
    "\n",
    "6. Calculate the MSE loss between the true noise $\\epsilon$ and the predicted noise $\\epsilon_{\\theta}(x_{t}, t)$\n",
    "\n",
    "7. Take a gradient descent (or Adam) step with respect to the model parameters $\\theta$\n",
    "\n",
    "8. Repeat steps 1 to 7 until converged\n",
    "\n",
    "These steps are performed batch-wise in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sO7NC3sGE1EQ"
   },
   "outputs": [],
   "source": [
    "# define the model and send to the GPU\n",
    "model = ##\n",
    "\n",
    "# specify the Adam optimizer with a learning rate of 2e-4\n",
    "optimiser = ##\n",
    "\n",
    "# set the criterion to be the mse loss\n",
    "criterion = ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-l_qgtsE1EQ"
   },
   "outputs": [],
   "source": [
    "# Ensure model is in training mode\n",
    "model.train()\n",
    "\n",
    "# Set number of epochs and loop over them\n",
    "n_epoch = 5\n",
    "\n",
    "# Set up liveloss plot\n",
    "liveloss = PlotLosses()\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "\n",
    "    # Loop over each batch using tqdm to report progress and loss stats\n",
    "    # Note that here we're denoting the uncorrupted batch as x0 and that\n",
    "    # we once again do not care for their labels\n",
    "    pbar = tqdm(train_loader)\n",
    "    tot_loss = 0\n",
    "    logs = {}\n",
    "    for x0, _ in pbar:\n",
    "\n",
    "        # Zero the gradients\n",
    "        ##\n",
    "\n",
    "        # Send the batch to the GPU\n",
    "        x0 = ##\n",
    "\n",
    "        # Sample t from a uniform distribution\n",
    "        t = ##\n",
    "\n",
    "        # Sample e from a normal distribution\n",
    "        e = ##\n",
    "\n",
    "        # Run the forward diffusion process to add noise to x0\n",
    "        # The corrupted batch (at various levels of noise) is denoted as xt\n",
    "        xt = ##\n",
    "\n",
    "        # Calculate the loss between the predicted noise and the true noise\n",
    "        loss = ##\n",
    "\n",
    "        # Backpropagation to obtain gradients w.r.t model parameters\n",
    "        ##\n",
    "\n",
    "        # Report current loss using tqdm\n",
    "        tot_loss += loss.item()\n",
    "        pbar.set_description(f\"loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Take an optimisation step\n",
    "        ##\n",
    "\n",
    "    logs['loss'] = tot_loss / len(train_loader)\n",
    "    liveloss.update(logs)\n",
    "    liveloss.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZaEedCoE1EQ"
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), 'ddpm.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQcXTRMgE1ER"
   },
   "outputs": [],
   "source": [
    "# load the model\n",
    "model.load_state_dict(torch.load('ddpm.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeCCesdAEa9x"
   },
   "source": [
    "### Reverse process: Sampling and Langevin dynamics\n",
    "\n",
    "Diffusion models are generative models, so the ultimate goal is to learn the underlying probability distribution of our data so that we can generate new samples from it.\n",
    "\n",
    "If everything has gone well, once our network is trained, we can sample from the learned probability distribution by running the reverse diffusion process.\n",
    "\n",
    "In theory, once **$\\mathbf{\\epsilon}$** is known, we could use the forward diffusion process to recover **$x_0$**:\n",
    "\n",
    "$$\n",
    "x_t = \\sqrt{\\bar\\alpha_t} x_0 + \\sqrt{1 - \\bar\\alpha_t} \\epsilon\n",
    "$$\n",
    "\n",
    "But as it turns out, direct sampling of this form is **unstable** (remember GANs?)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Instead, we use the concept of **Langevin dynamics**.\n",
    "\n",
    "Langevin dynamics is a concept from physics, developed for statistically modeling molecular systems. Combined with stochastic gradient descent, stochastic gradient Langevin dynamics can **iteratively** produce samples from a probability density $p(x)$ using gradients of the probability density $\\nabla_x \\log p(x)$:\n",
    "\n",
    "$$\n",
    "x_t = x_{t-1} + \\frac{\\delta}{2} \\nabla_x \\log p(x_{t-1}) + \\sqrt{\\delta} z\n",
    "$$\n",
    "\n",
    "\n",
    "Where $z \\sim N(0, 1)$, $\\delta$ is the step size and as the process converges $t \\rightarrow \\infty, \\epsilon \\rightarrow 0$.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Can you find the resemblance to stochastic gradient descent?**\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://yang-song.net/assets/img/score/langevin.gif\" alt=\"network\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "Given that noise is Gaussian, it is possible to modify this expression to fit the DDPM approach. The result is the following denoising **reverse process**:\n",
    "\n",
    "$$\n",
    "x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar\\alpha_t}} \\epsilon_\\theta(x_t,t) \\right) + \\sigma_t z\n",
    "$$\n",
    "\n",
    "Where $z \\sim N(0, 1)$ and $\\sigma_t^2 = \\beta_t$. If interested, you can find the derivation [here](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/).\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Why is it relevant to add Gaussian noise to the sampling process?**\n",
    "\n",
    "<br/>\n",
    "\n",
    "With this in mind, let's implement the reverse diffusion process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1WZ2lUm7JRpt"
   },
   "outputs": [],
   "source": [
    "# Let's define a function that implements the reverse diffusion process\n",
    "def reverse_diffusion(x, t, e, z):\n",
    "    \"\"\"run the reverse diffusion process\"\"\"\n",
    "    sigma = fix_batch(##, t, x.device)\n",
    "\n",
    "    sqrt_recip_alphas = fix_batch(##, t, x.device)\n",
    "\n",
    "    scale = ##\n",
    "\n",
    "    return ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deEpiqWrIELK"
   },
   "source": [
    "The sampling process then will be:\n",
    "\n",
    "1. Randomly sample $x_{T}$ from a zero-mean normal distribution with a variance of 1\n",
    "\n",
    "2. If $t>0$, randomly sample $z$ from a zero-mean normal distribution with a variance of 1, otherwise set $z=0$\n",
    "\n",
    "3. Calculate the updated sample $x_{t-1}$ by subtracting the predicted noise $\\epsilon_{\\theta}(x_{t}, t)$ from $x_{t}$\n",
    "\n",
    "4. Apply $z$, scaled by $\\sigma_{t}$, to the sample\n",
    "\n",
    "5. Repeat steps 2 to 4 for $t=T,\\dots,1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8zSLqDDeIELL"
   },
   "outputs": [],
   "source": [
    "# ensure model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Randomly sample an xT batch from a normal distribution\n",
    "# We will simply refer to xT as x\n",
    "x = ##\n",
    "\n",
    "# send to the GPU and only consider first 16 of the batch to ease the computational burden\n",
    "x = x.to(device)[:16]\n",
    "\n",
    "# Container to store sample throughout the reverse diffusion\n",
    "diffusion = torch.empty((T, x.shape[0], x.shape[1], x.shape[2], x.shape[3])).to(x.device)\n",
    "\n",
    "# Context management to ensure gradients are not tracked for any torch tensors or parameters\n",
    "# since we are only interested in sampling, not training\n",
    "##\n",
    "\n",
    "    # Loop over time from T to 0\n",
    "    ##\n",
    "\n",
    "        # sample z from a normal distribution if condition met\n",
    "        if t > 0:\n",
    "            z = torch.randn_like(x)\n",
    "        else:\n",
    "            z = 0\n",
    "\n",
    "        # visualize samples x every 100 iterations\n",
    "        if t % 50 == 0:\n",
    "            print(f'samples at t={t}')\n",
    "            plot_batch(x.cpu())\n",
    "\n",
    "        # convert t to a tensor, send to the GPU, and expand its first dimension to be equal to batch size\n",
    "        t = ##\n",
    "\n",
    "        # denoise x\n",
    "        x = ##\n",
    "\n",
    "        # clipping to ensure the sample values remain in the range -1 to 1\n",
    "        x = torch.clamp(x, -1, 1)\n",
    "\n",
    "        # store the current samples x\n",
    "        diffusion[t] = ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZYpGCJGvIELL"
   },
   "outputs": [],
   "source": [
    "# visualize the diffusion process for a single sample\n",
    "plot_batch(diffusion[::66,0].flip(0).cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8q0m2EYpIELL"
   },
   "source": [
    "Congratulations, you have implemented and trained a diffusion model!\n",
    "\n",
    "- Sharper than VAEs\n",
    "- More stable than GANs\n",
    "- Downsides? Larger architectural demands + Sampling is no longer one-shot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvLIjTElH2V2"
   },
   "source": [
    "### **Why add noise?**\n",
    "\n",
    "A reasonable question at this point is: why are we interested in adding noise to our data samples as part of the diffusion process?\n",
    "\n",
    "Let's say we have the following data samples in our dataset, which we have used to build our network:\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://drive.google.com/thumbnail?id=1QdzctWC2-KCEp3Bc1Ti_12-0SvjvjzGj&sz=w1500\" width=\"800\"/>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "The issue here is that in the regions with low density of samples, our network will have an inaccurate model of the probability density function. What happens then if our initial sample for the denoissing process falls in that low-density area?\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://yang-song.net/assets/img/score/pitfalls.jpg\" alt=\"network\" width=\"800\"/>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "Another important issue is that high-dimensional data actually resides on low-dimensional latent manifolds that are embedded in high-dimensional space (see the [manifold hypothesis](https://en.wikipedia.org/wiki/Manifold_hypothesis)). If the data is restricted to a low-dimensional manifold, the gradients of the log-data distribution in the high-dimensional data-space will be undefined.\n",
    "\n",
    "The solution is to **perturb** the sampled data points with noise. By doing that, we can effectively populate the areas where the probability density function is small:\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://yang-song.net/assets/img/score/multi_scale.jpg\" alt=\"network\" width=\"800\"/>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Or maybe not?**\n",
    "\n",
    "Research on diffusion modelling is moving fast. Not long ago, [this](https://arxiv.org/pdf/2010.02502) paper reformulated the DDPM framework into a **deterministic mapping**, showing that such approach (**DDIM**) may improve performance of diffusion model sampling by ~10x:\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://drive.google.com/thumbnail?id=1IBsICmE9CadAD9m9PehAgKzmzeDGkvTe&sz=w1500\" alt=\"network\" width=\"800\"/>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "where $\\eta$ are varied levels of stochasticity, and $\\hat{\\sigma}$ is the original DDPM implementation, all sampled for $\\tau=10$ timesteps.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Also, [this](https://arxiv.org/abs/2208.09392) paper demonstrated that *any* image transformation that \"degrades\" the  quality of a sample can be used to build a diffusion model. This includes transformations that **do not contain any randomness whatsoever**.\n",
    "\n",
    "This finding represents a significant challenge to our current theoretical understanding of diffusion models, e.g. the theoretical justifications behind DDPMs and SBGMs! According to the semi-official PyTorch DDPM [implementation](https://github.com/lucidrains/denoising-diffusion-pytorch), it \"turns out none of the technicalities really matters at all\" üíÄ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 824
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1760968422418,
     "user": {
      "displayName": "Carlos Cueto Mond√©jar",
      "userId": "10605816413378977833"
     },
     "user_tz": -60
    },
    "id": "W6z6EFK9t5VZ",
    "outputId": "75d6f4d8-fa3e-4ddd-94f4-78e49576c74d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://tuananhbui89.github.io/blog/2024/paper-cold-diffusion/\" width=\"1000\" height=\"800\">\n",
       "</iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://tuananhbui89.github.io/blog/2024/paper-cold-diffusion/\" width=\"1000\" height=\"800\">\n",
    "</iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVCoA3Jpt6SG"
   },
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "### **Why use a U-Net?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 824
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1760968422424,
     "user": {
      "displayName": "Carlos Cueto Mond√©jar",
      "userId": "10605816413378977833"
     },
     "user_tz": -60
    },
    "id": "QOsJ22Yaf6g1",
    "outputId": "962b0020-beb7-4ad8-dcd7-a4a0610aeb2a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://benanne.github.io/2022/01/31/diffusion.html\" width=\"1000\" height=\"800\">\n",
       "</iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://benanne.github.io/2022/01/31/diffusion.html\" width=\"1000\" height=\"800\">\n",
    "</iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9M61ZQSfcun"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## 3. Score-based models\n",
    "\n",
    "So far, we have been looking at the DDPM approach to Diffusion models. There is another family of Diffusion models called score-based models (SGM) that take a slightly different approach.\n",
    "\n",
    "Instead of approximating $\\epsilon$, SGM attempts to create a neural network $s_\\theta(x)$ that will directly learn how to calculate $s_\\theta(x) \\approx \\nabla_x \\log p(x)$ that appears in the Langevin dynamics equation:\n",
    "\n",
    "$$\n",
    "x_t = x_{t-1} + \\frac{\\delta}{2} \\nabla_x \\log p(x_{t-1}) + \\sqrt{\\delta} z\n",
    "$$\n",
    "\n",
    "For further information on the specifics of how score-based generative models are trained and used, see Yang Song's excellent [blogpost](https://yang-song.net/blog/2021/score/) and [this](https://arxiv.org/abs/2101.03288) tutorial.\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
