{"cells":[{"cell_type":"markdown","metadata":{"id":"9YehS8enAmDn"},"source":["<img src=\"https://drive.google.com/uc?id=1cXtXdAcwedVDbapmz1pj_hULsQrhEcff\" width=\"500\"/>\n","\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"__g36CjU8CUi"},"source":["# **Diffusion models**\n","\n","#### **Morning contents/agenda**\n","\n","1. *Why* use diffusion models?\n","\n","2. *What* are diffusion models?\n","\n","3. Score-based models\n","\n","#### **Learning outcomes**\n","\n","1. Understand how Diffusion models differ from VAEs and GANs\n","\n","2. Understand how the forward and reverse diffusion processes work\n","\n","3. Understand how we can sample from Diffusion models and the importance of noise in their training\n","\n","<br>\n","\n","#### **Afternoon contents/agenda**\n","\n","1. Implement cosine variance schedule\n","\n","2. Improve the U-Net architecture\n","\n","#### **Learning outcomes**\n","\n","1. Understand the impact of the variance schedule on Diffusion models\n","\n","2. Understand some of the design choices that make Diffusion models feasible in practice\n","\n","<br/>\n","\n","---\n","\n","<br/>"]},{"cell_type":"markdown","metadata":{"id":"_vAAbLOwPojk"},"source":["## 1. A different variance schedule\n","\n","The variance schedule, generally designated by $\\{ \\beta_t \\in (0,1) \\}_{t=1}^T$, controls how much noise is added to the data in each step of the forward diffusion. In the morning session, we used a fairly simple linear schedule:\n","\n","```\n","betas = torch.linspace(beta_1, beta_T, T)\n","```\n","\n","This afternoon, we are going to try implementing a cosine variance schedule, as suggested by [this](https://arxiv.org/abs/2102.09672) paper.\n","\n","To do this, we will begin with the same implementation for the Diffusion Model that we had this morning. You can copy the implementation from this morning's notebook, but I recommend that you try implementing it from scratch as a good learning excercise.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_kYki018eTFJ"},"outputs":[],"source":["!pip install pycm livelossplot torchinfo\n","%pylab inline\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","from livelossplot import PlotLosses\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","import seaborn as sns\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","from torch.utils.data.dataset import random_split\n","from torchinfo import summary\n","import torchvision\n","from torchvision import transforms"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LI8sNA9feT3H"},"outputs":[],"source":["def set_seed(seed):\n","    \"\"\"\n","    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n","    \"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n","    torch.backends.cudnn.enabled   = False\n","\n","    return True\n","\n","device = 'cpu'\n","if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n","    print(\"Cuda installed! Running on GPU!\")\n","    device = 'cuda'\n","else:\n","    print(\"No GPU available!\")"]},{"cell_type":"markdown","source":["### **1.1. Dataset preparation**\n","\n","Let's start by loading the the MNIST dataset and applying some initial processing to the images."],"metadata":{"id":"Hwu-U9UVztKn"}},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"aOz3dIi2zLcV"},"outputs":[],"source":["# define torch transformations\n","img_dim = 32\n","tf = transforms.Compose([##,  ## convert to a tensor and normalize to [0,1]\n","                         ##,  ## resize images to 32x32 for ease-of-use with the U-Net architecture\n","                         ##])  ## scale to the range [-1,1]\n","\n","# download and define MNIST as a torch dataset, using the above transformations\n","dataset = ##\n","\n","# define a torch dataloader with a batch size of 64\n","dataloader = ##"]},{"cell_type":"markdown","source":["We can plot a few samples from the dataset to check that everything is as we expect."],"metadata":{"id":"CyJuoWlZ0SEy"}},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"97THri7LzLcW"},"outputs":[],"source":["# grab a batch of data and pull out the first 16 images\n","x0 = ##\n","\n","# define a plotting function to visualize the batch as a grid image\n","def plot_batch(x):\n","    \"\"\"plot batch x as grid image\"\"\"\n","\n","    # re-scale and clamp to the range [0,1]\n","    x = torch.clamp((x+1)/2, 0, 1)\n","\n","    plt.figure(figsize=(10, 10))\n","    plt.imshow(torchvision.utils.make_grid(x, pad_value=0, padding=2).permute(1, 2, 0))\n","    plt.xticks([]); plt.yticks([])\n","    plt.show()\n","\n","# visualize the batch\n","##"]},{"cell_type":"markdown","source":["### **1.2. Cosine scheduling**\n","\n","Next, let's define our cosine scheduling.\n","\n","According to [the paper](https://arxiv.org/abs/2102.09672), the cosine schedule is defined by:\n","\n","$$\n","\\bar\\alpha_t = \\frac{f(t)}{f(0)}\n","$$\n","\n","$$\n","f(t) = \\cos\\left( \\frac{t/T + s}{1 + s} \\frac{\\pi}{2} \\right)^2\n","$$\n","\n","$$\n","\\beta_t = 1 - \\frac{\\bar\\alpha_t}{\\bar\\alpha_{t-1}}\n","$$\n","\n","where $s = 0.0081$.\n","\n","We can now implement these calculations using PyTorch operations:"],"metadata":{"id":"NF78Ybfgpvxq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DtA9yzkQR3er","tags":[]},"outputs":[],"source":["# set maximum time value\n","T = ##\n","y = ##\n","\n","s = ##\n","alphas_bar = ##\n","betas = ##"]},{"cell_type":"markdown","metadata":{"id":"rYyffev8wuWg"},"source":["### **1.3. Forward diffusion process**\n","\n","As we did this morning, we can now implement the forward diffusion process using our new variance schedule.\n","\n","Let's start with some utilities for later on:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7hKXPfwdMErB","tags":[]},"outputs":[],"source":["# pre-calculate other helpful terms that derive from the variance schedule\n","alphas = ##\n","alphas_bar = ##\n","\n","# define a function to handle the discrepancy between batch vs schedule shapes\n","def fix_batch(sched, t, device):\n","    \"\"\"extract sched at time t, and expand to batch dimensionality\"\"\"\n","    return sched.to(device)[t.to(device)[:,None,None,None]]"]},{"cell_type":"markdown","metadata":{"id":"ro6TppaCzLcX"},"source":["Let's now implement the forward diffusion process, where we take a data sample and progressively corrupt it by adding a sequence of Gaussian noise controlled by the variance schedule:\n","\n","$$\n","x_t = \\sqrt{\\bar\\alpha_t} x_0 + \\sqrt{1 - \\bar\\alpha_t} \\epsilon\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"0yK_ph68zLcX"},"outputs":[],"source":["# let's define a function that implements the forward diffusion process\n","def forward_diffusion(x0, t, e):\n","    \"\"\"run the forward diffusion process on x up to time t using noise e\"\"\"\n","    ##\n","\n","    return ##"]},{"cell_type":"markdown","metadata":{"id":"S8vHLM4-zLcX"},"source":["Let's test the forward diffusion process out on our current batch $x_{0}$."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"grBWZEO4zLcX"},"outputs":[],"source":["# sample 16 values for t that are evenly spaced from t=0 to t=T\n","t = ##\n","\n","# draw a random sample from a normal distribution with unit variance\n","e = ##\n","\n","# calculate the forward diffusion process for a single image from batch x0\n","x_t = forward_diffusion(x0[0], t, e)"]},{"cell_type":"markdown","source":["And do some plotting of the effect of the schedule on the images and histograms."],"metadata":{"id":"_zxnsG_94k4_"}},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"-6UUiy5JzLcX"},"outputs":[],"source":["# display t values and visualize the forward diffusion process\n","print(t)\n","plot_batch(x_t)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"ebcx-VkgzLcX"},"outputs":[],"source":["# display histograms of the pixel values throughout the forward diffusion\n","plt.figure(figsize=(15, 4))\n","for i in range(len(t)):\n","    plt.subplot(2, len(t)//2, 1+i)\n","    plt.hist(x_t[i].flatten(), bins=np.arange(x_t[-1].min(), x_t[-1].max() + 0.5, 0.5), edgecolor='black', linewidth=1.2)\n","    plt.xlim(x_t[-1].min(), x_t[-1].max())\n","    plt.xticks([]); plt.yticks([])\n","    plt.title(f'$t={t[i]}$')\n","    sns.despine()"]},{"cell_type":"markdown","source":["**Consider:** Do you see any differences with respect to the linear schedule that we used this morning?\n","\n","The forward diffusion using the linear variance schedules arrives at an entirely noise corrupted sample *significantly before* $t=T$. Due to their high noise, these time steps do not contribute much to the quality of the reverse diffusion process. The forward diffusion with the cosine schedule appears to more evenly corrupt the samples during the over $t=0$ to $t=T$."],"metadata":{"id":"jb1oz2J64s4T"}},{"cell_type":"markdown","source":["### **1.4. Building the Diffusion model**\n","\n","We will use the same network definition from this morning.\n","\n","Let's start by defining the position embeddings:"],"metadata":{"id":"55_Aj90WqV-c"}},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"U5riA04iE1EO"},"outputs":[],"source":["# define the sinusoidal position embeddings\n","class SinusoidalPositionEmbeddings(nn.Module):\n","    \"\"\"sinusoidal position embedding, https://arxiv.org/abs/1706.03762\"\"\"\n","\n","    def __init__(self, dim):\n","        super().__init__()\n","        self.dim = dim\n","\n","    def forward(self, time):\n","        device = time.device\n","        half_dim = self.dim // 2\n","        embeddings = np.log(10000) / (half_dim - 1)\n","        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n","        embeddings = time[:, None] * embeddings[None, :]\n","        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n","        return embeddings"]},{"cell_type":"markdown","metadata":{"id":"V-UBI1cmE1EP"},"source":["This embedding is often passed through a small multi-layered perceptron (or fully connected network), and is then added to each convolutional block throughout the U-Net.\n","\n","Let's create a `ConvBlock` that will apply a series of operations:\n","\n","- 2D convolution on input\n","- linear operation on position embedding\n","- batch normalisation\n","- SiLU activation function\n","\n","We will then use this `ConvBlock` to creating an encoding `EncBlock` and a decoding `DecBlock`:\n","\n","- `EncBlock` will apply two `ConvBlock`, followed by a max pooling operation.\n","- `DecBlock` will apply a transposed convolution, followed by two `ConvBlock`. `DecBlock` will also include a skip connection from the encoder."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"VN0Ys_3JE1EP"},"outputs":[],"source":["# define a convolutional block with time-embedding\n","class ConvBlock(nn.Module):\n","    \"\"\"time-conditioned convolutional block\"\"\"\n","\n","    def __init__(self, in_c, out_c, embed_dim):\n","        super().__init__()\n","        self.conv = ##\n","        self.dense = ##  ## reshapes the time embedding length to the number of channels\n","        self.bn = ##\n","        self.act = ##\n","\n","    def forward(self, x, t_embed):\n","        ##\n","        return x\n","\n","# define an encoder block of the U-Net with time-embedding\n","class EncBlock(nn.Module):\n","    \"\"\"time-conditioned U-Net encoder block\"\"\"\n","\n","    def __init__(self, in_c, out_c, embed_dim):\n","        super().__init__()\n","        self.conv_block1 = ##\n","        self.conv_block2 = ##\n","        self.pool = ##\n","\n","    def forward(self, x, t_embed):\n","        ##\n","        return h, p\n","\n","# define an decoder block of the U-Net with time-embedding\n","class DecBlock(nn.Module):\n","    \"\"\"time-conditioned U-Net decoder block\"\"\"\n","\n","    def __init__(self, in_c, out_c, embed_dim):\n","        super().__init__()\n","        self.up = ##\n","        self.conv_block1 = ##\n","        self.conv_block2 = ##\n","\n","    def forward(self, x, s, t_embed):\n","        ##\n","        return h"]},{"cell_type":"markdown","metadata":{"id":"wgjVbBsjE1EP"},"source":["Putting it all together, we can now define our time-conditioned U-Net!"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"ZdVzGr11E1EP"},"outputs":[],"source":["class Unet(nn.Module):\n","    \"\"\"DDPM U-Net, https://arxiv.org/abs/2006.11239 and https://arxiv.org/abs/1505.04597\"\"\"\n","\n","    def __init__(self, n_channels, embed_dim):\n","        super().__init__()\n","\n","        # time positional embedding MLP\n","        self.embed = nn.Sequential(SinusoidalPositionEmbeddings(embed_dim),\n","                                   nn.Linear(embed_dim, embed_dim),\n","                                   nn.GELU(),\n","                                   nn.Linear(embed_dim, embed_dim))\n","\n","        # encoder\n","        self.e1 = EncBlock(n_channels, ##, embed_dim)\n","        self.e2 = EncBlock(##, embed_dim)\n","        self.e3 = EncBlock(##, embed_dim)\n","        self.e4 = EncBlock(##, embed_dim)\n","\n","        # bottleneck\n","        self.b1 = ConvBlock(##, embed_dim)\n","        self.b2 = ConvBlock(##, embed_dim)\n","\n","        # decoder\n","        self.d1 = DecBlock(##, embed_dim)\n","        self.d2 = DecBlock(##, embed_dim)\n","        self.d3 = DecBlock(##, embed_dim)\n","        self.d4 = DecBlock(##, embed_dim)\n","\n","        # output layer\n","        self.output = ##\n","\n","    def forward(self, x, t):\n","\n","        t_embed = self.embed(t)\n","\n","        # encoder\n","        ##\n","\n","        # bottleneck\n","        ##\n","\n","        # decoder\n","        ##\n","\n","        # output\n","        ##\n","\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"AUwk7c_RE1EQ"},"source":["### **1.5. Model training**\n","\n","To train the network, we will use the following procedure:\n","\n","1. Randomly select a sample from the training data $x_{0}$\n","\n","2. Randomly sample $t$ from a uniform distribution over the range $[1,T]$\n","\n","3. Randomly sample $\\epsilon$ from a zero-mean normal distribution with a variance of 1\n","\n","4. Apply the forward diffusion process on $x_{0}$ using $\\epsilon$ and $t$\n","\n","5. Predict the noise by evaluating $\\epsilon_{\\theta}(x_{t}, t)$\n","\n","6. Calculate the MSE loss between the true noise $\\epsilon$ and the predicted noise $\\epsilon_{\\theta}(x_{t}, t)$\n","\n","7. Take a gradient descent (or Adam) step with respect to the model parameters $\\theta$\n","\n","8. Repeat steps 1 to 7 until converged\n","\n","These steps are performed batch-wise in practice.\n","\n","Let's start by instantiating our model, optimizer and criterion:"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"sO7NC3sGE1EQ"},"outputs":[],"source":["# define the model and send to the GPU\n","model = ##\n","\n","# specify the Adam optimizer with a learning rate of 2e-4\n","opt = ##\n","\n","# set the criterion to be the mse loss\n","criterion = ##"]},{"cell_type":"markdown","source":["Next, let's build our training loop following the steps above:"],"metadata":{"id":"0s0583Vu6THA"}},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"i-l_qgtsE1EQ"},"outputs":[],"source":["# ensure model is in training mode\n","##\n","\n","# set number of epochs and loop over them\n","n_epoch = 5\n","\n","# Set up liveloss plot\n","liveloss = PlotLosses()\n","\n","for epoch in range(n_epoch):\n","\n","    # loop over each batch using tqdm to report progress and loss stats\n","    pbar = tqdm(dataloader)\n","    tot_loss = 0\n","    logs = {}\n","    for x0, _ in pbar:\n","\n","        # zero the gradients\n","        opt.zero_grad()\n","\n","        # send the batch to the GPU\n","        x0 = x0.to(device)\n","\n","        # sample t from a uniform distribution\n","        t = ##\n","\n","        # sample e from a normal distribution\n","        e = ##\n","\n","        # run the forward diffusion process to add noise to x0\n","        x_t = ##\n","\n","        # calculate the loss between the predicted noise and the true noise\n","        loss = ##\n","\n","        # backpropagation to obtain gradients w.r.t model parameters\n","        loss.backward()\n","\n","        # Report current loss using tqdm\n","        tot_loss += loss.item()\n","        pbar.set_description(f\"loss: {loss.item():.4f}\")\n","\n","        # take an optimisation step\n","        opt.step()\n","\n","    logs['loss'] = tot_loss / len(dataloader)\n","    liveloss.update(logs)\n","    liveloss.draw()"]},{"cell_type":"markdown","source":["### **Reverse diffusion**\n","\n","With a trained model, let's sample from it!\n","\n","We'll start by implementing the reverse diffusion process:"],"metadata":{"id":"X1dHD5dxrhO0"}},{"cell_type":"code","source":["# let's define a function that implements the reverse diffusion process\n","def reverse_diffusion(x, t, e, z):\n","    \"\"\"run the reverse diffusion process\"\"\"\n","    ##\n","\n","    return ##"],"metadata":{"id":"QXq8b2olrhO0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M6hEmsNerhO0"},"source":["The sampling process then will be:\n","\n","1. Randomly sample $x_{T}$ from a zero-mean normal distribution with a variance of 1\n","\n","2. If $t>0$, randomly sample $z$ from a zero-mean normal distribution with a variance of 1, otherwise set $z=0$\n","\n","3. Calculate the updated sample $x_{t-1}$ by subtracting the predicted noise $\\epsilon_{\\theta}(x_{t}, t)$ from $x_{t}$\n","\n","4. Apply $z$, scaled by $\\sigma_{t}$, to the sample\n","\n","5. Repeat steps 2 to 4 for $t=T,\\dots,1$"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"8bYv4341rhO1"},"outputs":[],"source":["# ensure model is in evaluation mode\n","##\n","\n","# randomly sample an xT batch from a normal distribution\n","x = ##\n","\n","# send to the GPU and only consider first 16 of the batch to ease the computational burden\n","x = x.to(device)[:16]\n","\n","# container to store sample throughout the reverse diffusion\n","diffusion = torch.empty((T, x.shape[0], x.shape[1], x.shape[2], x.shape[3])).to(x.device)\n","\n","# context management to ensure gradients are not tracked for any torch tensors or parameters\n","with torch.no_grad():\n","\n","    # loop over time from T to 0\n","    for t in reversed(range(0,T)):\n","\n","        # sample z from a normal distribution if condition met\n","        ##\n","\n","        # visualize samples x every 100 iterations\n","        if t % 50 == 0:\n","            print(f'samples at t={t}')\n","            plot_batch(x.cpu())\n","\n","        # convert t to a tensor, send to the GPU, and expand its first dimension to be equal to batch size\n","        t = ##\n","\n","        # denoise x\n","        x = ##\n","\n","        # clipping to ensure the sample values remain in the range -1 to 1\n","        x = torch.clamp(x, -1, 1)\n","\n","        # store the current samples x\n","        diffusion[t] = x"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"ZKFzK_nWrhO1"},"outputs":[],"source":["# visualize the diffusion process for a single sample\n","plot_batch(diffusion[::66,0].flip(0).cpu())"]},{"cell_type":"markdown","metadata":{"id":"4WsozOmjwoJm"},"source":["<br>\n","\n","---\n","\n","<br>\n","\n","## 2. Fine-tuning our model\n","\n","We are going to make some changes to the model above to squeeze out some performance improvements.\n","\n","Under **appendix B** of the DDPM paper, it states that they use a modified U-Net with [group normalisation](https://arxiv.org/abs/1803.08494). So, first we are going to replace batch normalisation with group normalisation.\n","\n","**Consider:** Why do you think group normalisation might be more suitable than batch normalisation for the DDPM model?\n","\n","<br>\n","\n","Under **appendix B** of the DDPM paper, it states that they use [residual](https://arxiv.org/abs/1512.03385) blocks in the downsampling and upsampling paths of the U-Net. We are also going to incorporate residual connections into the encoder and decoder blocks.\n","\n","**Consider:** Why do you think residual connections might have an impact in this case?\n","\n","<br>\n","\n","You may also notice that the DDPM model uses the [attention](https://arxiv.org/abs/1706.03762) mechanism used by Transformers. We will not be implementing attention today.\n","\n","### **2.1. Updated diffusion model**\n","\n","Let's make the necessary changes to our `ConvBlock`, `EncBlock` and `DecBlock`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Iafd2_9olKr"},"outputs":[],"source":["# define a convolutional block with time-embedding\n","class ConvBlock(nn.Module):\n","    \"\"\"time-conditioned convolutional block\"\"\"\n","\n","    def __init__(self, in_c, out_c, embed_dim):\n","        super().__init__()\n","        self.conv = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n","        self.dense = nn.Linear(embed_dim, out_c) # reshapes the time embedding length to the number of channels\n","        self.gn = ##  ## group normalisation\n","        self.act = nn.SiLU()\n","\n","    def forward(self, x, t_embed):\n","        x = self.conv(x)\n","        x += self.dense(t_embed)[...,None,None] # add the output of the time embedding dense layer\n","        x = ##  ## group normalisation\n","        x = self.act(x)\n","        return x\n","\n","# define an encoder block of the U-Net with time-embedding\n","class EncBlock(nn.Module):\n","    \"\"\"time-conditioned U-Net encoder block\"\"\"\n","\n","    def __init__(self, in_c, out_c, embed_dim):\n","        super().__init__()\n","        self.conv_block1 = ConvBlock(in_c, out_c, embed_dim)\n","        self.conv_block2 = ConvBlock(out_c, out_c, embed_dim)\n","        self.res = ##  ## add a convolutional layer to handle skip connection dimensionality\n","        self.pool = nn.MaxPool2d((2, 2))\n","\n","    def forward(self, x, t_embed):\n","        h = self.conv_block1(x, t_embed)\n","        h = self.conv_block2(h, t_embed)\n","        ##  ## residual block skip connection\n","        p = self.pool(h)\n","        return h, p\n","\n","# define an decoder block of the U-Net with time-embedding\n","class DecBlock(nn.Module):\n","    \"\"\"time-conditioned U-Net decoder block\"\"\"\n","\n","    def __init__(self, in_c, out_c, embed_dim):\n","        super().__init__()\n","        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n","        self.conv_block1 = ConvBlock(out_c+out_c, out_c, embed_dim)\n","        self.conv_block2 = ConvBlock(out_c, out_c, embed_dim)\n","\n","    def forward(self, x, s, t_embed):\n","        x = self.up(x)\n","        h = torch.cat([x, s], axis=1) # concatenate x with U-Net skip connection from encoder\n","        h = self.conv_block1(h, t_embed)\n","        h = self.conv_block2(h, t_embed)\n","        return ##  ## residual block skip connection"]},{"cell_type":"markdown","metadata":{"id":"Dpd16e26oonP"},"source":["Putting it all together, we can now define our time-conditioned U-Net!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xWgTvd438CUo"},"outputs":[],"source":["class Unet(nn.Module):\n","    \"\"\"DDPM U-Net, https://arxiv.org/abs/2006.11239 and https://arxiv.org/abs/1505.04597\"\"\"\n","\n","    def __init__(self, n_channels, embed_dim):\n","        super().__init__()\n","\n","        # time positional embedding MLP\n","        self.embed = nn.Sequential(SinusoidalPositionEmbeddings(embed_dim),\n","                                   nn.Linear(embed_dim, embed_dim),\n","                                   nn.GELU(),\n","                                   nn.Linear(embed_dim, embed_dim))\n","\n","        # encoder\n","        self.e1 = EncBlock(n_channels, 64, embed_dim)\n","        self.e2 = EncBlock(64, 128, embed_dim)\n","        self.e3 = EncBlock(128, 256, embed_dim)\n","        self.e4 = EncBlock(256, 512, embed_dim)\n","\n","        # bottleneck\n","        self.b1 = ConvBlock(512, 1024, embed_dim)\n","        self.b2 = ConvBlock(1024, 1024, embed_dim)\n","\n","        # decoder\n","        self.d1 = DecBlock(1024, 512, embed_dim)\n","        self.d2 = DecBlock(512, 256, embed_dim)\n","        self.d3 = DecBlock(256, 128, embed_dim)\n","        self.d4 = DecBlock(128, 64, embed_dim)\n","\n","        # output layer\n","        self.output = nn.Conv2d(64, n_channels, kernel_size=1, padding=0)\n","\n","    def forward(self, x, t):\n","\n","        t_embed = self.embed(t)\n","\n","        # encoder\n","        s1, x = self.e1(x, t_embed)\n","        s2, x = self.e2(x, t_embed)\n","        s3, x = self.e3(x, t_embed)\n","        s4, x = self.e4(x, t_embed)\n","\n","        # bottleneck\n","        x = self.b1(x, t_embed)\n","        x = self.b2(x, t_embed)\n","\n","        # decoder\n","        x = self.d1(x, s4, t_embed)\n","        x = self.d2(x, s3, t_embed)\n","        x = self.d3(x, s2, t_embed)\n","        x = self.d4(x, s1, t_embed)\n","\n","        # output\n","        output = self.output(x)\n","\n","        return output"]},{"cell_type":"markdown","source":["### **2.2. Model training**\n","\n","To train the network, we will use the following procedure:\n","\n","1. Randomly select a sample from the training data $x_{0}$\n","\n","2. Randomly sample $t$ from a uniform distribution over the range $[1,T]$\n","\n","3. Randomly sample $\\epsilon$ from a zero-mean normal distribution with a variance of 1\n","\n","4. Apply the forward diffusion process on $x_{0}$ using $\\epsilon$ and $t$\n","\n","5. Predict the noise by evaluating $\\epsilon_{\\theta}(x_{t}, t)$\n","\n","6. Calculate the MSE loss between the true noise $\\epsilon$ and the predicted noise $\\epsilon_{\\theta}(x_{t}, t)$\n","\n","7. Take a gradient descent (or Adam) step with respect to the model parameters $\\theta$\n","\n","8. Repeat steps 1 to 7 until converged\n","\n","These steps are performed batch-wise in practice.\n","\n","Let's start by instantiating our model, optimizer and criterion:"],"metadata":{"id":"8LORVMW1rmUT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3SQMTwt_iBxF"},"outputs":[],"source":["# define the model and send to the GPU\n","model = Unet(1, img_dim*4).to(device)\n","\n","# specify the Adam optimizer with a learning rate of 2e-4 (see appendix B)\n","opt = torch.optim.Adam(model.parameters(), lr=2e-4)\n","\n","# set the criterion to be the mse loss\n","criterion = F.mse_loss"]},{"cell_type":"markdown","source":["Now, let's build our training loop again:"],"metadata":{"id":"UaXmjkml8hnm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"CV1VFKMrHqSM"},"outputs":[],"source":["# ensure model is in training mode\n","model.train()\n","\n","# set number of epochs and loop over them\n","n_epoch = 5\n","\n","# Set up liveloss plot\n","liveloss = PlotLosses()\n","\n","for epoch in range(n_epoch):\n","\n","    # loop over each batch using tqdm to report progress and loss stats\n","    pbar = tqdm(dataloader)\n","    tot_loss = 0\n","    logs = {}\n","    for x0, _ in pbar:\n","\n","        # zero the gradients\n","        opt.zero_grad()\n","\n","        # send the batch to the GPU\n","        x0 = x0.to(device)\n","\n","        # sample t from a uniform distribution\n","        t = torch.randint(0, T, (x0.shape[0],), dtype=torch.long).to(x0.device)\n","\n","        # sample e from a normal distribution\n","        e = torch.randn_like(x0)\n","\n","        # run the forward diffusion process to add noise to x0\n","        x_t = forward_diffusion(x0, t, e)\n","\n","        # calculate the loss between the predicted noise and the true noise\n","        loss = criterion(model(x_t, t), e)\n","\n","        # backpropagation to obtain gradients w.r.t model parameters\n","        loss.backward()\n","\n","        # Report current loss using tqdm\n","        tot_loss += loss.item()\n","        pbar.set_description(f\"loss: {loss.item():.4f}\")\n","\n","        # take an optimisation step\n","        opt.step()\n","\n","    logs['loss'] = tot_loss / len(dataloader)\n","    liveloss.update(logs)\n","    liveloss.draw()"]},{"cell_type":"markdown","source":["### **2.3. Reverse diffusion**\n","\n","Let's implement the reverse diffusion process again:"],"metadata":{"id":"jY_827QlrTnU"}},{"cell_type":"code","source":["# let's define a function that implements the reverse diffusion process\n","def reverse_diffusion(x, t, e, z):\n","    \"\"\"run the reverse diffusion process\"\"\"\n","    sigma = fix_batch(torch.sqrt(betas), t, x.device)\n","    sqrt_recip_alphas = fix_batch(torch.sqrt(1.0 / alphas), t, x.device)\n","    scale = fix_batch((1 - alphas) / torch.sqrt(1. - alphas_bar), t, x.device)\n","\n","    return sqrt_recip_alphas * (x - scale*e) + sigma*z"],"metadata":{"id":"jWtxaZYFrQ6i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"deEpiqWrIELK"},"source":["The sampling process then will be:\n","\n","1. Randomly sample $x_{T}$ from a zero-mean normal distribution with a variance of 1\n","\n","2. If $t>0$, randomly sample $z$ from a zero-mean normal distribution with a variance of 1, otherwise set $z=0$\n","\n","3. Calculate the updated sample $x_{t-1}$ by subtracting the predicted noise $\\epsilon_{\\theta}(x_{t}, t)$ from $x_{t}$\n","\n","4. Apply $z$, scaled by $\\sigma_{t}$, to the sample\n","\n","5. Repeat steps 2 to 4 for $t=T,\\dots,1$"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"8zSLqDDeIELL"},"outputs":[],"source":["# ensure model is in evaluation mode\n","model.eval()\n","\n","# randomly sample an xT batch from a normal distribution\n","x = torch.randn_like(next(iter(dataloader))[0])\n","\n","# send to the GPU and only consider first 16 of the batch to ease the computational burden\n","x = x.to(device)[:16]\n","\n","# container to store sample throughout the reverse diffusion\n","diffusion = torch.empty((T, x.shape[0], x.shape[1], x.shape[2], x.shape[3])).to(x.device)\n","\n","# context management to ensure gradients are not tracked for any torch tensors or parameters\n","with torch.no_grad():\n","\n","    # loop over time from T to 0\n","    for t in reversed(range(0,T)):\n","\n","        # sample z from a normal distribution if condition met\n","        if t > 0:\n","            z = torch.randn_like(x)\n","        else:\n","            z = 0\n","\n","        # visualize samples x every 100 iterations\n","        if t % 50 == 0:\n","            print(f'samples at t={t}')\n","            plot_batch(x.cpu())\n","\n","        # convert t to a tensor, send to the GPU, and expand its first dimension to be equal to batch size\n","        t = torch.tensor(t).to(device).expand(x.shape[0])\n","\n","        # denoise x\n","        x = reverse_diffusion(x, t, model(x, t), z)\n","\n","        # clipping to ensure the sample values remain in the range -1 to 1\n","        x = torch.clamp(x, -1, 1)\n","\n","        # store the current samples x\n","        diffusion[t] = x"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"ZYpGCJGvIELL"},"outputs":[],"source":["# visualize the diffusion process for a single sample\n","plot_batch(diffusion[::66,0].flip(0).cpu())"]},{"cell_type":"markdown","metadata":{"id":"5OhaJK-PEqEf"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VI1ot19YExW-"},"source":["## 3. Stable diffusion demo"]},{"cell_type":"markdown","metadata":{"id":"MhA1FXvwC6_H"},"source":["[Stable diffusion](https://github.com/CompVis/stable-diffusion) is an open-source pre-trained text-to-image model based on the latent diffusion model presented in [this](https://arxiv.org/abs/2112.10752) paper.\n","\n","Feel free to play with this pre-trained model to see how powerful Diffusion Models can be."]},{"cell_type":"markdown","metadata":{"id":"SkuRhfYmE2fc"},"source":["---\n","\n","<br>\n","<center>\n","⚠\n","\n","<br>\n","\n","Do not burn through all of your Google colab compute units!\n","\n","⚠\n","\n","<br>\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"YTCxARn2DuJG"},"source":["#### **3.1. Configuration**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mc_Z-ef3Zb9o"},"outputs":[],"source":["# install necessary libraries\n","!pip install --quiet --upgrade diffusers transformers scipy mediapy accelerate"]},{"cell_type":"markdown","metadata":{"id":"oHTjlPgtGT1K"},"source":["*You may need to restart your kernel for the accelerate library installation to work correctly.*\n","\n","**Step 1.** You will need a hugging face account to be able to access and download the stable diffusion model weights.\n","\n","<br>\n","\n","**Step 2.** Once you have an account, you will need to generate a token to login. Execute the following cell and enter your token.\n","\n","<br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L2v8aWJJIwpu"},"outputs":[],"source":["!huggingface-cli login"]},{"cell_type":"markdown","metadata":{"id":"P26nZFnRHKjn"},"source":["**Step 3.** Visit the [card](https://huggingface.co/CompVis/stable-diffusion-v1-4) for the stable diffusion v1-4 model we will be using today, read license and tick the check box if you agree.\n","\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"GYRsTJnKGzz6"},"source":["#### **3.2. Imports**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DQyHWXlJGzNG"},"outputs":[],"source":["from diffusers import PNDMScheduler\n","from diffusers import StableDiffusionPipeline\n","import mediapy as media\n","import torch"]},{"cell_type":"markdown","metadata":{"id":"-2eliWMIEQ_F"},"source":["#### **3.3. Variance schedule**\n","\n","There are multiple proposed variance schedules that can be used with pre-trained diffusion models such as Stable diffusion. We will use the [PNDM](https://arxiv.org/abs/2202.09778) scheduler here, but there are many other suitable alternatives."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0KQKMazRbhIn"},"outputs":[],"source":["# define the PNDM scheduler\n","scheduler = PNDMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", skip_prk_steps=True, steps_offset=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fu-FxkgdZ_u2"},"outputs":[],"source":["# load the pre-trained stable diffusion model pipeline\n","pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", scheduler=scheduler,\n","                                               torch_dtype=torch.float16, variant=\"fp16\", use_auth_token=True)\n","\n","# send pipeline to the GPU\n","pipe.to(\"cuda\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YDF4jEBvaAXI"},"outputs":[],"source":["# define a text prompt\n","prompt = \"A sausage dog walking through Imperial College London\"\n","\n","# generate multiple images from the same text prompt\n","num_images = 4\n","prompts = [prompt] * num_images\n","\n","# run the reverse diffusion\n","images = pipe(prompts, guidance_scale=8, num_inference_steps=50).images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L_tv6Zp-bSJi"},"outputs":[],"source":["media.show_images(images, height=400)"]},{"cell_type":"markdown","source":["<br>\n","\n","---\n","\n","<br>"],"metadata":{"id":"3Bb3a-NS9VBM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jQPiZjH0Cq5s"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":0}