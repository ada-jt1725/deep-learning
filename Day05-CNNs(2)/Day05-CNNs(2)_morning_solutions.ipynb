{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/ese-ada-lovelace-2024/DL_Module_2024_Teaching/blob/main/Day05-CNNs(2)_and_Probability/Day05-CNNs(2)_morning_solutions.ipynb","timestamp":1732721812374}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1cXtXdAcwedVDbapmz1pj_hULsQrhEcff\" width=\"500\"/>\n","\n","---\n"],"metadata":{"id":"9YehS8enAmDn"}},{"cell_type":"markdown","source":["# **CNNs: convolutional neural networks (part 2)**\n","\n","#### **Morning contents/agenda**\n","\n","1. Commonly used datasets in computer vision\n","\n","2. Important CNN architectures\n","\n","3. U-nets and upsampling (unpooling & transpose convolutions)\n","\n","4. Transfer learning\n","\n","5. CNN receptive field\n","\n","#### **Learning outcomes**\n","\n","1. Awareness of well-established CNN architectures\n","\n","2. Understand how to upsample data\n","\n","3. Understand how and why transfer learning is used\n","\n","<br>\n","\n","#### **Afternoon contents/agenda**\n","\n","1. Inspection of CNN filters\n","\n","2. Transfer learning from ImageNet to Bees and Ants\n","\n","#### **Learning outcomes**\n","\n","1. Become familiar with the effect that filters have (sometimes you can interpret them, sometimes they have abstracted the data too far to develop intuitions)\n","\n","2. Hands-on knowledge on how to apply transfer learning\n","\n","\n","<br/>\n","\n","---\n","\n","<br/>"],"metadata":{"id":"9CY6juJtSkmD"}},{"cell_type":"code","source":["!pip install pycm livelossplot\n","%pylab inline\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","from livelossplot import PlotLosses\n","from pycm import *\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","import torchvision.datasets\n","import torchvision.transforms as transforms\n","from torchvision.datasets import MNIST\n","from torchsummary import summary"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_kYki018eTFJ","outputId":"6c4faebb-7d49-4bfa-a76a-5476e72df39b","executionInfo":{"status":"ok","timestamp":1732967249663,"user_tz":0,"elapsed":27132,"user":{"displayName":"Carlos Cueto Mondéjar","userId":"10605816413378977833"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pycm\n","  Downloading pycm-4.1-py3-none-any.whl.metadata (49 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting livelossplot\n","  Downloading livelossplot-0.5.5-py3-none-any.whl.metadata (8.7 kB)\n","Collecting art>=1.8 (from pycm)\n","  Downloading art-6.4-py3-none-any.whl.metadata (71 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from pycm) (1.26.4)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from livelossplot) (3.8.0)\n","Requirement already satisfied: bokeh in /usr/local/lib/python3.10/dist-packages (from livelossplot) (3.6.1)\n","Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (3.1.4)\n","Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (1.3.1)\n","Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (24.2)\n","Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (2.2.2)\n","Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (11.0.0)\n","Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (6.0.2)\n","Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (6.3.3)\n","Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (2024.9.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (4.55.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (1.4.7)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (2.8.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.9->bokeh->livelossplot) (3.0.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->bokeh->livelossplot) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->bokeh->livelossplot) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->livelossplot) (1.16.0)\n","Downloading pycm-4.1-py3-none-any.whl (70 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading livelossplot-0.5.5-py3-none-any.whl (22 kB)\n","Downloading art-6.4-py3-none-any.whl (608 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.6/608.6 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: art, pycm, livelossplot\n","Successfully installed art-6.4 livelossplot-0.5.5 pycm-4.1\n","Populating the interactive namespace from numpy and matplotlib\n"]}]},{"cell_type":"code","source":["def set_seed(seed):\n","    \"\"\"\n","    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n","    \"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n","    torch.backends.cudnn.enabled   = False\n","\n","    return True\n","\n","device = 'cpu'\n","if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n","    print(\"Cuda installed! Running on GPU!\")\n","    device = 'cuda'\n","else:\n","    print(\"No GPU available!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LI8sNA9feT3H","outputId":"a33995c5-cef7-435e-e39d-2bf4a6a64766"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cuda installed! Running on GPU!\n"]}]},{"cell_type":"markdown","source":["## CNN Recap\n","\n","A visual representation of a convolutional operation:\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1Et5iK-HvMJbnvxiUrCK3h9l6y49jZiGa\" width=\"400\"/></center>\n","\n","<br>\n","\n","Which can be expressed mathematically as:\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1ME4bFACe5hE9pSYyIvclE1lE542YbEL3\" width=\"800\"/></center>\n","\n","<br>\n","\n","Convolutions can be described with only 4 parameters:\n","\n","- input size\n","- filter or kernel size\n","- stride\n","- padding\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1I8s30COXw4SRdV97gy2Jn13ann7Ng8Uo\" width=\"800\"/></center>\n","\n","<br>\n","\n","A few simple examples to help clarify convolutions with different strides and paddings:\n","\n","<img src=\"https://drive.google.com/uc?id=1QTvlkJm1-ZZuJSZhWK1_YHQMuVMOaW6F\" width=\"200\"/>\n","<img src=\"https://drive.google.com/uc?id=1XWKNVrCIl6M9Lfn0Gt91bDGwHeWfFR9A\" width=\"200\"/>\n","<img src=\"https://drive.google.com/uc?id=1jNUFxCrd_wAB7pxlzzEq1GOEyCz7GSwf\" width=\"200\"/>\n","<img src=\"https://drive.google.com/uc?id=1S2pgp7r6oQ_tqgnYnmDB-q3TcmgzKrs5\" width=\"200\"/>\n","\n","<br>\n","\n","To better illustrate how the input and output channels are defined in convolutional layers, consider the case where our input image has three channels (R,G,B) and the output only has one channel:\n","\n","<p align = \"center\"><img src=\"https://drive.google.com/uc?id=1jyD_4d3HvulHQ5obeKJ57ANt6-U66din\" width=\"800\"/></p><p align = \"center\">\n","<i>3-channel input (RGB image) and 1-channel output example</i>\n","</p>\n","\n","<br>\n","\n","We can compare this to the case where the input has three channels (R,G,B) but the output has two channels\n","\n","<br>\n","\n","<p align = \"center\"><img src=\"https://drive.google.com/uc?id=1400AvvRTZkRH06-13XHDR19GILqBxwTP\" width=\"800\"/></p><p align = \"center\">\n","<i>3-channel input (RGB image) and 2-channel output example</i>\n","</p>\n","\n","<br>\n","\n","---\n","\n","<br>"],"metadata":{"id":"Pw6y-zji3xVv"}},{"cell_type":"markdown","source":["## 1. Commonly used datasets in computer vision\n","\n","As we saw on the first week, the network capacity has to be adjusted in order to avoid overfitting to the data. In other words, very deep networks with large number of trainable parameters require big datasets because they have a lot of capacity to accomodate variations in the data.\n","\n","So far we have seen MNIST and similarly small-sized datasets:\n","\n","<br>\n","\n","<p align = \"center\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\" width=\"400\"/></p><p align = \"center\">\n","<i>MNIST dataset: 60k training & 10k test images</i>\n","</p>\n","\n","\n","\n","\n","\n","<br>\n","\n","It is often desirable to have datasets of natural images, as they can be used for a broader range of applications than MNIST-like datasets. [CIFAR-10 and CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html) are two datasets of natural images with 10 and 100 classes respectively:\n","\n","<br>\n","\n","<p align = \"center\"><img src=\"https://docs.pytorch.org/tutorials/_images/cifar10.png\" width=\"400\"/></p><p align = \"center\">\n","<i>CIFAR-10 dataset: 50k training & 10k test images</i>\n","</p>\n","\n","<br>\n","\n","<p align = \"center\"><img src=\"https://miro.medium.com/max/1400/0*fqFMfJeP6CuBTuYc.webp\" width=\"400\"/></p><p align = \"center\">\n","<i>CIFAR-100 dataset: 50k training & 10k test images</i>\n","</p>\n","\n","<br>\n","\n","But larger datasets exist as well. [ImageNet](https://www.image-net.org/) has been used in various competitions, and it contains more than 14 million images and 20k classes:\n","\n","<p align = \"center\"><img src=\"https://i0.wp.com/syncedreview.com/wp-content/uploads/2020/06/Imagenet.jpg?fit=1400%2C600&ssl=1\" width=\"800\"/></p><p align = \"center\">\n","<i>ImageNet: >14M images and 20k classes </i>\n","</p>\n","\n","\n","[Here](https://pytorch.org/vision/stable/datasets.html) is a list of available datasets in `torchvision.datasets`\n","\n"],"metadata":{"id":"NVjTNdKHVF0H"}},{"cell_type":"markdown","source":["## 2. Important CNN architectures\n","\n","Since their introduction in 1998 with LeNet-5, convolutional neural networks have evolved significantly and competed for the top spot in computer vision tasks.\n","\n","You can find a [good overview here](https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d) (the images below are from this website).\n","\n","<br>\n","\n","<img src=\"https://drive.google.com/uc?id=15lzuEPzqWAfhqzDoCxBkYdB1HdmjQt42\" width=\"800\"/>\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1Qkxygn32yhPx0ZsObJdeFsTbY1RfJg_7\" width=\"800\"/></center>\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=13HyGvrfO1y9cgetruyXtm5livL09MB2A\" width=\"800\"/></center>\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1CoVWz8e9DGHgtsKSd8oQDo6qL35gzLS6\" width=\"800\"/></center>\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1ZC2aIndGpfFd4oAIMloafDS7Lqax1_Uu\" width=\"800\"/></center>\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1B47JOoRDygFH0hW9DL6wDll_cU4NZ9vO\" width=\"800\"/></center>\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1ZqGa7aPcPXmXPOJBluB6Xjzj2wDnk1qT\" width=\"800\"/></center>\n","\n","<br>\n","\n","As you can see, network sizes increase over time thanks to advances in computational power (better GPUs with more memory, etc):\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1AQIN4za44eMsinsGxfV4YCfpHKVo-tqX\" width=\"600\"/></center>\n","\n","<br>\n","\n","But even this numbers are considered small in modern architectures. For example, **GPT-4** has 1.7 trillion parameters.\n","\n","\n","<br>\n","\n","---\n","\n","<br/>"],"metadata":{"id":"sinIebTtMUvs"}},{"cell_type":"markdown","source":["## 3. U-nets and upsampling (unpooling & transpose convolutions)\n","\n","What are the outputs of the CNNs we have seen so far?\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=14Bj-nbg5nbTN_7HT9EtMX8oJq5I5xTIk\" width=\"800\"/></center>\n","\n","<br>"],"metadata":{"id":"yPC2aDTe83n_"}},{"cell_type":"markdown","source":["<br>\n","\n","These were classification networks, and their outputs were vectors of probabilities (calculated using the Softmax activation):\n","\n","<center><img src=\"https://drive.google.com/uc?id=13QroJYkGtpKX0jBhH2folt-Lj9q_MZ1w\" width=\"800\"/></center>\n","\n","<br>\n","\n","But CNNs have other applications. In the field of computer vision, a very common architecture is the **U-Net** which is a type of convolutional autoencoder (we will see autoencoders next week):\n","\n","<p align = \"center\"><img src=\"https://drive.google.com/uc?id=1glxl06_zsq-2Off0E21VOQmG5k22R6_z\" width=\"800\"/></p><p align = \"center\">\n","<i> sources: <a href=\"https://arxiv.org/pdf/1505.04597.pdf\">original unet</a>, <a href=\"https://www.kaggle.com/c/tgs-salt-identification-challenge\"> seismic segmentation</a></i>\n","</p>\n","\n","<br>\n","\n","An important operation we perform to generate U-Net (and other architectures) is upscaling. The most common methods are:\n","\n","- nearest neighbour\n","- \"bed of nails\"\n","- Max unpooling\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1sVrf6toOeBLhYOD4v7L-OWTXX-ahQIiJ\" width=\"800\"/></center>\n","\n","<br>\n","\n","and\n","\n","- Transposed convolution or up-convolution, but **not deconvolution!**\n","\n"],"metadata":{"id":"oAWL3EvbUEOV"}},{"cell_type":"markdown","source":["## Transposed convolutions\n","\n","Transposed convolutions can be computed by following an easy recipe:\n","\n","<center><img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*54-7typHLLXhdvAhlku9SQ.png\" width=\"900\"/></center>\n","\n","\n","where we know that:\n","- `s`: stride\n","- `p`: padding\n","- `k`: kernel size\n","\n","and we use this hyperparameters to calculate:\n","- `z`: how many zeros to insert in between pixels of my input\n","- `p'`: how much padding do I add around the image\n","\n","But with the added caveat that, **as the name indicates**, we need to **transpose the kernel** before using it to convolve with the input. Transposing the kernel in this case implies flipping the kernel along each of its axes.\n","\n","# Exercise:\n","Let's practice with an example. Let's try and calculate a simple case by hand:\n","\n","\n","<p align = \"center\"><img src=\"https://drive.google.com/uc?id=1JwdzoUrh2EC-4qj4fYrRUVUaHG9pyfp6\" width=\"800\"/></p><p align = \"center\">\n","\n","To check if we have the right solution we can use [`conv_transpose2d`](https://pytorch.org/docs/stable/generated/torch.nn.functional.conv_transpose2d.html), which allows us to pass an input and a predefined filter, whereas the [`ConvTranspose2d`](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html#torch.nn.ConvTranspose2d) layer randomly initialises the weights of the kernel which is not what we need now."],"metadata":{"id":"urjrxKFIRF_2"}},{"cell_type":"code","source":["x = torch.tensor([[1, 5.], [2., 3.]])          ## why do I need to get an instance as below?\n","#x = torch.tensor([[[[1, 5.], [2., 3.]]]])     ## how many square brackets do I need, and what do they do?\n","print(x.shape)\n","\n","kernel = torch.tensor([[0, 1.], [2., 3.]])\n","# kernel = torch.tensor([[[[0, 1.], [2., 3.]]]])\n","\n","torch.nn.functional.conv_transpose2d(x, kernel, stride=1, padding=0)"],"metadata":{"id":"qU5o6oetS_qo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Transpose convolutions, particularly with stride bigger than 1 can lead to checkerboard imprints on the outputs:"],"metadata":{"id":"u3FyeCyaRjuo"}},{"cell_type":"code","source":["%%html\n","<iframe src=\"https://distill.pub/2016/deconv-checkerboard/\" width=\"1000\" height=\"500\"></iframe>"],"metadata":{"id":"wXcJzNfOmFAG","colab":{"base_uri":"https://localhost:8080/","height":525},"outputId":"f5945aa5-a3af-4034-8ec2-5c1dc522411e"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<iframe src=\"https://distill.pub/2016/deconv-checkerboard/\" width=\"1000\" height=\"500\"></iframe>\n"]},"metadata":{}}]},{"cell_type":"markdown","source":["<br>\n","\n","---\n","\n","<br>"],"metadata":{"id":"YaUY8kGw-v4u"}},{"cell_type":"markdown","source":["## 4. Transfer learning\n","\n","What is transfer learning and why is it useful? A definition from the Deep Learning book by Goodfellow et al (2016):\n","\n",">*Transfer learning and domain adaptation refer to the situation where what has been learned in one setting ... is exploited to improve generalization in another setting.*\n","\n","<br>\n","\n","- The most well-known CNN designs are **available** on-line and have been **successfully trained** on very large number of images (millions).\n","\n","- In many applications we often work with a relatively **small number of images** (or data in general).\n","\n","- The idea of transfer learning is to use an existing trained CNN model which tries to solve a problem of similar nature and **tailor the model** to our particular application.\n","\n","The two main strategies are:\n","\n","1. **Add one (or more) layers, or retrain the last layer(s) of a pre-trained network**: This strategy assumes that the filters of most of the network do a good job at extracting data features we can use. The last layers, then, act as a final fine-tunning to capture the specific features of our data.\n","\n","2. **Retrain the whole network with small learning rates:** This strategy assumes that as a whole, the network captures data features well, and it only needs a bit of a ‘nudge’ to adapt the network parameters to our particular problem. In this case, we want to keep the underlying abstraction that the network does at different scales, but fine-tune it to our problem.\n","\n","We will see examples of both in this afternoon exercise.\n","\n","<br>\n","\n","---\n","\n","<br>"],"metadata":{"id":"4LzLdZmAdnPd"}},{"cell_type":"markdown","source":["## 5. CNN receptive field\n","\n","The receptive field is defined as the region of the input that affects the output, and it can be defined between adjacent or non-adjacent CNN layers.\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1Z08dAQ5fvDBXX8XAnzK5yfpsUTRuRrKm\" width=\"800\"/></center>\n","\n","<br>\n","\n","Why are we interested in the receptive field in our network?\n","\n","Because the receptive field will determine what are the hyperparameters\n","I need to ensure full receptive field on my inputs:\n","\n","- filter size and stride\n","- number of convolutional layers in the network\n","\n","[Here](https://www.baeldung.com/cs/cnn-receptive-field-size) you can find a more detailed explanation with a few formulas to compute receptive fields.\n","\n","<br>\n","\n","---\n","\n","<br>"],"metadata":{"id":"eKJiEBxrgHc3"}},{"cell_type":"markdown","source":["# Solution exercise transpose convolution:\n","\n","\n","\n","<p align = \"center\"><img src=\"https://drive.google.com/uc?id=1JwdzoUrh2EC-4qj4fYrRUVUaHG9pyfp6\" width=\"800\"/></p><p align = \"center\">\n","\n","<p align = \"center\"><img src=\"https://drive.google.com/uc?id=1K1QClrmvD781tnZxM2RZTtzP4QmsG_QW\" width=\"800\"/></p><p align = \"center\">"],"metadata":{"id":"-yXL0BFKZglc"}},{"cell_type":"markdown","source":["And check that we can get the same result with a normal convolution"],"metadata":{"id":"ZHuxr6YEhRb2"}},{"cell_type":"code","source":["#x = torch.tensor([[1, 5.], [2., 3.]])          ## why do I need to get an instance as below?\n","x = torch.tensor([[[[1, 5.], [2., 3.]]]])     ## how many square brackets do I need, and what do they do?\n","print(x.shape)\n","\n","kernel = torch.tensor([[[[0, 1.], [2., 3.]]]]) ## transpose the kernel here since we will use an 'normal' convolution\n","print(kernel.shape)\n","\n","torch.nn.functional.conv_transpose2d(x, kernel, stride=1, padding=0)"],"metadata":{"id":"mCfb6buxhR8k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732967289903,"user_tz":0,"elapsed":632,"user":{"displayName":"Carlos Cueto Mondéjar","userId":"10605816413378977833"}},"outputId":"ebd82dbf-6bcc-4adc-a1dd-0cef4d751505"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 1, 2, 2])\n","torch.Size([1, 1, 2, 2])\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[[[ 0.,  1.,  5.],\n","          [ 2., 15., 18.],\n","          [ 4., 12.,  9.]]]])"]},"metadata":{},"execution_count":4}]}]}