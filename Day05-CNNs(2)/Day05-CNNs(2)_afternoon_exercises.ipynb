{"cells":[{"cell_type":"markdown","metadata":{"id":"9YehS8enAmDn"},"source":["<img src=\"https://drive.google.com/uc?id=1cXtXdAcwedVDbapmz1pj_hULsQrhEcff\" width=\"500\"/>\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"BV8Up_Q3Z5La"},"source":["# **CNNs: convolutional neural networks (part 2)**\n","\n","#### **Morning contents/agenda**\n","\n","1. Commonly used datasets in computer vision\n","\n","2. Important CNN architectures\n","\n","3. U-nets and upsampling (unpooling & transpose convolutions)\n","\n","4. Transfer learning\n","\n","5. Summary of CNNs\n","\n","#### **Learning outcomes**\n","\n","1. Awareness of well-established CNN architectures\n","\n","2. Understand how to upsample data\n","\n","3. Understand how and why transfer learning is used\n","\n","<br>\n","\n","#### **Afternoon contents/agenda**\n","\n","1. Inspection of CNN filters\n","\n","2. Transfer learning from ImageNet to Bees and Ants\n","\n","#### **Learning outcomes**\n","\n","1. Become familiar with the effect that filters have (sometimes you can interpret them, sometimes they have abstracted the data too far to develop intuitions)\n","\n","2. Hands-on knowledge on how to apply transfer learning\n","\n","\n","<br/>\n","\n","---\n","\n","<br/>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_kYki018eTFJ"},"outputs":[],"source":["!pip install pycm livelossplot\n","%pylab inline\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","from livelossplot import PlotLosses\n","from pycm import *\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","import torchvision.datasets\n","import torchvision.transforms as transforms\n","from torchvision.datasets import MNIST\n","from torchsummary import summary\n","\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import random"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LI8sNA9feT3H"},"outputs":[],"source":["def set_seed(seed):\n","    \"\"\"\n","    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n","    \"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n","    torch.backends.cudnn.enabled   = False\n","\n","    return True\n","\n","device = 'cpu'\n","if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n","    print(\"Cuda installed! Running on GPU!\")\n","    device = 'cuda'\n","else:\n","    print(\"No GPU available!\")"]},{"cell_type":"markdown","metadata":{"id":"sinIebTtMUvs"},"source":["## 1. Inspection of CNN filters\n","\n","Pytorch provides users with a rich set of pre-trained neural network architectures. These have mostly been pre-trained on ImageNet.   \n","[```torchvision.models```](https://pytorch.org/vision/stable/models.html) provides us with an interface to these pretrained deep neural networks.\n","\n","### **1.1. Pretrained AlexNet**\n","\n","### Load the model\n","\n","We're going to start by loading a pretrained [AlexNet](https://arxiv.org/abs/1404.5997) model from ```torchvision.models``` ([Source Code](https://pytorch.org/vision/stable/_modules/torchvision/models/alexnet.html) for AlexNet in Pytorch):\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pskzXya6gUN_"},"outputs":[],"source":["from torchvision import models\n","from torchsummary import summary\n","\n","alexnet = ##  ## access AlexNet from torchvision.models, load it with pretrained parameters, and send model to the device\n","##  ## print AlexNet\n","##  ## print a summary of the model"]},{"cell_type":"markdown","metadata":{"id":"F7EO_OudaYH1"},"source":["### Access the first layer\n","\n","Now, we can manually access the weights of the first layer.\n","\n","Notice in the prints above, that the layers of the AlexNet model have been grouped into two `Sequential` blocks named `features` and `classifier`. These blocks are essentially lists of `nn.Module` and can be indexed as such."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UDXRepBaEI2O"},"outputs":[],"source":["first_layer = ##  ## access the first layer by indexing on the `features` block of layers\n","##  ## print the first layer\n","##  ## access the weights of that first layer\n","\n","# Normalisation for plotting\n","min_w, max_w =  weights.min(), weights.max()\n","weights -= min_w\n","weights /= (max_w-min_w)\n","\n","fig, axarr = plt.subplots(8, 8, figsize=(12, 12))\n","axarr = axarr.flatten()\n","\n","for ax, kernel in zip(axarr, weights.cpu().numpy()):   # plot the three channels at the same\n","  ax.imshow(np.swapaxes(kernel, 0, 2))                 # time as an RGB image\n","  # ax.imshow(kernel)                                  # imshow does not like this shape (that's why we swap the axes in the line above)\n","\n","# for ax, kernel in zip(axarr, weights[:,1,:,:].cpu().numpy()):  # or plot one channel at a time\n","  # ax.imshow(kernel)                                            # (second dimension of the tensor)"]},{"cell_type":"markdown","metadata":{"id":"XPnuktDwbvPg"},"source":["### Access the second layer\n","\n","We can then follow a similar procedure to acces the second layer of the network.\n","\n","Notice that in the `Sequential` block called `features`, there's more components than just layers, including `ReLU` and `MaxPool2d`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p7U9nKHSERCQ"},"outputs":[],"source":["secondconv_layer = ##  ## access the second layer by indexing on the `features` block of layers\n","##  ## print the layer\n","weights = ##  ## access the weights of that second layer\n","##  ## check the shape of the weights\n","\n","fig, axarr = plt.subplots(12, 16, figsize=(12, 12))\n","axarr = axarr.flatten()\n","filter_chan = ##  ## because kernels in the second layer have 64 channels, we are going to plot them by choosing one single channel at a time\n","print(\"\")\n","print(\"filters: \", filter_chan)\n","print(\"\")\n","for ax, kernel in zip(axarr, weights[:,filter_chan,:,:].cpu().numpy()):\n","  ax.imshow(kernel)\n","  ax.set_axis_off()"]},{"cell_type":"markdown","metadata":{"id":"R3e8Cnb4Fb1x"},"source":["### **1.2. Pretrained ResNet**\n","\n","### Load the model\n","\n","Let's repeat the process for the ResNet18 pretrained network, `resnet18`.\n","\n","We can start by loading the pretrained model from `torchvision.models`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nb_S_g2NFV2y"},"outputs":[],"source":["resnet = ##  ## access ResNet from torchvision.models, load it with pretrained parameters, and send model to the device\n","##  ## print it\n","##  ## print a summary of the model"]},{"cell_type":"markdown","metadata":{"id":"9VviHB37dPrT"},"source":["### Access the first layer\n","\n","We can similarly access the first layer of the network and its weights.\n","\n","Notice that, in this case, the first layer is not part of a `Sequential` block."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2adT4ZkBFZAa"},"outputs":[],"source":["first_layer = ##  ## directly access the first layer by name\n","##  ## print the layer\n","weights = ##  ## access the weights of that first layer\n","##  ## check the shape\n","\n","# Normalisation for plotting (values are a bit crazy otherwise, try to comment these lines below and see what happens)\n","min_w, max_w =  weights.min(), weights.max()\n","weights -= min_w\n","weights /= (max_w-min_w)\n","\n","print('shape of weights: ', weights.shape)\n","\n","fig, axarr = plt.subplots(8, 8, figsize=(12, 12))\n","axarr = axarr.flatten()\n","\n","for ax, kernel in zip(axarr, weights.cpu().numpy()):   # plot the three channels at the same\n","  ax.imshow(np.swapaxes(kernel, 0, 2))                 # time as an RGB image\n","  # ax.imshow(kernel)                                  # imshow does not like this shape (that's why we swap the axes in the line above)\n","\n","\n","# for ax, kernel in zip(axarr, weights[:,1,:,:].cpu().numpy()):  # or plot one channel at a time\n","  # ax.imshow(kernel)                                            # (second dimension of the tensor)"]},{"cell_type":"markdown","metadata":{"id":"izEN3IenIfa5"},"source":["### **1.3. Pretrained Inception**\n","\n","### Load the model\n","\n","Finally, we can also try the loading the `inception_v3` model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"djyZjduDHDR4"},"outputs":[],"source":["## complete the code"]},{"cell_type":"markdown","metadata":{"id":"fCcwHn5beMlL"},"source":["### Access an intermediate layer\n","\n","And access one of the intermediate layers, the third layer of the block `Mixed_5d`.\n","\n","Notice that here, the layers have a hierarchical structure and can be accessed by name in a nested manner."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ehqpXgjtHnv-"},"outputs":[],"source":["## complete the code"]},{"cell_type":"markdown","metadata":{"id":"rfOjp9orI4g3"},"source":["## 2. Transfer learning from ImageNet to Bees and Ants\n","\\[*adapted from [this PyTorch tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)*\\]\n","\n","The basic principle behind transfer learning is to leverage features learned on one dataset and recycle them to perform tasks on another dataset.\n","\n","Instead of training a network from randomly initialised weights, we start from a network with weights trained in a different domain and fine-tune it to a different source task. By doing this, the weights of the network already have useful properties that can improve optimisation.\n","\n","To be able to apply transfer learning effectively, the data distribution that a very powerful model was trained on should follow a similar distribution as the other dataset that we are trying to apply transfer learning to.    \n","\n","For example: _We want to create a new classifier for cats and dogs given only a small set of say 100 training images of each category._\n","\n","Large neural networks that have been trained on ImageNet or CIFAR have similar categories in their dataset, say horses and maybe cows and many more categories of natural images.  \n","\n","The rationale is that since we've already learned a set of efficient filters on ImageNet, we can simply use a deep network as a feature extractor and only retrain the final layer —or alternatively fine tune all layers— of a given network to adapt it to our particular task\n","\n","\n","Transfer learning is a powerful tool because it can:\n","- prevent poor training from random weights when only small datasets are available.\n","- reduce training time for tasks that use data with 'similar' distributions\n","\n","<br>\n","\n","We will apply transfer learning to a small dataset containing images of bees and ants by transfering learning done by training with the ImageNet dataset.\n","\n","ImageNet is arguably the most popular dataset for benchmarking classification models. It contains around 14 million annotated natural images spread over 22 thousand categories. Images are of size 3 x 224 x 224, with normalised means and stds of [0.485, 0.456, 0.406] and [0.229, 0.224, 0.225]. In transfer learning is common practice to use the means and standard deviations of the data used for pretraining to normalise the new dataset. Note that the most popular networks (VGG, ResNet, AlexNet, etc.) have been designed to take as input 3 x 224 x 224 images to accomodate for ImageNet.\n","\n","<br>\n","\n","<p align = \"center\"><img src=\"https://i0.wp.com/syncedreview.com/wp-content/uploads/2020/06/Imagenet.jpg?fit=1400%2C600&ssl=1\" width=\"800\"/></p><p align = \"center\">\n","<i>ImageNet: >14M images and 20k classes </i>\n","</p>\n","\n","<br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TMw6UvW2JdQt"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"]},{"cell_type":"markdown","metadata":{"id":"w6nXgVBWuwkw"},"source":["\n","Let's start by setting some hyperparameters for the next few blocks:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JT06TZYTjyTK"},"outputs":[],"source":["seed = 42\n","lr = 1e-2\n","momentum = 0.9\n","batch_size = 64\n","test_batch_size = 1000\n","n_epochs = 30"]},{"cell_type":"markdown","metadata":{"id":"-dAUDSUqtp9r"},"source":["### **2.1 Loading and Visualising the Data**\n","\n","[Download](https://download.pytorch.org/tutorial/hymenoptera_data.zip) the dataset into your current directory (you can do it using the code provided below)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HpqBDWTegydz"},"outputs":[],"source":["!wget -nc https://download.pytorch.org/tutorial/hymenoptera_data.zip && unzip -oq hymenoptera_data.zip"]},{"cell_type":"markdown","metadata":{"id":"ZPRUi3GMhZyV"},"source":["Use [```torchvision.datasets.ImageFolder```](https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html) to load the training and validation data into a Pytorch ```Dataset```.\n","\n","Remember to transform the data into a tensor."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ClWBbBL1iNke"},"outputs":[],"source":["from torchvision import datasets, transforms, models\n","\n","transform = transforms.Compose([\n","        transforms.ToTensor(),\n","    ])\n","\n","train_ds = datasets.ImageFolder(\"./hymenoptera_data/train\", transform=transform)\n","test_ds = datasets.ImageFolder(\"./hymenoptera_data/val\", transform=transform)\n","\n","print(train_ds)\n","print(train_ds.classes)\n","print(train_ds.class_to_idx)\n","print(train_ds[0]) # an example of calling  __getitem__, which is what the dataloader does\n","print(train_ds.samples[0]) # get image path inside samples\n","print(\"\\n\\n\")\n","\n","# Get mean and std\n","tmp_loader = DataLoader(train_ds, batch_size=1, num_workers=0)\n","data = next(iter(tmp_loader))\n","mean = [torch.mean(data[0][0][i].flatten()).item() for i in range(3)]\n","std = [torch.std(data[0][0][i].flatten()).item() for i in range(3)]\n","print(mean, std)"]},{"cell_type":"markdown","metadata":{"id":"6K0ElZ-6hl8N"},"source":[" Investigate and visualize a few examples of the dataset, by plotting them, looking at their shape, etc."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6rg29zjqkb1l"},"outputs":[],"source":["def show_batch(dataset, nr=4, nc=4):\n","  fig, axarr = plt.subplots(nr, nc, figsize=(10, 10))\n","  for i in range(nr):\n","      for j in range(nc):\n","          idx = random.randint(0, len(dataset))\n","          sample, target = dataset[idx]\n","          if sample.min() < 0.:\n","            sample = (sample - sample.min())/(sample.max() - sample.min())\n","          try:\n","            axarr[i][j].imshow(sample) # if PIL\n","          except:\n","            axarr[i][j].imshow(sample.permute(1,2,0)) # if tensor of shape CHW\n","          target_name = dataset.classes[target]\n","          axarr[i][j].set_title(\"%s (%i)\"%(target_name, target))\n","\n","  fig.tight_layout(pad=1.5)\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-3IffdzCmaTQ"},"outputs":[],"source":["show_batch(train_ds, 5, 5)"]},{"cell_type":"markdown","metadata":{"id":"utc-wlRHh049"},"source":["Now, given what we know about ImageNet and the ResNet architecture, what other pre-processing is required?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xSHLIl2Vjir6"},"outputs":[],"source":["# Finally add normalisation to transforms\n","train_transform = transforms.Compose([\n","        ##,  ## add a transform that does a random crop with the right size here\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        ##,  ## normalise the dataset\n","    ])\n","test_transform = transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        ##,  ## normalise the dataset\n","    ])\n","\n","train_ds = ##  ## create a training ImageFolder\n","test_ds =  ##  ## create a testing ImageFolder\n","\n","show_batch(train_ds, 5, 5)"]},{"cell_type":"markdown","metadata":{"id":"mX98TVD3iHUq"},"source":["Finally, let's create `DataLoader`s for your transformed dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dy1Vh94WiH25"},"outputs":[],"source":["# Create dataloader\n","train_loader = ## ## create a DataLoader for the training data\n","test_loader =  ## ## create a DataLoader for the testing data"]},{"cell_type":"markdown","metadata":{"id":"zcRkTyYxxIA9"},"source":["### **2.2. Adapting training, validation and evaluation functions**\n","\n","Start by adapting the training, validation and evaluation functions to the appropriate size of inputs that the network expects.\n","\n","Before you start writing the code, consider what this size should be given what you know about ResNet."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VKoKWyIgwZQ2"},"outputs":[],"source":["def train(model, optimizer, criterion, data_loader):\n","    model.train()\n","    train_loss, train_accuracy = 0, 0\n","    for X, y in data_loader:\n","        X, y = X.to(device), y.to(device)\n","        optimizer.zero_grad()\n","        a2 = model(X.view(## adapt size here))\n","        loss = criterion(a2, y)\n","        loss.backward()\n","        train_loss += loss*X.size(0)\n","        y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n","        train_accuracy += accuracy_score(y.cpu().numpy(), y_pred.detach().cpu().numpy())*X.size(0)\n","        optimizer.step()\n","\n","    return train_loss/len(data_loader.dataset), train_accuracy/len(data_loader.dataset)\n","\n","def validate(model, criterion, data_loader):\n","    model.eval()\n","    validation_loss, validation_accuracy = 0., 0.\n","    for X, y in data_loader:\n","        with torch.no_grad():\n","            X, y = X.to(device), y.to(device)\n","            a2 = model(X.view(## adapt size here))\n","            loss = criterion(a2, y)\n","            validation_loss += loss*X.size(0)\n","            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n","            validation_accuracy += accuracy_score(y.cpu().numpy(), y_pred.cpu().numpy())*X.size(0)\n","\n","    return validation_loss/len(data_loader.dataset), validation_accuracy/len(data_loader.dataset)\n","\n","def evaluate(model, data_loader):\n","    model.eval()\n","    ys, y_preds = [], []\n","    for X, y in data_loader:\n","        with torch.no_grad():\n","            X, y = X.to(device), y.to(device)\n","            a2 = model(X.view(## adapt size here))\n","            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n","            ys.append(y.cpu().numpy())\n","            y_preds.append(y_pred.cpu().numpy())\n","\n","    return np.concatenate(y_preds, 0),  np.concatenate(ys, 0)"]},{"cell_type":"markdown","metadata":{"id":"38oqk6JAgcAH"},"source":["### **2.3. Instantiate untrained Resnet18**\n"]},{"cell_type":"markdown","metadata":{"id":"ZHSiYQSdkK-q"},"source":["Instantiate an untrained ResNet18 from [```torchvision.models```](https://pytorch.org/vision/stable/models.html). Then, reinitialise the last connected layer ```model.fc``` with a new fully connected layer with appropriate input and output sizes.\n","\n","Why is it important to replace the last layer of the network?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OjS87IIyiW4_"},"outputs":[],"source":["set_seed(seed)\n","\n","model = models.resnet18().to(device)\n","print(model)\n","model.fc = nn.Linear(model.fc.in_features, 2).to(device)  ## this line is provided, but what does it do?"]},{"cell_type":"markdown","metadata":{"id":"6Jaqp-CtkSlt"},"source":["We can now train the model using our modified train and validation loops.\n","\n","What conclusions can you extract from the training results?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dJjvFhvQkRtz"},"outputs":[],"source":["optimizer = ##  ## define your optimiser (SGD with momentum)\n","criterion = ##  ## define your loss\n","\n","liveloss = PlotLosses()\n","for epoch in range(n_epochs):\n","    logs = {}\n","    train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n","\n","    logs['' + 'log loss'] = train_loss.item()\n","    logs['' + 'accuracy'] = train_accuracy\n","\n","    validation_loss, validation_accuracy = validate(model, criterion, test_loader)\n","    logs['val_' + 'log loss'] = validation_loss.item()\n","    logs['val_' + 'accuracy'] = validation_accuracy\n","\n","    liveloss.update(logs)\n","    liveloss.draw()\n","\n","test_loss, test_accuracy = validate(model, criterion, test_loader)\n","print(\"Avg. Test Loss: %1.3f\" % test_loss.item(), \" Avg. Test Accuracy: %1.3f\" % test_accuracy)\n","print(\"\")\n","\n","model_save_name = 'resnet18_bees_and_ants_classifier_full_training_set_baseline.pt'\n","path = F\"/content/gdrive/MyDrive/models/{model_save_name}\"\n","torch.save(model.state_dict(), path)"]},{"cell_type":"markdown","metadata":{"id":"P_-rX8y8xShf"},"source":["Our model is not doing too well. The lack of data is taking a toll on our training and giving a large generalisation error."]},{"cell_type":"markdown","metadata":{"id":"xRTvtY_wzYS5"},"source":["### **2.3. Finetuning a pre-trained Resnet**\n","\n","Now, let's instantiate the pre-trained ReseNet18 by passing the argument ``pretrained=True``."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dw6iEkZDzcrX"},"outputs":[],"source":["model = models.resnet18(pretrained=True).to(device)\n","model.fc = nn.Linear(model.fc.in_features, 2).to(device)"]},{"cell_type":"markdown","metadata":{"id":"yvDqB14DnleW"},"source":["And let's perform fine-tuning using a smaller learning rate of ```1e-3```.\n","\n","What do you notice now with respect to the previous training?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gcAAw4DZnuFS"},"outputs":[],"source":["optimizer = ##  ## define your optimiser (SGD with momentum) and make sure to adapt the learning rate to 'fine-tune' the network\n","criterion = ##  ## define your loss\n","\n","liveloss = PlotLosses()\n","for epoch in range(n_epochs):\n","    logs = {}\n","    train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n","\n","    logs['' + 'log loss'] = train_loss.item()\n","    logs['' + 'accuracy'] = train_accuracy\n","\n","    validation_loss, validation_accuracy = validate(model, criterion, test_loader)\n","    logs['val_' + 'log loss'] = validation_loss.item()\n","    logs['val_' + 'accuracy'] = validation_accuracy\n","\n","    liveloss.update(logs)\n","    liveloss.draw()\n","\n","test_loss, test_accuracy = validate(model, criterion, test_loader)\n","print(\"Avg. Test Loss: %1.3f\" % test_loss.item(), \" Avg. Test Accuracy: %1.3f\" % test_accuracy)\n","print(\"\")\n","\n","model_save_name = 'resnet18_bees_and_ants_classifier_full_training_set_imagenet_finetune.pt'\n","path = F\"/content/gdrive/MyDrive/models/{model_save_name}\"\n","torch.save(model.state_dict(), path)"]},{"cell_type":"markdown","metadata":{"id":"3Li-0gq5QfKf"},"source":["This is much better now.\n","\n","We could also try to change how we normalise the dataset by using the same mean and standard deviation that the original training used for ImageNet:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DdPzPIsin2_H"},"outputs":[],"source":["train_transform = transforms.Compose([\n","        ##  ## add a transform that does a random crop with the right size here\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),  ## why are we using these numbers here?\n","    ])\n","test_transform = transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),  ## why are we using these numbers here?\n","    ])\n","\n","train_ds = datasets.ImageFolder(\"./hymenoptera_data/train\", transform=train_transform)\n","test_ds = datasets.ImageFolder(\"./hymenoptera_data/val\", transform=test_transform)\n","\n","train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0)"]},{"cell_type":"markdown","metadata":{"id":"6-ZCmCh8oECg"},"source":["Let's see how this impacts the training results.\n","\n","What conclusions can you draw from this?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w21MJEIun65V"},"outputs":[],"source":["model = models.resnet18(pretrained=True).to(device)\n","model.fc = nn.Linear(model.fc.in_features, 2).to(device)\n","\n","optimizer = ##  ## define your optimiser (SGC with momentum) and make sure to adapt the learning rate to 'fine-tune' the network\n","criterion = ##  ## define your loss\n","\n","liveloss = PlotLosses()\n","for epoch in range(n_epochs):\n","    logs = {}\n","    train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n","\n","    logs['' + 'log loss'] = train_loss.item()\n","    logs['' + 'accuracy'] = train_accuracy\n","\n","    validation_loss, validation_accuracy = validate(model, criterion, test_loader)\n","    logs['val_' + 'log loss'] = validation_loss.item()\n","    logs['val_' + 'accuracy'] = validation_accuracy\n","\n","    liveloss.update(logs)\n","    liveloss.draw()\n","\n","test_loss, test_accuracy = validate(model, criterion, test_loader)\n","print(\"Avg. Test Loss: %1.3f\" % test_loss.item(), \" Avg. Test Accuracy: %1.3f\" % test_accuracy)\n","print(\"\")\n","\n","model_save_name = 'resnet18_bees_and_ants_classifier_full_training_set_imagenet_finetune.pt'\n","path = F\"/content/gdrive/MyDrive/models/{model_save_name}\"\n","torch.save(model.state_dict(), path)"]},{"cell_type":"markdown","metadata":{"id":"0AeBUMRFj5H5"},"source":["### **2.4. Pre-trained Resnet with retrained last layer**\n","\n","Use the following provided functions:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j-e4S1a6yV-d"},"outputs":[],"source":["def set_parameter_requires_grad(model, requires_grad=False):\n","    \"\"\"https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\"\"\"\n","    for param in model.parameters():\n","        param.requires_grad = requires_grad\n","    return None\n","\n","def get_params_to_update(model):\n","    \"\"\" Returns list of model parameters that have required_grad=True\"\"\"\n","    params_to_update = []\n","    for name,param in model.named_parameters():\n","        if param.requires_grad == True:\n","            params_to_update.append(param)\n","    return params_to_update"]},{"cell_type":"markdown","metadata":{"id":"dNex84dEPD37"},"source":["Now, repeat the step above, but now freezing optimisation for all layers except the final classifying layer (instead of retraining the whole network with a smaller learning rate)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aHyiZZiCi7BE"},"outputs":[],"source":["model = ##     ## get an instance of a pre-trained resnet-18\n","##             ## use the provdied set_parameter_requires_grad to disable training\n","model.fc = ##  ## add a last layer of the network that you need. Newly initialised layers automatically have requires_grad=True"]},{"cell_type":"markdown","metadata":{"id":"hyPZFaSlojpn"},"source":["We can see the impact on the training results.\n","\n","Which of the approaches so far would be most appropriate for this dataset?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CchtcdRxojyz"},"outputs":[],"source":["optimizer = ##  ## define optimiser and use the provided get_params_to_update function to tell it to only update what you want\n","criterion = nn.CrossEntropyLoss()\n","\n","liveloss = PlotLosses()\n","for epoch in range(n_epochs):\n","    logs = {}\n","    train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n","\n","    logs['' + 'log loss'] = train_loss.item()\n","    logs['' + 'accuracy'] = train_accuracy\n","\n","    validation_loss, validation_accuracy = validate(model, criterion, test_loader)\n","    logs['val_' + 'log loss'] = validation_loss.item()\n","    logs['val_' + 'accuracy'] = validation_accuracy\n","\n","    liveloss.update(logs)\n","    liveloss.draw()\n","\n","test_loss, test_accuracy = validate(model, criterion, test_loader)\n","print(\"Avg. Test Loss: %1.3f\" % test_loss.item(), \" Avg. Test Accuracy: %1.3f\" % test_accuracy)\n","print(\"\")\n","\n","model_save_name = 'resnet18_bees_and_ants_classifier_full_training_set_imagenet_feature_extract.pt'\n","path = F\"/content/gdrive/MyDrive/models/{model_save_name}\"\n","torch.save(model.state_dict(), path)"]},{"cell_type":"markdown","metadata":{"id":"-qWGFNyJ-u2F"},"source":["If feature extraction can provide sufficiently good accuracy, we can significantly cut in training time, particularly when the network is very deep and input images are large.\n","\n","Fine tuning our model parts from the idea that the model's initial weights and biases are already quite close to the global minimum in the solution space, and that all we are doing is converging to the new minimum defined by our problem (which is close to the one for the pretrained network).\n","\n","Feature extraction makes use of the exact features that are used to classified another dataset, only really tuning the final classifying layer."]},{"cell_type":"markdown","metadata":{"id":"N5DmQlc_ebYf"},"source":["<img src=\"https://pbs.twimg.com/media/Ev-f6AaU8AgMeRd.jpg\" alt=\"drawing\" width=\"600\"/>"]},{"cell_type":"markdown","metadata":{"id":"qRArCrs3j-12"},"source":["### **2.5. What if pretrained on MNIST instead?**\n","\n","Finally, let's train a ResNet on MNIST from scratch and use those weights to repeat step 5 on the bees and ants dataset.\n","\n","Once you have done this, think whether this suitable for this problem."]},{"cell_type":"markdown","metadata":{"id":"GvQ8SKahEfqV"},"source":["Let's start by preparing the MNIST dataset to work with ResNet:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qvoZPexxj93l"},"outputs":[],"source":["train_transform = transforms.Compose([\n","        transforms.Resize(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.1307,), (0.3081,)),\n","        transforms.Lambda(lambda x: x.expand(3, 224, 224)),   # expand to 3 channels\n","    ])\n","test_transform = transforms.Compose([\n","        transforms.Resize(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.1307,), (0.3081,)),\n","        transforms.Lambda(lambda x: x.expand(3, 224, 224)),   # expand to 3 channels\n","    ])\n","\n","mnist_train = MNIST(\"./\", download=True, train=True, transform=train_transform)\n","mnist_test = MNIST(\"./\", download=True, train=False, transform=test_transform)\n","\n","mnist_train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=0)\n","mnist_test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=0)"]},{"cell_type":"markdown","metadata":{"id":"TWgFhpaJqHOC"},"source":["We can then instantiate the ResNet model again. Remember to replace the last layer of the network."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qrqIFeFJqHZt"},"outputs":[],"source":["model = models.resnet18().to(device)\n","model.fc = ##  ## add the last layer you need here\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n","criterion = nn.CrossEntropyLoss()"]},{"cell_type":"markdown","metadata":{"id":"d7Tii5ksqPyl"},"source":["And let's train the resulting model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YF2P_h5gqP7s"},"outputs":[],"source":["liveloss = PlotLosses()\n","for epoch in range(n_epochs):\n","    logs = {}\n","    train_loss, train_accuracy = train(model, optimizer, criterion, mnist_train_loader)\n","\n","    logs['' + 'log loss'] = train_loss.item()\n","    logs['' + 'accuracy'] = train_accuracy\n","\n","    validation_loss, validation_accuracy = validate(model, criterion, mnist_test_loader)\n","    logs['val_' + 'log loss'] = validation_loss.item()\n","    logs['val_' + 'accuracy'] = validation_accuracy\n","\n","    liveloss.update(logs)\n","    liveloss.draw()\n","\n","model_save_name = 'resnet18_mnist_classifier_full_training_set_baseline_.pt'\n","path = F\"/content/gdrive/MyDrive/models/{model_save_name}\"\n","torch.save(model.state_dict(), path)\n","\n","\n","test_loss, test_accuracy = validate(model, criterion, test_loader)\n","print(\"Avg. Test Loss: %1.3f\" % test_loss.item(), \" Avg. Test Accuracy: %1.3f\" % test_accuracy)\n","print(\"\")"]},{"cell_type":"markdown","metadata":{"id":"j38pm3TBExK6"},"source":["### **2.6. Transfer learning on the ResNet pretrained on MNIST**\n","\n","Now that we have trained the ResNet model on MNIST, we are going to do transfer learning to the Bees and Ants dataset as we have done before.\n","\n","Let's load the pre-trained ResNet and then replace the last layer.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"15DVykO4kaw_"},"outputs":[],"source":["model = models.resnet18().to(device)\n","model.fc = ##  ## add the last layer you need here\n","\n","path = F\"/content/gdrive/My Drive/models/{model_save_name}\"\n","model.load_state_dict(torch.load(path))\n","\n","model.fc = ##  ## add the last layer you need here"]},{"cell_type":"markdown","metadata":{"id":"34rDkwsfqsjA"},"source":["Train the model as we have done before.\n","\n","How does the network perform?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kmNY9sV6qsth"},"outputs":[],"source":["optimizer = torch.optim.SGD(model.fc.parameters(), lr=1e-3, momentum=momentum)\n","criterion = nn.CrossEntropyLoss()\n","\n","liveloss = PlotLosses()\n","for epoch in range(n_epochs):\n","    logs = {}\n","    train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n","\n","    logs['' + 'log loss'] = train_loss.item()\n","    logs['' + 'accuracy'] = train_accuracy\n","    liveloss.update(logs)\n","    liveloss.draw()\n","    logs['val_' + 'log loss'] = 0.\n","    logs['val_' + 'accuracy'] = 0.\n","\n","model_save_name = 'resnet18_bees_and_antes_classifier_full_training_set_mnist_transfer.pt'\n","path = F\"/content/gdrive/MyDrive/models/{model_save_name}\"\n","torch.save(model.state_dict(), path)\n","\n","\n","test_loss, test_accuracy = validate(model, criterion, test_loader)\n","print(\"Avg. Test Loss: %1.3f\" % test_loss.item(), \" Avg. Test Accuracy: %1.3f\" % test_accuracy)\n","print(\"\")"]},{"cell_type":"markdown","metadata":{"id":"aWu5w9zJB5oa"},"source":["<br>\n","<br>\n","<br>\n","<br>\n","\n","---\n","---\n","\n","<br>\n","\n","## **Clarifying issues with loss averaging:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M66z-bLYB-X9"},"outputs":[],"source":["def train(model, optimizer, criterion, data_loader):\n","    model.train()\n","    train_loss, train_accuracy = 0, 0\n","    for X, y in data_loader:\n","        X, y = X.to(device), y.to(device)\n","        optimizer.zero_grad()\n","        a2 = model(X.view(-1, 3, 224, 224))\n","        loss = criterion(a2, y)\n","        loss.backward()\n","        train_loss += loss*X.size(0) ## this undoes normalisation\n","        y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n","        train_accuracy += accuracy_score(y.cpu().numpy(), y_pred.detach().cpu().numpy())*X.size(0)\n","        optimizer.step()\n","\n","    return train_loss/len(data_loader.dataset), train_accuracy/len(data_loader.dataset)"]},{"cell_type":"markdown","metadata":{"id":"uAuNMG7tCTVQ"},"source":["The cross entropy loss provided by PyTorch has a normalisation added to it in [`CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html). This line:\n","\n","`train_loss += loss*X.size(0)`\n","\n","undoes the normalisation done by `torch.nn.CrossEntropyLoss` and adds the value to the accumulated losses over minibatches.\n","\n","Then, once we have gone over all the data (that is, over all the minibatches), we will have completed one epoch, and we want to know the value of the normalised loss. To get it, we divide it by the total number of samples in my data:\n","\n","`return train_loss/len(data_loader.dataset), train_accuracy/len(data_loader.dataset)`\n","\n","A similar business is going on with the accuracy. In this case we use scikit-learn's [`accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html). Then, we do:\n","\n","`train_accuracy += accuracy_score(y.cpu().numpy(), y_pred.detach().cpu().numpy())*X.size(0)`\n","\n","and then we divide the accuracy by the total length of the data.\n","\n","<br>\n","<br>\n","\n","Finally, this line before the data_loader is iterated:\n","\n","`train_loss, train_accuracy = 0, 0`\n","\n","resets the value of the loss value at every epoch.\n","\n","\n","## Alternative implementations:\n","\n","Simply modify the default behaviour of the loss and the accuracy calculations:\n","\n","`torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=- 100, reduce=None, `**`reduction='mean'`**`, label_smoothing=0.0)`\n","\n","change default to **`reduction='sum'`** when you define the loss.\n","\n","<br>\n","\n","And for the accuracy, just change the line:\n","\n","`train_accuracy += accuracy_score(y.cpu().numpy(), y_pred.detach().cpu().numpy(),`**`normalize=False`**`)`\n","\n","to accumulate the sum of the loss, so it is naturally scaled to the batch size.\n","\n","\n","<br>\n","\n","---\n","\n","<br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zyp8XnabM7gb"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"A100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}