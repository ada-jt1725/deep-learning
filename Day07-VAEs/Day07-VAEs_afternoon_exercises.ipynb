{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b619b9b",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/thumbnail?id=1cXtXdAcwedVDbapmz1pj_hULsQrhEcff&sz=w1500\" width=\"500\"/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06860585",
   "metadata": {},
   "source": [
    "# **Variational AutoEncoders (VAEs)**\n",
    "\n",
    "#### **Morning contents/agenda**\n",
    "\n",
    "1. AutoEncoders\n",
    "\n",
    "2. Variational AutoEncoders\n",
    "\n",
    "3. Other VAE architectures\n",
    "\n",
    "4. A simple VAE demo\n",
    "\n",
    "#### **Learning outcomes**\n",
    "\n",
    "1. Understand how Autoencoders can be used for data dimensionality reduction\n",
    "\n",
    "2. Gain intuition about what a latent space is and how the latent space of Autoencoders and VAEs differ\n",
    "\n",
    "3. Understand how the reparametrization trick makes VAEs trainable\n",
    "\n",
    "4. Differentiate the role of the KL Divergence and reconstruction fidelity terms in the loss function of VAEs\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **Afternoon contents/agenda**\n",
    "\n",
    "1. Load the FashionMNIST dataset\n",
    "\n",
    "2. Implement a convolutional VAE\n",
    "\n",
    "3. Implement a conditioned convolutional VAE\n",
    "\n",
    "#### **Learning outcomes**\n",
    "\n",
    "1. Practice implementation VAEs\n",
    "\n",
    "2. Understand how to refactor a linear VAE into a convolutional VAE\n",
    "\n",
    "3. Learn how to perform class-conditioned generation\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864ed6e8",
   "metadata": {},
   "source": [
    "### **Conditional VAEs**\n",
    "\n",
    "There is one type of VAE that is of special interest in generative problems: **Conditional Variational AutoEncoders (cVAEs)**.\n",
    "\n",
    "We have been looking at how VAEs can learn latent representations of a dataset, which can then be used to generate new, original data points. However, what happens if we want to constrain the decoder to produce examples within a certain category?\n",
    "\n",
    "cVAEs address this by introducing an additional conditional input $c$ to both the encoder and the decoder:\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://drive.google.com/thumbnail?id=1CuNqPhxOCkkwTN7HtIInKo7o3JsppUic&sz=w1500\" width=\"600\"/>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "At generation time, we may simply pass a class of our choosing along with the random vector, and condition our generation.\n",
    "\n",
    "In this afternoon practical we will look at how to implement these conditional VAEs in PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae234d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839f1718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
    "    torch.backends.cudnn.enabled   = False\n",
    "\n",
    "    return True\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
    "    print(\"Cuda installed! Running on GPU!\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"No GPU available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be10298",
   "metadata": {},
   "source": [
    "## 1. Loading and Visualising the Data\n",
    "\n",
    "Tasks\n",
    "\n",
    "\n",
    "- Define a preprocessing transform using `transforms.ToTensor()` so that images are converted to `PyTorch` tensors in the `[0, 1]` range.\n",
    "\n",
    "- Download and load the training split of `FashionMNIST` from `torchvision.datasets`, applying your transform.\n",
    "\n",
    "- Use `StratifiedShuffleSplit` to create an `80/20` stratified split of the dataset indices, using the class labels to preserve class balance between training and validation sets.\n",
    "\n",
    "- Construct `torch.utils.data.Subset` objects for the training and validation subsets, and wrap each one in a `DataLoader` with `batch_size=128`.\n",
    "\n",
    "- Draw a batch from the training dataloader and select the first 32 images to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef691a83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c944360",
   "metadata": {},
   "source": [
    "## 2. Training Helper Functions\n",
    "\n",
    "\n",
    "Write the `train` and `validation` functions that will handle optimisation of your VAE. They follow the same structure we used in class, but try to re-write them yourself rather than copying and pasting. This is a good chance to check that you really understand what each step in the training loop is doing, from computing the loss to updating the weights and keeping track of performance. Can you write the `ELBO` loss as its own ``nn.Module`` class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb8ecd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5f657b7",
   "metadata": {},
   "source": [
    "## 3. Convolutional VAE\n",
    "Now let's refactor our linear VAE into a convolutional VAE. Like in class, you will build an encoder, a decoder, and a VAE wrapper, and then check that the tensor shapes line up correctly.\n",
    "\n",
    "Tasks\n",
    "- Implement a `ConvEncoder` that:\n",
    "    - Takes `input_channels` and `latent_size` as arguments.\n",
    "\n",
    "    - Uses a `nn.Sequential` of convolution → batch norm → GELU → max-pooling blocks to map an input image to a compact feature map. Use three blocks with channel sizes 20 → 40 → 60, kernel size 3, appropriate padding to preserve the shape, and `MaxPool2d(2)` to downsample from 32\\times32 to 4\\times4.\n",
    "\n",
    "    - Flattens the final feature map (of size 60\\times4\\times4) and passes it through two linear layers to output `mu` and `logvar`, each of dimension `latent_size`.\n",
    "\n",
    "- Implement a ``ConvDecoder`` that:\n",
    "    - First maps a latent vector of size latent_size back to a flattened feature map using a linear layer, then reshapes it to (60, 4, 4), padding it with 3 so that the effective input size is 32x32\n",
    "\n",
    "    - Uses a `nn.Sequential` of convolution → batch norm → GELU → nearest-neighbour upsampling blocks to progressively upsample back to the image resolution. Mirror the encoder with channel sizes 60 → 60 → 40 → 20 → `input_channels`, and use `Upsample(scale_factor=2, mode=\"nearest\")` between blocks. If you wish, you can also write your decoder with `ConvTranspose2d`\n",
    "\n",
    "    - Ends with a Sigmoid activation to produce outputs in [0, 1], and crops the borders so the final output has the same spatial size as the original input.\n",
    "\n",
    "- Implement a `ConvVAE` that:\n",
    "    - Composes the `ConvEncoder` and `ConvDecoder` in the initialisation.\n",
    "    - Includes a `sample(mu, logvar)` function that applies the reparameterisation trick $z = \\mu + \\epsilon\\, \\sigma$ with $\\epsilon \\sim \\mathcal{N}(0, I)$.\n",
    "    - In `forward(x)`, encodes x to `mu`, `logvar`, samples `z`, and returns the reconstructed image together with `mu` and `logvar`.\n",
    "\n",
    "- Test your implementation by:\n",
    "    - Instantiating ConvEncoder, ConvDecoder, and ConvVAE with input_channels=1 and latent_size=128.\n",
    "\n",
    "    - Passing a mini-batch from your FashionMNIST dataloader through each component and printing the output shapes to confirm they match the expected dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4807945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fdcfa23",
   "metadata": {},
   "source": [
    "## 4. Train your ConvVAE\n",
    "Tasks\n",
    "- Train the `ConvVAE` for 10 epochs using latent size = 32, an Adam optimiser with learning rate $1\\times10^{-3}$, and an ELBO loss with $\\beta = 1$. Track both training and validation losses throughout.\n",
    "\n",
    "- After training, take a validation batch and plot the first 32 original images next to their reconstructions to assess reconstruction quality.\n",
    "\n",
    "- Sample 32 latent vectors from a standard normal distribution, decode them, and plot the resulting generated images to inspect the model’s generative behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0e10a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccdad166",
   "metadata": {},
   "source": [
    "## 5. Expanding our ConVAE with Class-Conditioning\n",
    "\n",
    "In a conditional VAE, additional information such as class labels is incorporated into the model so that both the encoder and decoder operate with awareness of the conditioning variable. This is usually achieved by concatenating the label (or a learned embedding of it) to the encoder input and the latent vector, or by injecting the conditioning value directly into these inputs so that the latent space becomes class-aware during training. For this task, we will implement conditioning via addition. This means that we will “add” the value of our label to our input. But before we do that, we must project the one-hot labels through a linear layer so that they lie in the same dimensional space as the corresponding input: one projection matching the spatial dimensions of the encoder input, and another matching the dimensionality of the latent space so that conditioning can also be applied within the decoder. One fun outcome of this is that our model is also learning the \"best\" projection during optimisation!\n",
    "\n",
    "\n",
    "Tasks\n",
    "\n",
    "\n",
    "- Write a function that converts a batch of integer class labels into one-hot vectors of dimension 10, ensuring the tensor is created on the same device as the labels. \n",
    "\n",
    "- Rewrite the convolutional encoder and decoder from the previous task so that each includes a linear layer mapping a one-hot vector of 10 classes to the appropriate dimension $1\\times 28\\times 28$ for the encoder input and the latent size for the decoder. Then add these embeddings to the inputs before forwarding through the network and adjust your final convolutional VAE accordingly. In this step, you may find it easier and more succinct to *inherit* from your previously implemented classes.\n",
    "\n",
    "- Adjust your `train` and `validate` functions accordingly to use those class labels.\n",
    "\n",
    "- Train your conditional VAE with the same hyperparameters from Question #4\n",
    "\n",
    "- Generate 10 samples for each class by drawing random latent vectors, assigning a fixed label to each group, decoding them with the conditioned decoder, and plotting a row of images per label to inspect how well the model controls class-specific generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c870a67c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
