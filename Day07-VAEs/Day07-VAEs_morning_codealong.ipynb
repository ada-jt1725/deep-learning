{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YehS8enAmDn"
   },
   "source": [
    "<img src=\"https://drive.google.com/thumbnail?id=1cXtXdAcwedVDbapmz1pj_hULsQrhEcff&sz=w1500\" width=\"500\"/>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__g36CjU8CUi"
   },
   "source": [
    "# **Variational AutoEncoders (VAEs)**\n",
    "\n",
    "#### **Morning contents/agenda**\n",
    "\n",
    "1. AutoEncoders\n",
    "\n",
    "2. Variational AutoEncoders\n",
    "\n",
    "3. Other VAE architectures\n",
    "\n",
    "4. A simple VAE demo\n",
    "\n",
    "#### **Learning outcomes**\n",
    "\n",
    "1. Understand how Autoencoders can be used for data dimensionality reduction\n",
    "\n",
    "2. Gain intuition about what a latent space is and how the latent space of Autoencoders and VAEs differ\n",
    "\n",
    "3. Understand how the reparametrization trick makes VAEs trainable\n",
    "\n",
    "4. Differentiate the role of the KL Divergence and reconstruction fidelity terms in the loss function of VAEs\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **Afternoon contents/agenda**\n",
    "\n",
    "1. Load the FashionMNIST dataset\n",
    "\n",
    "2. Implement a convolutional VAE\n",
    "\n",
    "3. Implement a conditioned convolutional VAE\n",
    "\n",
    "#### **Learning outcomes**\n",
    "\n",
    "1. Practice implementation VAEs\n",
    "\n",
    "2. Understand how to refactor a linear VAE into a convolutional VAE\n",
    "\n",
    "3. Learn how to perform class-conditioned generation\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rational of Generative Models\n",
    "---\n",
    "\n",
    "Generative models are a class of unsupervised models that generate new data that is similar to the data in the training set. They are used in a variety of applications, including image and video generation, natural language processing, and music generation.\n",
    "\n",
    "The key goal of generative models is to learn the underlying probability distribution of the data $p(x)$ of a dataset $X$ so we can draw samples from it. That is the goal of any generative model, including statistical models that do not rely on deep learning.\n",
    "\n",
    "<br>\n",
    "\n",
    "<p align = \"center\"><img src=\"https://drive.google.com/thumbnail?id=13yVxGyVbefU6on-QFtgS8PHHOM-CnIdE&sz=w1000\" width=\"300\"/></p><p align = \"center\">\n",
    "<i> <a href=\"https://openai.com/blog/generative-models/\">image source</a></i>\n",
    "</p>\n",
    "\n",
    "<!-- <p align = \"center\"><img src=\"https://drive.google.com/uc?id=13yVxGyVbefU6on-QFtgS8PHHOM-CnIdE\" width=\"300\"/></p><p align = \"center\">\n",
    "<i> <a href=\"https://openai.com/blog/generative-models/\">image source</a></i>\n",
    "</p> -->\n",
    "\n",
    "\n",
    "\n",
    "In the expressiveness of deep learning, however, we are able to model much more complex distributions than with traditional statistical models.\n",
    "\n",
    "Let a neural network $G$ be a mapping function from a variable $z$ to data space, parameterised by $\\theta$.\n",
    "\n",
    "$$\n",
    "G(z;\\theta) \\rightarrow x \\sim \\hat{p}(x),\n",
    "$$\n",
    "\n",
    "where $z$ is a random variable from a prior distribution $p(z)$ that we can compute, $x$ is the generated data, $\\hat{p}(x)$ is the probability density function of the generated data.\n",
    "\n",
    "If we want our generated data to be as close as possible to the real data, then we must find the parameters $\\theta$ of the model $G$ that satisfies $\\hat{p}(x) \\approx p(x)$ as best as possible.\n",
    "\n",
    "<p align = \"center\"><img src=\"https://images.openai.com/blob/1c914c57-32d4-415a-9a2f-53282fd20a32/gen_models_diag_2.svg\" width=\"800\"/></p><p align = \"center\">\n",
    "<i> <a href=\"https://openai.com/blog/generative-models/\">image source</a></i>\n",
    "</p>\n",
    "\n",
    "\n",
    "We will see in the next few lectures how different architectures try to achieve this goal from different perspectives.\n",
    "\n",
    "Today we focus on Variational Autoencoders.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5CluZ9CZtgV"
   },
   "source": [
    "## 1. AutoEncoders\n",
    "\n",
    "Before we dive into **Variational** Autoencoders, we need to first look briefly at what an Autoencoder is.\n",
    "\n",
    "**Autoencoders** are an unsupervised learning architecture whose goal is to learn how to compress (or encode) input data into a compact, low-dimensional representation; and how to then reconstruct (or decode) the original input data from this compact representation. Autoencoders, then, are a form of dimensionality reduction (like PCA) that uses neural networks to learn how *best* to compress data. They were originally introduced through a series of contributions during the 1990s like [this one from Mark Kramer](https://aiche.onlinelibrary.wiley.com/doi/epdf/10.1002/aic.690370209) and [this one from Geoffrey Hinton](https://proceedings.neurips.cc/paper/1993/file/9e3cfc48eccf81a0d57663e129aef3cb-Paper.pdf).\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://www.ibm.com/content/dam/connectedassets-adobe-cms/worldwide-content/creative-assets/s-migr/ul/g/b6/f4/variational-autoencoder-neural-network.png\" width=\"600\"/>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "Autoencoders consist of an **encoder**, which transforms an input into a compressed representation, and a **decoder**, which uses a compressed representation to reconstruct the original input. By progressively decreasing the dimensionality of the layers in the encoder towards a **bottleneck**, we force the network to learn compact representations.\n",
    "\n",
    "Each dimension of the autoencoder's compressed representations can be seen as a **latent variable**, an underlying property of the data that the network has inferred. The set of latent variables of a specific dataset is known as the **latent space**. The compressed representation of a specific input within the dataset is called a **latent vector**. If we had trained an Autoencoder using people's faces, our latent space might look like this:\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://drive.google.com/thumbnail?id=1aRCxIuSg-K7lSUujvj_lLirUg0fi6b8z&sz=w1500\" width=\"800\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "The latent compression in the image above readily interpretable to us. We can very well describe the image by measuring how much the person's face is smiling, what their hair color is, how much beard they have, etc. These are the latent variables of the dataset.\n",
    "\n",
    "In reality, as we train the network, the latent space contains **latent variables** that are not immediately interpretable to us. We can only describe the latent space in terms of the **variables that the network has learned to extract**.\n",
    "\n",
    "<br>\n",
    "\n",
    "To train an autoencoder that takes a data input $x_i$, encodes it, and decodes $\\hat x_i$, we measure the quality of the reconstruction with respect to the input. Multiple algorithms, including cross-entropy loss and MSE can be used for this.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
    "    torch.backends.cudnn.enabled   = False\n",
    "\n",
    "    return True\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
    "    print(\"Cuda installed! Running on GPU!\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"No GPU available!\")\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_size=2):\n",
    "        super().__init__()\n",
    "        self.latent_size = ###\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(###),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(###),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(###)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(###), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(###),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(###),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        ####     # z is the latent space, which we pass to the decoder\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        ######  # x is the reconstructed image\n",
    "        x = x.view(x.size(0), 1, 28, 28)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ####\n",
    "        return x\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "dataset = torchvision.datasets.MNIST(root='data', train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "\n",
    "# Split dataset into train and validation sets\n",
    "stratified_split = StratifiedShuffleSplit(train_size=0.8, test_size=0.2)\n",
    "train_idxs, valid_idxs = list(stratified_split.split(dataset.data, dataset.targets))[0]\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_idxs)\n",
    "valid_dataset = torch.utils.data.Subset(dataset, valid_idxs)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=0)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=128, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(dataloader):\n",
    "        data = data.to(device)\n",
    "        ####\n",
    "        ####\n",
    "        #### # we train our model to reconstruct the data exactly\n",
    "        ####\n",
    "        ####\n",
    "        train_loss += loss.item() * len(data)\n",
    "    return train_loss / len(dataloader.dataset)\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in dataloader:\n",
    "            data = data.to(device)\n",
    "            ###\n",
    "            ###\n",
    "            valid_loss += loss.item() * len(data)\n",
    "    return valid_loss / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and optimizer\n",
    "set_seed(42)\n",
    "model = MNISTAutoencoder().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "# Reconstruction loss\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "train_losses, valid_losses = [], []\n",
    "for epoch in tqdm(range(15)):\n",
    "    # Train and Validate\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    valid_loss = validate(model, valid_loader, criterion, device)\n",
    "    \n",
    "    # Log losses\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    # Clear and update plot\n",
    "    clear_output(wait=True)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "    ax.plot(train_losses, label='Training Loss')\n",
    "    ax.plot(valid_losses, label='Validation Loss')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Training Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise the reconstruction of the images for a validation batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, _ = ###\n",
    "###\n",
    "###\n",
    "\n",
    "# Make grid of original and reconstructed images\n",
    "data_grid = make_grid(data, nrow=8, padding=2, normalize=True)[0]\n",
    "recon_grid = make_grid(recon, nrow=8, padding=2, normalize=True)[0]\n",
    "\n",
    "# Plot original and reconstructed images\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].imshow(data_grid.cpu().numpy(), cmap='gray')\n",
    "axs[1].imshow(recon_grid.detach().cpu().numpy(), cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not an exceptional result, but it's a start. Possibly better models and more expressive latent spaces (i.e. more than 2 dimensions) would yield better results.\n",
    "\n",
    "But since our latent space is 2D, we can visualize it. First, let's encode the dataset into their **latent vectors** so we can construct our **latent space**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = []\n",
    "labels = []\n",
    "\n",
    "model.eval()\n",
    "####\n",
    "\n",
    "\n",
    "# stack latents\n",
    "latents = torch.cat(latents, dim=0).cpu().numpy()\n",
    "labels = torch.cat(labels, dim=0).cpu().numpy()\n",
    "\n",
    "print(latents.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample is now represented by a quantity of a latent variable 1 and another quantity of a latent variable 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot latents with discrete cmap\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 7))\n",
    "im = ax.scatter(latents[:, 0], latents[:, 1], c=labels, cmap=plt.cm.Set3, s=5)\n",
    "ax.set_xlabel('Latent Variable 1')  \n",
    "ax.set_ylabel('Latent Variable 2')  \n",
    "ax.set_title('Latent Space Visualization')\n",
    "plt.colorbar(im, ax=ax, label='Digit Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model has learned to represent the data in a way that the classes are somewhat disentangled, although not perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Navigating the latent space\n",
    "\n",
    "With a trained network, we can navigate the latent space by interpolating between two or more latent vectors. Let's have a look at how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, _ = next(iter(valid_loader))\n",
    "sample_A, sample_B = samples[8].unsqueeze(0), samples[10].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A very simple interpolation/blending function\n",
    "def interpolate(x, y, alpha):\n",
    "    return ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: interpolate between samples\n",
    "sample_interpolated = ###\n",
    "\n",
    "# Visualize results\n",
    "fig, ax = plt.subplots(1, 3, figsize=(6, 2))\n",
    "ax[0].imshow(sample_A[0,0].cpu().numpy(), cmap='gray')\n",
    "ax[1].imshow(sample_interpolated[0,0].cpu().numpy(), cmap='gray')\n",
    "ax[2].imshow(sample_B[0,0].cpu().numpy(), cmap='gray')\n",
    "ax[0].set_title('Sample A')\n",
    "ax[1].set_title('Interpolated Sample')\n",
    "ax[2].set_title('Sample B')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does not look like much. In the latent space this is a bit more interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Get latent representations of sample A and B\n",
    "    latent_A = ###\n",
    "    latent_B = ###\n",
    "\n",
    "    # Interpolate between latents\n",
    "    latent_interpolated = ###\n",
    "\n",
    "    # Decode interpolated latent\n",
    "    recon_interpolated = ###\n",
    "\n",
    "# Visualize results\n",
    "fig, ax = plt.subplots(1, 3, figsize=(6, 2))\n",
    "ax[0].imshow(sample_A[0,0].cpu().numpy(), cmap='gray')\n",
    "ax[1].imshow(recon_interpolated[0,0].cpu().numpy(), cmap='gray')\n",
    "ax[2].imshow(sample_B[0,0].cpu().numpy(), cmap='gray')\n",
    "ax[0].set_title('Sample A')\n",
    "ax[1].set_title('Latent Interpolated Sample')\n",
    "ax[2].set_title('Sample B')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reconstruction of the interpolated latent variable does not exist in our dataset and does not correspond to any real image. But its reconstruction does seem to belong to the distribution of our data. \n",
    "\n",
    "We have succesfully generated our first data sample!\n",
    "\n",
    "But what exactly is happening here? When we navigate the somewhat disentangled latent space of our model, the path between class X and Y may continuously cross class Z. This is a hint that our model is learning something *meaningful* about the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 7))\n",
    "im = ax.scatter(latents[:, 0], latents[:, 1], c=labels, cmap=plt.cm.Set3, s=5)\n",
    "ax.scatter(latent_A[:, 0].cpu(), latent_A[:, 1].cpu(), c='r', s=20)\n",
    "ax.scatter(latent_B[:, 0].cpu(), latent_B[:, 1].cpu(), c='r', s=20)\n",
    "ax.scatter(latent_interpolated[:, 0].cpu(), latent_interpolated[:, 1].cpu(), c='g', s=20)\n",
    "ax.annotate(r'$S_A$', (latent_A[:, 0].cpu(), latent_A[:, 1].cpu()), fontsize=12)\n",
    "ax.annotate(r'$S_B$', (latent_B[:, 0].cpu(), latent_B[:, 1].cpu()), fontsize=12)\n",
    "ax.annotate(r'$S_{int}$', (latent_interpolated[:, 0].cpu(), latent_interpolated[:, 1].cpu()), fontsize=12)\n",
    "ax.set_xlabel('Latent Feature 1')\n",
    "ax.set_ylabel('Latent Feature 2')  \n",
    "ax.set_title('Latent Space Visualization')\n",
    "plt.colorbar(im, ax=ax, label='Digit Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instead of interpolating between vectors, what if we just randomly sample from the latent space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_latent = ###\n",
    "random_reconstruction = ###\n",
    "\n",
    "# Make grid of random reconstructions and plot\n",
    "random_grid = make_grid(random_reconstruction)[0]\n",
    "plt.imshow(random_grid, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just generated our first batch of synthetic data, defined only by the random state of our latent vectors and the weights of our model.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Variational AutoEncoders\n",
    "\n",
    "Although Autoencoders can be used for generation, they are not the best choice for this task. Here's why:\n",
    "\n",
    "<!-- ### **Intuitive motivation**\n",
    "\n",
    "**Variational Autoencoders (VAEs)** share a similar architecture with Autoencoders, but their goal is not to perform dimensionality reduction. Instead, they learn how to generate new samples that resemble those in the training dataset. They do this by sampling from the latent space of the dataset.\n",
    "\n",
    "The question is: **could we use an Autoencoder to do this?** Let's look at the 2D latent space of an Autoencoder trained on MNIST: -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pick a vector in the \"void\" of the latent space\n",
    "z = ###\n",
    "recon = ###\n",
    "\n",
    "\n",
    "# Plot the latent vector in the latent space\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 7))\n",
    "im = ax.scatter(latents[:, 0], latents[:, 1], c=labels, cmap=plt.cm.Set3, s=5)\n",
    "ax.scatter(z[:, 0].cpu(), z[:, 1].cpu(), c='r', s=20)\n",
    "ax.annotate(r'$?$', (z[:, 0].cpu(), z[:, 1].cpu()), fontsize=16)\n",
    "ax.set_xlabel('Latent Feature 1')\n",
    "ax.set_ylabel('Latent Feature 2')  \n",
    "ax.set_title('Latent Space Visualization')\n",
    "plt.colorbar(im, ax=ax, label='Digit Class')\n",
    "plt.show()\n",
    "\n",
    "# Plot the reconstruction\n",
    "recon = recon.cpu().detach().numpy()\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
    "plt.imshow(recon[0].reshape(28, 28), cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that sampling from this latent space is be problematic unless remain very close to the training set. Having this constraint would significantly limit our generative freedom.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uw7QKqdKcwtR"
   },
   "source": [
    "<!-- \n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*-i8cp3ry4XS-05OWPAJLPg.png\" width=\"400\"/>\n",
    "</center>\n",
    "\n",
    "<br> -->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This is where **Variational Autoencoders (VAEs)** come in. \n",
    "\n",
    "VAEs share the same architecture as Autoencoders, but with a few key differences that explicitly enforce data modelling.\n",
    "\n",
    "They transform the discrete latent space of autoencoders into a **probabilistic latent space**, where each feature in the latent vector will be a probability distribution:\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://drive.google.com/thumbnail?id=1feye1FElxPRhTI0vspSfODltXUJOFkr1&sz=w1500\" width=\"800\"/>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "Intuitively, this means that we are forcing each sample to be described by a range of possible latent values, instead of a single one.\n",
    "\n",
    "The decoder will then randomly sample from each latent probability distributions and reconstruct the input.\n",
    "\n",
    "Because sampling from distributions is a stochastic process, every draw introduces intrinsic randomness into the reconstruction process.\n",
    "\n",
    "So, for **any sample drawn from the latent distribution** of a given input, we drive the decoder to yield an accurate reconstruction, which in turn enforces that neighbouring latent vectors **generate highly similar outputs**.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://www.jeremyjordan.me/content/images/2018/06/Screen-Shot-2018-06-20-at-2.48.42-PM.png\" width=\"800\"/>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "The result of this enforcement is a latent space that is **smooth, continous and well-structured**, one that we can more freely and confidently navigate. \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:786/format:webp/1*BIDBG8MQ9-Kc-knUUrkT3A.png\" width=\"400\"/>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "Variational Autoencoders were first introduced in [this 2013 paper](https://arxiv.org/abs/1312.6114), where it was applied to MNIST digits and people's faces:\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://drive.google.com/thumbnail?id=1GZLWSscyJr05IPvLFzmzQT_iq-_Lwqi_&sz=w1500\" width=\"1000\"/>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Encoder**\n",
    "\n",
    "Now that we understand the intuition behind VAEs, how do we implement a probabilistic latent space?\n",
    "\n",
    "A probability distribution is defined by a series of parameters. For the normal distribution, for example, these are the mean and variance. We will then modify the bottleneck of the previous Autoencoders so that we can generate a set of parameters for every dimension in the latent space.\n",
    "\n",
    "Let's say our latent space has two dimensions that are parametrised as Gaussian. Our network would then look similar to this:\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://www.jeremyjordan.me/content/images/2018/03/Screen-Shot-2018-03-18-at-12.24.19-AM.png\" width=\"800\"/>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "For each of our two latent dimensions we now have two parameters: a mean and a variance. These are grouped together into a mean latent vector and a variance latent vector. Here, we assume that the dimensions of our latent space are completely uncorrelated with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_size=2):\n",
    "        super().__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        # Here our hidden layers split into two: mu and logvar\n",
    "        # We use logvar instead of var because it is more flexible and numerically stable\n",
    "        # https://stats.stackexchange.com/questions/353220/why-in-variational-auto-encoder-gaussian-variational-family-we-model-log-sig\n",
    "        self.mu_layer = ###\n",
    "        self.logvar_layer = ###\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = ###\n",
    "        x = ###\n",
    "        # Do we need activation here?\n",
    "        mu = ###\n",
    "        logvar = ###\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Decoder**\n",
    "\n",
    "That covers the encoding process, but what happens with the decoder? How do we sample from these probability distributions?\n",
    "\n",
    "Recall that we are looking for a sample $z \\sim q(z | x)$, that belongs to a Gaussian distribution defined by the parameters $\\mu$ and $\\sigma$. \n",
    "\n",
    "We could simply draw random samples from the latent probability distributions using any standard routine for generating Gaussian noise. However, such standard routines rely on non-differentiable stochastic steps, which prevents backpropagation.\n",
    "\n",
    "A vector of random values, by definition, has no derivative, and so we cannot backpropagate through it.\n",
    "\n",
    "Fortunately, we can use something called the \"reparametrization trick\":\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://www.jeremyjordan.me/content/images/2018/03/Screen-Shot-2018-03-18-at-4.36.34-PM.png\" width=\"800\"/>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "Now we do not need to backpropagate through the random vector!\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://www.jeremyjordan.me/content/images/2018/03/Screen-Shot-2018-03-18-at-4.39.41-PM.png\" width=\"800\"/>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's now write our decoder and the complete VAE model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 256)\n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.fc3 = nn.Linear(512, 784)\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, z):\n",
    "        x = ###\n",
    "        x = ###\n",
    "\n",
    "        # We use a sigmoid activation for the final layer\n",
    "        # to ensure that the output is always in the range [0, 1]\n",
    "        x = ###\n",
    "        return x.view(-1, 1, 28, 28)\n",
    "\n",
    "\n",
    "class MNISTVAE(nn.Module):\n",
    "    def __init__(self, latent_size=2):\n",
    "        super().__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.encode = ###\n",
    "        self.decode = ###\n",
    "\n",
    "    def sample(self, mu, log_var):\n",
    "        \"\"\" This method applies the reparameterization trick \"\"\"\n",
    "        ###\n",
    "        ###\n",
    "        return ###\n",
    "\n",
    "    def forward(self, x):\n",
    "        ###\n",
    "        ###\n",
    "        return ###   # return the decoded image, the mean and the log variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    " \n",
    "### **Loss function**\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://drive.google.com/thumbnail?id=1kz-B9iKAWLBghJ4nrsNZrg2pqfFy-yT6&sz=w1500\" width=\"1000\"/>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "The loss function of the VAE contains two terms:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{VAE} = \\mathcal{L}_{recon}(x_i,  x_i') + \\sum_j KL \\, (q_{\\theta}^j(z|x) \\, || \\, p(z)),\n",
    "$$\n",
    "\n",
    "where $KL(q_{\\theta}(z|x) || p(z))$ is the Kullback-Leibler divergence between the encoder distribution $q_{\\theta}(z|x)$ and the prior $p(z)$ of our choice. Usualy we use the unit Gaussian as prior because it is a simple distribution with nice mathematical properties.\n",
    "\n",
    "The first term is, like with Autoencoders, a measure of reconstruction error between the input of the encoder $x_i$ and the output of the decoder $\\hat x_i$. This could be, for example, the MSE loss.\n",
    "\n",
    "The second term acts as a **regulariser**, which encourages the distributions that the VAE learns to remain close to a prior probability distribution $p(z)$ for every dimension $j$ of the latent space. The intution is this:\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://www.jeremyjordan.me/content/images/2018/06/Screen-Shot-2018-06-20-at-2.51.06-PM.png\" width=\"800\"/>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Recall from yesterday's lecture that a special closed-form case of the KL Divergence occurs between two Gaussian distributions. If one of the distributions is a unit Gaussian ($\\mu = 0$, $\\sigma = 1$), then the closed-form is further simplified to:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "KL \\, (q_{\\theta}(z|x) \\, || \\, p(z)  ) = - \\frac{1}{2} \\sum_j (1 + \\log \\sigma_j^2 - \\mu_j^2 - \\sigma_j^2)\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Since we model our latent variables as Gaussian distributions, we can use this closed-form expression to calculate the KL Divergence between them and the unit Gaussian we choose as our prior.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(mean, log_var):\n",
    "    return ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have everything we need to train our VAE. First, we need to modify our training and validation functions. We can also add a KL divergence weight to balance the reconstruction and KL divergence losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device, klw):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(dataloader):\n",
    "        data = data.to(device)\n",
    "        ###\n",
    "        ###\n",
    "        loss = criterion(recon, data) +  # add KL divergence to reconstruction loss\n",
    "        ###\n",
    "        ###\n",
    "        train_loss += loss.item() * len(data)\n",
    "    return train_loss / len(dataloader.dataset)\n",
    "\n",
    "def validate(model, dataloader, criterion, device, klw):\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in dataloader:\n",
    "            data = data.to(device)\n",
    "            ###\n",
    "            loss = criterion(recon, data) + ###\n",
    "            valid_loss += loss.item() * len(data)\n",
    "    return valid_loss / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything else is the same as we did in the Autoencoder example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and optimizer\n",
    "set_seed(42)\n",
    "model = MNISTVAE().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "kl_weight = 1.0\n",
    "\n",
    "# Reconstruction loss\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Training loop\n",
    "train_losses, valid_losses = [], []\n",
    "for epoch in tqdm(range(15)):\n",
    "    # Train and Validate\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, device, kl_weight)\n",
    "    valid_loss = validate(model, valid_loader, criterion, device, kl_weight)\n",
    "\n",
    "    # Append to lists\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    # Clear and update plot\n",
    "    clear_output(wait=True)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "    ax.plot(train_losses, label='Training Loss')\n",
    "    ax.plot(valid_losses, label='Validation Loss')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Training Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = []\n",
    "labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data, label in tqdm(valid_loader):\n",
    "        data = data.to(device)\n",
    "        ###\n",
    "\n",
    "# stack latents\n",
    "latents = torch.cat(latents, dim=0).cpu().numpy()\n",
    "labels = torch.cat(labels, dim=0).cpu().numpy()\n",
    "\n",
    "print(latents.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot latents with discrete cmap\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 7))\n",
    "im = ax.scatter(latents[:, 0], latents[:, 1], c=labels, cmap=plt.cm.Set3, s=5)\n",
    "ax.set_ylabel('Latent Feature 2')  \n",
    "ax.set_title('Latent Space Visualization')\n",
    "plt.colorbar(im, ax=ax, label='Digit Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the latent space is smoother, more continuous. Another nice property is that the range of values in our latent vectors is bound to that of a unit Gaussian, like our prior. So we can simply sample from a unit Gaussian and get a valid latent vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z = ###  # draw random samples directly from the prior we train our model to learn\n",
    "    random_reconstruction = ###\n",
    "\n",
    "random_grid = make_grid(random_reconstruction)[0]\n",
    "plt.imshow(random_grid, cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the impact of each term in the loss in this figure:\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://www.jeremyjordan.me/content/images/2018/03/Screen-Shot-2018-03-18-at-7.22.24-PM.png\" width=\"800\"/>\n",
    "</center>\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **Evidence Lower Bound (ELBO)** (Advanced)\n",
    "\n",
    "It is by design that minimising $\\mathcal{L}_{VAE}$ is equivalent to maximising something called the [**Evidence Lower Bound (ELBO)**](https://en.wikipedia.org/wiki/Evidence_lower_bound):\n",
    "\n",
    "$$\n",
    "ELBO = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - \\mathbb{E}_{q(z|x)}[KL(q(z|x) || p(z))]\n",
    "$$\n",
    "\n",
    "The ELBO is also sometimes referred to as the **variational lower bound**, which is where the name \"variational\" comes from.\n",
    "\n",
    "Remember that generative models aim to capture the underlying data distribution by maximising the likelihood of the observed data under the model $p(x)$. When this approximation is successful, the model can then draw new samples that lie close to the original data.\n",
    "\n",
    "However, how can we push our model towards the very distribution we are trying to learn? It's for this reason that the ELBO is an important quantity in deep learning: it provides **tractable** (i.e. easy to compute) **lower bound** on the log-likelihood of the data under the model $p(x)$.\n",
    "\n",
    "Therefore, by maximising the ELBO, we are effectively pushing our model towards the very distribution we are trying to learn without ever having to explicitly compute it.\n",
    "\n",
    "Although an important concept in deep learning, you don't need to know the ELBO to use VAEs. It is just a tool that helps us understand the inner workings of the model. It does not fall within the scope of this course and therefore is not assessed. For those who are interested, you can find a more in-depth explanation [here](https://lilianweng.github.io/posts/2018-08-12-vae/) and [here](https://www.jeremyjordan.me/variational-autoencoders/)\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJUhzCvKHJyu"
   },
   "source": [
    "## 3. Other VAE architectures\n",
    "\n",
    "### **Conditional VAEs**\n",
    "\n",
    "There is one type of VAE that is of special interest in generative problems: **Conditional Variational AutoEncoders (cVAEs)**.\n",
    "\n",
    "We have been looking at how VAEs can learn latent representations of a dataset, which can then be used to generate new, original data points. However, what happens if we want to constrain the decoder to produce examples within a certain category?\n",
    "\n",
    "cVAEs address this by introducing an additional conditional input $c$ to both the encoder and the decoder:\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://drive.google.com/thumbnail?id=1CuNqPhxOCkkwTN7HtIInKo7o3JsppUic&sz=w1500\" width=\"600\"/>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "This conditional information could contain, for example, a hot-encoded label. We could use this in the case of MNIST to generate specific digits after training.\n",
    "\n",
    "### **Very Deep VAEs**\n",
    "\n",
    "In an attempt to improve the performance of VAEs, we could try to increase the amount of layers. However, doing this naively results in limited performance improvements.\n",
    "\n",
    "Instead, researchers have proposed [**Very Deep Variational AutoEncoders (VDVAEs)**](https://arxiv.org/pdf/2011.10650) as a way to enable increasing performance with increasingly large layer count.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://drive.google.com/thumbnail?id=1YfybnjKu155INV-Mg4wJrFhwfC_ujkwO&sz=w1500\" width=\"700\"/>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "VDVAEs are based on the construction of intermediate latent spaces at the end of every block, creating a multi-scale latent representation. Each convolution is preceded by the GELU non-linearity. In this diagram, $q_φ$ and $p_Θ$ are diagonal Gaussian distributions (parametrised by their mean and variance), and $z$ is sampled from $q_φ$ during training and $p_Θ$ when sampling.\n",
    "\n",
    "**Does this architecture look familiar?**\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://drive.google.com/thumbnail?id=1LXs4wzJxvjPu_yhFCqRoyHQcv4hepJfV&sz=w1500\" width=\"700\"/>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r49TVqaZjppW"
   },
   "source": [
    "## 4. A simple VAE demo\n",
    "\n",
    "Check out this VAE demo where you can play with the latent space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 825
    },
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1729690370012,
     "user": {
      "displayName": "Carlos Cueto Mondéjar",
      "userId": "10605816413378977833"
     },
     "user_tz": -60
    },
    "id": "CtYzIx29iWws",
    "outputId": "4533a785-d7e0-4430-febf-2c614a22f03d"
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe src=\"https://www.siarez.com/projects/variational-autoencoder\" width=\"1000\" height=\"800\">\n",
    "</iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9xCb27blK9F"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZl7zxE_lH2j"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
