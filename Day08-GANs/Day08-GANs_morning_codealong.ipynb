{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YehS8enAmDn"
      },
      "source": [
        "<img src=\"https://drive.google.com/thumbnail?id=1cXtXdAcwedVDbapmz1pj_hULsQrhEcff&sz=w1500\" width=\"500\"/>\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA1tJ0r_vPQf"
      },
      "source": [
        "# **Generative Adversarial Networks (GANs)**\n",
        "\n",
        "#### **Morning contents/agenda**\n",
        "\n",
        "1. Why GANs?\n",
        "\n",
        "2. Recap of generative models and VAEs\n",
        "\n",
        "3. Formulation and training strategy\n",
        "\n",
        "4. Implemention of a simple GAN\n",
        "\n",
        "5. Pros and cons\n",
        "\n",
        "6. Overview of main GAN flavours\n",
        "\n",
        "#### **Learning outcomes**\n",
        "\n",
        "1. Understand the basic principles of adversarial traning (with two competing networks)\n",
        "\n",
        "2. Implementa a Generator and a Discriminator and train them with a simple GAN\n",
        "\n",
        "3. Be aware of the importance of GANs in deep learning, its many variations, benefits, and limitations\n",
        "\n",
        "<br>\n",
        "\n",
        "#### **Afternoon not-taught contents/agenda**\n",
        "\n",
        "1. Conditional GANs\n",
        "\n",
        "2. Wasserstein GANs theory and implementation example. Materials provided for Wasserstein GAN (for information only, **you will NOT be assessed on WGANs**)\n",
        "\n",
        "#### **Learning outcomes**\n",
        "\n",
        "1. Understand how Conditional GANs can be used to generate samples of a specific class\n",
        "\n",
        "<br/>\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sinIebTtMUvs"
      },
      "source": [
        "## 1. Why GANs?\n",
        "\n",
        "GANs were introduced by Ian Goodfellow and others (including Yoshua Bengio) in 2014.\n",
        "\n",
        "<center><img src=\"https://drive.google.com/thumbnail?id=1RPHM4jNVmYwT0_MTttcFnpYAJdjyQ6uC&sz=w1500\" width=\"800\"/></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "- GANs are very effective as a tool to implicitly learn data distributions.\n",
        "\n",
        "- They can be used in a wide variety of applications: data generation, clustering, representation learning, translation, etc.\n",
        "\n",
        "- GANs can handle different types of data: audio, images, video, text, etc.\n",
        "\n",
        "<br>\n",
        "\n",
        "There has been very significant improvement from the publication of the [original paper](https://arxiv.org/pdf/1406.2661.pdf). Results in the original publication:\n",
        "\n",
        "<center><img src=\"https://drive.google.com/thumbnail?id=1J5IAUvAv0WPp0KjdEersf41hCPI69w8v&sz=w1500\" width=\"600\"/></center>\n",
        "\n",
        "More complex variations on the original idea have resulted in dramatic improvements. For example:\n",
        "\n",
        "- StyleGAN ([Paper](https://arxiv.org/pdf/1812.04948.pdf))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ljhihMQwwybE",
        "outputId": "58abacae-24f3-4b75-e76a-8e1717c1534d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<iframe src=\"https://thispersondoesnotexist.com/\" width=\"1000\" height=\"1000\"></iframe>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%html\n",
        "<iframe src=\"https://thispersondoesnotexist.com/\" width=\"1000\" height=\"1000\"></iframe>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Muvvr-f08inR"
      },
      "source": [
        "<center><img src=\"https://drive.google.com/thumbnail?id=12fF1vr_SNgc5vUaY0WcA0AC87Tv64Sq5&sz=w1500\" width=\"800\"/></center>\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "- cycleGAN ([Paper](https://arxiv.org/pdf/1703.10593.pdf))\n",
        "\n",
        "<center><img src=\"https://drive.google.com/thumbnail?id=1duze4mADJf-eGH1_15sNqa4bKsibThKu&sz=w1500\" width=\"800\"/></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfJTRsdbvpub"
      },
      "source": [
        "## 2. Recap of generative models and VAEs\n",
        "\n",
        "Generative models are designed to find the distribution that better explains where the samples of a particular dataset come from, and then use this distribution to generate data samples.\n",
        "\n",
        "<br>\n",
        "\n",
        "<p align = \"center\"><img src=\"https://drive.google.com/thumbnail?id=13yVxGyVbefU6on-QFtgS8PHHOM-CnIdE&sz=w1500\" width=\"300\"/></p><p align = \"center\">\n",
        "<i> <a href=\"https://openai.com/blog/generative-models/\">image source</a></i>\n",
        "</p>\n",
        "\n",
        "<br>\n",
        "\n",
        "but we cannot explicitly know the probability density function that describes $p_{data}(x)$. Instead we use a neural network that we will denote $G$ (Generator) to learn $p_{data}(x)$.\n",
        "\n",
        "<br>\n",
        "\n",
        "For a generative network $G$, and given a training real dataset:\n",
        "\n",
        "- $G$ is a mapping function from $z$ to data space, parameterised by $\\theta$ (in VAEs this is our decoder)\n",
        "- True data distribution $p_{data}(x)$ (<font color='blue'>p(x)</font> in the figure)\n",
        "- Generated data distribution $p_G(x)$ (<font color='green'>p̂(x)</font> in the figure)\n",
        "\n",
        "\n",
        "\n",
        "<p align = \"center\"><img src=\"https://images.openai.com/blob/1c914c57-32d4-415a-9a2f-53282fd20a32/gen_models_diag_2.svg\" width=\"800\"/></p><p align = \"center\">\n",
        "<i> <a href=\"https://openai.com/blog/generative-models/\">image source</a></i>\n",
        "</p>\n",
        "\n",
        "#### We want to find a generator $G^*$ that satisfies $p_{G^*} \\approx p_{data}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxyTU0eAARkw"
      },
      "source": [
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "To find the optimal $G^*$, we update our network parameters to minimise the difference (divergence) between $p_{G}$ and $p_{data}$:\n",
        "\n",
        "$$G^* = \\underset{G}{\\operatorname{argmin}}  \\text{Div} (p_{data}(x) || p_G(x))$$\n",
        "\n",
        "But we cannot define a simple loss function to measure this divergence explicitly, because the true distribution $p_{data}(x)$ is our unknown.\n",
        "\n",
        "As we saw yesterday, VAEs minimise this divergence by maximising the ELBO. It does so through a reconstruction loss (commonly MSE), and by constraining the latent space to a Gaussian distribution, in order to compute the KL divergence.\n",
        "\n",
        "Recap of training a VAE:\n",
        "\n",
        "1. Sample $x_i$ from training data.\n",
        "2. Train an Encoder$(x_i) \\rightarrow z_i$ and a Decoder$(z_i) \\rightarrow \\hat{x}_i$ by using an explicit pixel-wise loss (like MSE).\n",
        "3. After training is complete, sample $z \\approx p(z)$ to generate new data using the decoder.\n",
        "\n",
        "Consquences of using MSE loss in VAEs:\n",
        "\n",
        "- Pixel-wise losses are not 'intelligent' enough to account for global patterns\n",
        "- Blurry (low frequency) outputs\n",
        "\n",
        "<p align = \"center\"><img src=\"https://drive.google.com/thumbnail?id=1-m99v3GlBjhWWI1jZC1x0WR7o7pupYsq&sz=w1500\" width=\"600\"/></p><p align = \"center\">\n",
        "<i> which image is perceptually better? </a></i>\n",
        "</p>\n",
        "\n",
        "<br>\n",
        "\n",
        "Later today we will implement a simple GAN and confirm that it produces sharper images than the VAE we implemented yesterday:\n",
        "\n",
        "<br>\n",
        "\n",
        "<p align = \"center\"><img src=\"https://drive.google.com/thumbnail?id=1GY4bhQ01eEWGZdx_9oqR0tk7hs_JUGo6&sz=w1500\" width=\"400\"/></p><p align = \"center\">\n",
        "<i> VAE results on MNIST </a></i>\n",
        "</p>\n",
        "\n",
        "<br>\n",
        "\n",
        "<p align = \"center\"><img src=\"https://drive.google.com/thumbnail?id=1fXftd0wG_wKAooG_EZUTmXGhnuGc8L53&sz=w1500\" width=\"400\"/></p><p align = \"center\">\n",
        "<i> GAN results on MNIST </a></i>\n",
        "</p>\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "### **Adversarial loss** for the generative model:\n",
        "GANs provide a method to directly optimise $G^* = \\underset{G}{\\operatorname{argmin}} \\, \\text{Div} (P_{data}(x) \\, || \\, P_G(x))$ by using another network called a **Discriminator** for the loss function – the **adversarial loss**.\n",
        "\n",
        "\n",
        "Training GAN $\\rightarrow$ minimising Jensen-Shannon divergence (but other divergences/distances can also be used)\n",
        "\n",
        "The advantages of GANs compared to VAEs are:\n",
        "\n",
        "- Loss function used is more 'flexible' and 'intelligent'.\n",
        "- It produces perceptually better results than VAEs' MSE loss.\n",
        "\n",
        "\n",
        "<p align = \"center\">\n",
        "<img src=\"https://drive.google.com/thumbnail?id=1WhWQS-co1CB4_60zQfr-vxfYWx05vgnh&sz=w1500\" width=\"1000\"/>\n",
        "</p>\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62eBj4KIARh5"
      },
      "source": [
        "## 3. Formulation and training strategy\n",
        "\n",
        "\n",
        "<center><img src=\"https://drive.google.com/thumbnail?id=166bm5GMoWPS5kQxA4Pt3LOk1mqf73FkS&sz=w1500\" width=\"800\"/></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "To generate realistic samples from $G(z)$, we play a zero-sum competition game:\n",
        "\n",
        "- Train $D$ to **correctly label** its inputs as *REAL* or *FAKE*\n",
        "\n",
        "- Train $G$ to **'fool'** $D$ to label $G(z)$ as a *REAL* sample\n",
        "\n",
        "In this way, both Generator and Discriminator improve as the training workflow progresses: $G$ gets better at generating *FAKE* samples, because $D$ is getting better at spotting them. The optimal solution would be met when:\n",
        "\n",
        "$$\n",
        "{G: p_G = p_{data}}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "To play this zero-sum game, we use a **min-max loss function**:\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\n",
        "\\boxed {\\underset{G}{\\operatorname{min}} \\underset{D}{\\operatorname{max}}  V_{GAN} (G,D) = \\mathbb{E}_{x \\sim p_{data}(x)} [\\log D(x)]\n",
        "+ \\mathbb{E}_{z \\sim p_{z}(z)} [\\log(1 - D(G(z)))]}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "The equation itself computes the expected value of the discriminator labelling real samples as real and fake samples as fake. And we train it in two parts:\n",
        "\n",
        "1. Train $D$ to maximise the loss, so that:\n",
        "  - when input is *REAL*: $\\, \\, D(x) \\rightarrow 1$\n",
        "  - when input is *FAKE*: $\\, \\, D(G(z)) \\rightarrow 0$\n",
        "\n",
        "2. Train $G$ to minimise the loss:\n",
        "  - 'Fools' $D$ to output a high-score when when input is *FAKE*: $\\, \\, D(G(z)) \\rightarrow 1$\n",
        "\n",
        "\n",
        "So, when we train GANs we following this workflow:\n",
        "\n",
        "<br>\n",
        "\n",
        "<center><img src=\"https://drive.google.com/thumbnail?id=1om1rv0un8TWKifoo35iiu_ukphLsQ138&sz=w1500\" width=\"800\"/></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "A visual representation of the training process (image adapted from the [original paper](https://arxiv.org/pdf/1406.2661.pdf)):\n",
        "\n",
        "<br>\n",
        "\n",
        "<center><img src=\"https://drive.google.com/thumbnail?id=1ZmEJOigye_0hb9_PdHla-52-x_uYYEW-&sz=w1500\" width=\"800\"/></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "As we mentioned above, we are playing a zero-sum game. But what does this means? A zero-sum game is a mathematical representation of a situation involving two players (or more) where the gains of one player equal the losses of the other. In the context of GANs, it means that if the Generator gets better, by definition, the Discriminator must get worse. That does not mean that both of them can't get better if we train them in an alternate way, which is exactly what we do in GANs.\n",
        "\n",
        "When we have a look at the loss curves for a GANs, we should see this kind of behaviour:\n",
        "\n",
        "<br>\n",
        "\n",
        "<center><img src=\"https://drive.google.com/thumbnail?id=1IbN8wG8KdELj6aCtkh1UqGNpZ6TGPAXR&sz=w1500\" width=\"400\"/></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "The optimal discriminator $D^*$ is obtained by maximising the adversarial loss:\n",
        "\n",
        "$$\n",
        "D^*(x) = {\\underset{D}{\\operatorname{argmax}}} \\left[ p_{data}(x) \\log{D(x)} + p_G(x) \\log{(1-D(x))} \\right]\n",
        "$$\n",
        "\n",
        "And by setting the derivative of the objective function with respect to $D(x)$ to zero, this is equivalent to:\n",
        "\n",
        "\n",
        "$$\n",
        "D^*(x) = \\frac{p_{ data}(x) }{ p_{data}(x) + p_G(x) }\n",
        "$$\n",
        "\n",
        "\n",
        "Substituting this into the loss expression and noting that $1 - D^*(x) = \\frac{p_{G}(x) }{ p_{data}(x) + p_G(x) }$ gives us:\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\n",
        "V_{GAN} (G,D^*) = \\mathbb{E}_{x \\sim p_{data}(x)} \\left[\\frac{p_{data}(x) }{ p_{data}(x) + p_G(x) }  \\right] + \\mathbb{E}_{x \\sim p_G} \\left[\\frac{p_G(x)}{ p_{data}(x) + p_G(x) }  \\right] \\\\[20pt] = - \\log(4) + 2 JSD (p_{data} || p_G)\n",
        "$$\n",
        "\n",
        "where JSD is the [Jensen-Shannon Divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence) between $p_{data}$ and $p_G$ \\[you can find the full derivation in the original paper by Goodfellow\\].\n",
        "\n",
        "This means that, for an optimal $D^*(x)$, training the generator is equal to minimising the JS divergence.\n",
        "\n",
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ-LcBSuARU7"
      },
      "source": [
        "### **Intuitive explanation of GANs training procedure**:\n",
        "\n",
        "We can interpret GANs as a competition between a ***Forger*** ($G$) and a ***Detective*** ($D$):\n",
        "\n",
        "<br>\n",
        "\n",
        "<center><img src=\"https://miro.medium.com/max/1400/1*-gFsbymY9oJUQJ-A3GTfeg.webp\" width=\"800\"/> <br> <a href=\"https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f\"> <i> image source </i>  </a> </center>\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "But it is better to thing of it in terms of a ***teacher*** ($D$) and a ***student*** ($G$). Interaction between $G$ and $D$:\n",
        "\n",
        "1. $D$ is leading $G$:\n",
        "  - $D$ is trained first\n",
        "  - $D$ provides knowledge to update $G$\n",
        "\n",
        "2. $D$ needs to teach adapting to the level of $G$:\n",
        "  - Measure the distance between current $p_G$ and $p_{data}$\n",
        "\n",
        "<br>\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1_VwpWfWZlr-ijrK3wUKNOFEO3nwuWb3t\" width=\"800\"/> <br> <a href=\"https://robots.media.mit.edu/wp-content/uploads/sites/7/2021/03/EAAI-What-are-GANs_.pdf\"> <i> image source </i> </a> </center>\n",
        "\n",
        "<br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "id": "wXcJzNfOmFAG",
        "outputId": "a5751b7c-e8fd-477d-f404-1e6a4ce2c18b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<iframe src=\"https://poloclub.github.io/ganlab/\" width=\"1200\" height=\"600\"></iframe>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%html\n",
        "<iframe src=\"https://poloclub.github.io/ganlab/\" width=\"1200\" height=\"600\"></iframe>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIVf_uNQCIaD"
      },
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rS6CGKOyHbd"
      },
      "source": [
        "## 4. Implementation of a simple GAN on MNIST\n",
        "\n",
        "We will implement a very similar architecture to that of the original paper.\n",
        "\n",
        "A few imports before we get started"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsZ9BJ4pymFP"
      },
      "outputs": [],
      "source": [
        "!pip install livelossplot -q\n",
        "%pylab inline\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "from livelossplot import PlotLosses\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "import random\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
        "    torch.backends.cudnn.enabled   = False\n",
        "\n",
        "    return True\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
        "    print(\"Cuda installed! Running on GPU!\")\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print(\"No GPU available!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO1AbHsG27mD"
      },
      "source": [
        "### **Dataset & Dataloader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkccqfDm3EDA"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "# MNIST Dataset\n",
        "transform = transforms.Compose([\n",
        "    ##,\n",
        "    ##])                                          # The original implementation of GANs scales the data\n",
        "                                                  # between [-1, 1]. We do that through the normalise transform.\n",
        "                                                  # Removing 0.5 from the data scales the data from [0, 1] to [-0.5, 0.5]\n",
        "                                                  # Dividing it by 0.5 scales the data from [-0.5, 0.5] to [-1, 1]\n",
        "\n",
        "train_dataset = ##\n",
        "\n",
        "# Data Loader\n",
        "train_loader = ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4JI2OVu3kxq"
      },
      "source": [
        "#### Question:\n",
        "We are not splitting the data now, why? What does **generalisation** mean here?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTC6TDTpTEIg"
      },
      "source": [
        "### **Model and Optimiser**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJNceeGuzAIu"
      },
      "source": [
        "\n",
        "- The Generater network ($G$) maps the latent vector back to the data space and consists of 4 Linear layers;\n",
        "- the Discriminator network ($D$) is a classifier and also consists of 4 Linear layers.\n",
        "- Latent vector length: 100\n",
        "\n",
        "Network $G$:\n",
        "1. Layer 1: 100 $\\rightarrow$ 256 (100 is length of latent vector)\n",
        "2. Layer 2: 256 $\\rightarrow$ 512\n",
        "3. Layer 3: 512 $\\rightarrow$ 1024\n",
        "4. Layer 4: 1024 $\\rightarrow$ 784 (size of a MNIST image)\n",
        "\n",
        "Apply [`LeakyReLU`](https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html) with (negative_slope=0.2) activation functions for  layers $1 → 3$\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://pytorch.org/docs/stable/_images/LeakyReLU.png\" width=\"400\"/>\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "`LeakyReLU` is one of the adaptations of ReLU that helps to mitigate the vanishing gradient problem. **Why?**\n",
        "\n",
        "<br>\n",
        "\n",
        "Network $D$:\n",
        "1. Layer 1: 784 $\\rightarrow$ 1024\n",
        "2. Layer 2: 1024 $\\rightarrow$ 512\n",
        "3. Layer 3: 512 $\\rightarrow$ 256\n",
        "3. Layer 4: 256 $\\rightarrow$ 1\n",
        "\n",
        "Apply ``LeakyReLU(negative_slope=0.2)`` activation functions for layers $1 → 3$. Can also apply low rate ``Dropout(0.3)`` for  layers $1 → 3$.\n",
        "\n",
        "\n",
        "*Note that, although here we have used the Linear layers, it is equally possible, and often preferred for image data, to use Convolutional layers.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-LDihNb0MlL"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, ##):\n",
        "        super().__init__()\n",
        "        self.fc1 = ##\n",
        "        self.fc2 = ##\n",
        "        self.fc3 = ##\n",
        "        self.fc4 = ##\n",
        "        self.activation = ##\n",
        "\n",
        "    # forward method\n",
        "    def forward(self, z):\n",
        "        x = ##\n",
        "        x = ##\n",
        "        x = ##\n",
        "        return ##\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, d_input_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = ##\n",
        "        self.fc2 = ##\n",
        "        self.fc3 = ##\n",
        "        self.fc4 = ##\n",
        "        self.activation = ##\n",
        "        self.dropout = ##\n",
        "\n",
        "    # forward method\n",
        "    def forward(self, x):\n",
        "        x = ##\n",
        "        x = ##\n",
        "        x = ##\n",
        "        x = ##\n",
        "        x = ##\n",
        "        x = ##\n",
        "        return ##\n",
        "\n",
        "\n",
        "# Instantiate model and send to device\n",
        "##\n",
        "\n",
        "G(z_sample).shape, D(x_sample).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfRhiWPWy_2b"
      },
      "source": [
        "Print $G$ and $D$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nfRttXa0RgX"
      },
      "outputs": [],
      "source": [
        "##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vW9fnZRH0R4R"
      },
      "outputs": [],
      "source": [
        "##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9W6jVyHI0ZZ7"
      },
      "source": [
        "Optimiser Hyper-parameters:\n",
        "- Learning_rate: 2$e^{-4}$\n",
        "- Optimiser: Adam\n",
        "\n",
        "*Note that it is possible to have different optimiser hyperparameters for the generator and discriminator*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaNY8vk60ZLJ"
      },
      "outputs": [],
      "source": [
        "lr = ##\n",
        "G_optimiser = ##\n",
        "D_optimiser = ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZV3gLKjM8d5"
      },
      "source": [
        "### **Training Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGklEHpb0wja"
      },
      "source": [
        "We need two training functions: one to train the discriminator, another to train the generator. Let's define them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWPmsUf31dWe"
      },
      "outputs": [],
      "source": [
        "def D_train(x):\n",
        "    #-------------- Function of the discriminator training -------------------#\n",
        "    ##\n",
        "    D_optimiser.zero_grad()\n",
        "\n",
        "    bs = x.shape[0]\n",
        "\n",
        "    # Real data reshaped -- label for all real samples is 1\n",
        "    x_real, y_real = ##\n",
        "    x_real, y_real = ##\n",
        "\n",
        "    # Training loss for real data\n",
        "    D_output = ##\n",
        "    D_real_loss = ##\n",
        "\n",
        "    # Sample vector and generation of fake data -- label for all fake samples is 0\n",
        "    z = ##\n",
        "    x_fake, y_fake = ##\n",
        "\n",
        "    # Training loss for fake data\n",
        "    D_output = ##\n",
        "    D_fake_loss = ##\n",
        "\n",
        "    # Combine the losses\n",
        "    D_loss = ##\n",
        "\n",
        "    # Backpropagate and model update\n",
        "    ##\n",
        "    ##\n",
        "\n",
        "    return ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO4Hx6Md0weU"
      },
      "source": [
        "and for $G$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBKsW86d0wNR"
      },
      "outputs": [],
      "source": [
        "def G_train(x):\n",
        "    #-------------- Function of the generator training -------------------#\n",
        "    ##\n",
        "    G_optimiser.zero_grad()\n",
        "\n",
        "    bs = x.shape[0]\n",
        "\n",
        "    # Sample vector and generation of fake data\n",
        "    z = ##\n",
        "    x_fake = ##\n",
        "\n",
        "    # To \"fool\" the discriminator, fake data labels are set to 1\n",
        "    y = ##\n",
        "\n",
        "    # Training loss of the generator\n",
        "    D_output = ##\n",
        "    G_loss = ##\n",
        "\n",
        "    # Backpropagate and model update\n",
        "    ##\n",
        "    ##\n",
        "\n",
        "    return ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW8Vel4dNHnv"
      },
      "source": [
        "**Which Loss Function?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsenKfiGNMya"
      },
      "outputs": [],
      "source": [
        "criterion = ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0ySfyMN0wDQ"
      },
      "source": [
        "And loop over epochs to train:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0CuDxkZCCqV"
      },
      "outputs": [],
      "source": [
        "n_epoch = 100\n",
        "groups = {'Loss': ['D_Loss', 'G_Loss']}\n",
        "liveloss = PlotLosses(groups=groups)\n",
        "\n",
        "for epoch in range(1, n_epoch+1):\n",
        "    d_loss_epoch, g_loss_epoch = 0, 0\n",
        "    logs = {}\n",
        "\n",
        "    # Loop through batch and train discriminator then generator\n",
        "    ##\n",
        "\n",
        "    # Update logs\n",
        "    ##\n",
        "\n",
        "    liveloss.update(logs)\n",
        "    liveloss.draw()\n",
        "\n",
        "    # Save every 20th epochs\n",
        "    if(np.mod(epoch, 20) == 0):\n",
        "        ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcGjw8uY0v4i"
      },
      "source": [
        "### **Generating new images from a the Generator**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlK56skEZ07N"
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import make_grid\n",
        "\n",
        "def generate(G, z_dim, N=64, nrow=8, seed=0, plot=True):\n",
        "    ##\n",
        "    set_seed(seed)\n",
        "    ##\n",
        "    ##\n",
        "    ##\n",
        "\n",
        "    # Plot images\n",
        "    if plot:\n",
        "        grid = ##\n",
        "        fig = plt.figure(figsize=(10,10))\n",
        "        plt.imshow(grid[0].cpu().numpy(), cmap=\"gray\")\n",
        "        plt.show()\n",
        "\n",
        "    return generated\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Vg_Mbo53NGz"
      },
      "outputs": [],
      "source": [
        "# Download trained model's state dict\n",
        "!gdown 1pZwDD8poDWNrqpSJOQGa4IzSHLDtNnGu\n",
        "\n",
        "# Load state dict to G\n",
        "##\n",
        "\n",
        "# Generate and plot images\n",
        "##\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-0mPxxG3e-1"
      },
      "source": [
        "Compare against **VAE** result (using a VAE network with similar complexity):\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1GY4bhQ01eEWGZdx_9oqR0tk7hs_JUGo6\" width=\"800\"/> <i> <br>VAE result </i>\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCfv3-CAtDYh"
      },
      "source": [
        "## 5. Advantages and Disadvantages of GANs\n",
        "\n",
        "- <font color='green'>$\\checkmark$</font> GANs are an evolution of VAEs which produce **“sharper” and more “perceptually realistic” results** by using a neural network to produce samples indistinguishable from the training data\n",
        "\n",
        "- <font color='green'>$\\checkmark$</font>GANs implicitly model the data distribution **without explicitly requiring a probabilistic formulation or assumptions about the true data's distribution**, making them more flexible in capturing more complex distributions\n",
        "\n",
        "- <font color='green'>$\\checkmark$</font> Like VAEs, GANs are **one-shot samplers**, circumventing the need for costly Markov chains for iterative refinements.\n",
        "\n",
        "<br>\n",
        "\n",
        "- <font color='red'>$\\times$</font> GANs require a fine balancing of both networks to successfully train. If the discriminator becomes too strong, the generator may receive vanishing gradients, slowing or halting its learning. Conversely, if the generator outpaces the discriminator, the discriminator provides useless gradients. GANs are **notoriously unstable and difficult to train**.\n",
        "\n",
        "- <font color='red'>$\\times$</font> Following this instability, GANs are particularly **sensitive to the choice of hyperparameters** (learning rate, batch size, optimisers, etc.)\n",
        "\n",
        "- <font color='red'>$\\times$</font> GANs implicit learning of the data's distribution means they are **hard to interpret**.\n",
        "\n",
        "- <font color='red'>$\\times$</font> GANs suffer from *mode collapse*, which occurs when the **generator learns to produce limited diversity** in its output that successfully fool the discriminator\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iYSy1UlFtGcpWRFquJiqPA.png\" width=\"800\"/> <i><br>[source](https://medium.com/@miraytopal/what-is-mode-collapse-in-gans-d3428a7bd9b8) </i>\n",
        "\n",
        "\n",
        "<img src=\"https://www.researchgate.net/publication/385921282/figure/fig1/AS:11431281291249435@1731988036180/Mode-collapse-behavior-target-distribution-plain-dots-and-G-t-Z-cross-dots.ppm\" width=\"800\"/> <i><br>[source](https://www.researchgate.net/publication/385921282_Parallelly_Tempered_Generative_Adversarial_Networks)</i>\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smjlleYj_vxk"
      },
      "source": [
        "## 6. Other GAN flavours\n",
        "\n",
        "So far in this notebook we have implemented and discussed the original architecture of GANs proposed by Goodfellow, 2014. This is referred in the field as the **Vanilla GAN** implementation.\n",
        "\n",
        "Vanilla GANs and its adversarial formulation serve as the foundation for many subsequent GAN architectures.\n",
        "\n",
        "In particular, due to the limitations we discussed above, several variations and improvements from Vanilla GAN focus on improving the stability and convergence of these networks.\n",
        "\n",
        "#### [**Wasserstein GAN**](https://arxiv.org/abs/1701.07875) (today's extra materials)\n",
        "- Replaces the traditional discriminator loss function with a **Wasserstein loss**, which provides a **smoother gradient and avoids issues with vanishing gradients**.\n",
        "- Introduces **weight clipping in the discriminator** to ensure the model satisfies the Lipschitz continuity required for the Wasserstein distance.\n",
        "- Uses a continuous and more interpretable metric (Wasserstein distance) to measure how closely the generated data matches the real data distribution\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://www.researchgate.net/profile/Soheil-Kolouri-2/publication/324246144/figure/fig6/AS:612260317777920@1522985643291/These-plots-show-W-1-p-q-t-and-JSp-q-t-where-p-is-a-uniform-distribution-around.png' />\n",
        "<figcaption>These plots show W-distance and J-S divergence where p is a uniform distribution around zero and q τ (x) = p(x − τ). It is clear that JS divergence does not provide a usable gradient when distributions are supported on non-overlapping domains. (Kolouri et al, 2018)</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "#### [**DCGANs**](https://arxiv.org/abs/1511.06434)\n",
        "\n",
        "- Replaces fully connected layers with convolutional layers in both the generator and discriminator.\n",
        "\n",
        "- Uses batch normalisation to stabilise training.\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/37034031/43060428-dd0b2b88-8e8b-11e8-9f50-e199e1ea22ee.png\" width=\"800\"/> <i><br>Generated MNIST images with DCGAN [source](https://github.com/ChengBinJin/DCGAN-TensorFlow)</i>\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "#### [**Conditional GANs**](https://arxiv.org/abs/1411.1784) (today's afternoon excercise)\n",
        "\n",
        "- Incorporates conditional information (e.g., class labels, textual descriptions) into both the generator and discriminator (Mirza and Osindero, 2014) for guided synthetsis\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://www.machinelearningmastery.com/wp-content/uploads/2019/05/Example-of-100-Generated-items-of-Clothing-using-an-Conditional-GAN.png' />\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### [**StyleGAN**](https://arxiv.org/abs/1812.04948)\n",
        "\n",
        "- Uses a style-based generator architecture where latent vectors control different levels of detail (e.g., overall structure, fine textures) for controllable image synthesis\n",
        "\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://editor.analyticsvidhya.com/uploads/89435Screenshot%20(70).png' />\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "#### [**CycleGAN**](https://arxiv.org/abs/1703.10593)\n",
        "- Translates images from one domain (A) to another (B) and back, enforcing cycle consistency. This ensures that when an image is translated from domain A $\\to$ B $\\to$ A, it reconstructs the original image.\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://junyanz.github.io/CycleGAN/images/teaser.jpg' />\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "#### [**Progressively Growing GANs**](https://arxiv.org/abs/1710.10196) (\n",
        "- The generator and discriminator are trained progressively, starting with low-resolution images (e.g., 4 $\\times$ 4) and incrementally adding layers to increase the resolution to improve stability\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oT8CpImr95iyEZoY6wJTzg.png' />\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "and countless more....\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8aBnE76b6U0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}