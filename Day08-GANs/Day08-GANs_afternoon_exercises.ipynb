{"cells":[{"cell_type":"markdown","metadata":{"id":"kkZlumhnXRfp"},"source":["<img src=\"https://drive.google.com/uc?id=1cXtXdAcwedVDbapmz1pj_hULsQrhEcff\" width=\"500\"/>\n","\n","---\n"]},{"cell_type":"markdown","source":["# **Generative Adversarial Networks (GANs)**\n","\n","#### **Morning contents/agenda**\n","\n","1. Why GANs?\n","\n","2. Recap of generative models and VAEs\n","\n","3. Formulation and training strategy\n","\n","4. Implemention of a simple GAN\n","\n","5. Pros and cons\n","\n","6. Overview of main GAN flavours\n","\n","#### **Learning outcomes**\n","\n","1. Understand the basic principles of adversarial traning (with two competing networks)\n","\n","2. Implementa a Generator and a Discriminator and train them with a simple GAN\n","\n","3. Be aware of the importance of GANs in deep learning, its many variations, benefits, and limitations\n","\n","<br>\n","\n","#### **Afternoon contents/agenda**\n","\n","1. Conditional GANs\n","\n","2. Wasserstein GANs theory and implementation example. Materials provided for Wasserstein GAN (for information only, **you will NOT be assessed on WGANs**)\n","\n","#### **Learning outcomes**\n","\n","1. Understand how Conditional GANs can be used to generate samples of a specific class\n","\n","<br/>\n","\n","---\n"],"metadata":{"id":"WtMmreLm_09c"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZoLwAbmt8ZxT"},"outputs":[],"source":["!pip install livelossplot\n","%pylab inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lMGyZY7-8lOc"},"outputs":[],"source":["%pylab inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KK2h_09p81r7"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","from livelossplot import PlotLosses\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch.utils.data import TensorDataset, DataLoader\n","import torchvision.transforms as transforms\n","from torchvision.datasets import MNIST\n","\n","import random"]},{"cell_type":"code","source":["def set_seed(seed):\n","    \"\"\"\n","    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n","    \"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n","    torch.backends.cudnn.enabled   = False\n","\n","    return True\n","\n","device = 'cpu'\n","if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n","    print(\"Cuda installed! Running on GPU!\")\n","    device = 'cuda'\n","else:\n","    print(\"No GPU available!\")"],"metadata":{"id":"8PUB9Lu0IBlC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5kKUc4vLBag2"},"source":["## Conditional Generative Adversarial Networks (cGANs)\n","\n","Conditional GANs (cGANs) are a variation of conventional GANs that enable the generation of realistic data that match specific labels. This variation greatly improves the value of GANs in real practice.\n","\n","In our MNIST case, we can adapt the GAN architecture from the morning lecture to generate number-specific images (e.g., 1000 images of the number 5), instead of generating random digits every time.\n","\n","cGANs were introduced in [this](https://arxiv.org/abs/1411.1784) paper. In this exercise, we will implement this architure to include labels of MNIST in our generator. The architecture of cGANs is shown below:\n","\n","![](https://miro.medium.com/max/700/1*Vjo1df-yPFks2e_-TbdWdQ.png)\n","\n","Image credit: [medium blog](https://medium.com/@ma.bagheri/a-tutorial-on-conditional-generative-adversarial-nets-keras-implementation-694dcafa6282)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Bjd64qAc9M9r"},"source":["## 1. Train a conditional GAN\n","\n","The first question we need to ask ourselves in order to implemente a cGAN is: **how to put label into the network?**\n","\n","There are different ways we could do this, but the simplest one is just to concatenate the (one-hot encoded) label with the input of the network.\n","\n","If, for example, we had a latent vector of length 128 and the conditioning vector had size 10 (for the 10 classes in MNIST, with the value of 1 at the index of the class and 0 elsewhere), the input of the generator network will be of size 128 + 10 = 138.\n","\n","Similarly, if the input to our GAN discriminator is of size 784 (28 x 28 images in MNIST), the input to our cGAN discriminator will be 784 + 10 = 794.\n"]},{"cell_type":"markdown","source":["\n","### **1.1. Create the conditional generator**\n","\n","Let's start by building our conditional generator. For this, we will use the following hyperparameters:\n","\n","- Latent vector length: 128\n","- Length of the conditioning vector: 10\n","\n","The network itself will be composed of the following layers:\n","\n","0. Concatenate `z` and `label` using torch.cat()\n","1. Layer 1 for the latent vector: 128 + 10 -> 256\n","2. Layer 2 for the label vector: 256 -> 512\n","3. Layer 2: 512 -> 1024\n","4. Layer 4: 1024 -> 784 (size of a MNIST image)\n","\n","We will appply the ``LeakyReLU(0.2)`` activation functions for layers 1-3, and ``tanh`` to the final layer.\n","\n","**Consider:** Why do we use the `tanh` as the final layer's activation?"],"metadata":{"id":"_1QZ4E4_e1ro"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"EAC2gDEpBlPt"},"outputs":[],"source":["class Generator(nn.Module):\n","    def __init__(self, g_input_dim, g_output_dim, num_classes):\n","        super().__init__()\n","        self.fc1 = ##\n","        self.fc2 = ##\n","        self.fc3 = ##\n","        self.fc4 = ##\n","        self.activation = ##\n","\n","    # forward method\n","    def forward(self, z, label):\n","        ##\n","        return ##"]},{"cell_type":"markdown","source":["As always, let's test that the model is doing what we think it's doing:"],"metadata":{"id":"quBljS2IvCEs"}},{"cell_type":"code","source":["# Build model\n","z_dim = ##\n","x_dim = ##\n","num_classes = ##\n","\n","G = ##\n","\n","# Test output\n","G(torch.randn(10, z_dim).to(device), torch.randn(10, num_classes).to(device)).shape"],"metadata":{"id":"YQ4zHsFGuGLx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **1.2. Create the conditional discriminator**\n","\n","Next, we can build the conditional discriminator network. In this case, the network will have the following layers:\n","\n","0. Concatenate `x` and `label` here using torch.cat()\n","1. Layer 1: 794 -> 1024\n","2. Layer 2: 1024 -> 512\n","3. Layer 3: 512 -> 256\n","4. Layer 4: 256 -> 1\n","\n","\n","We will apply ``LeakyReLU(0.2)`` activation functions for layers 1-3, and ``Sigmoid`` to the final layer.\n","\n","**Consider:** Why do we use the `Sigmoid` as the final layer's activation?"],"metadata":{"id":"jo0LqMuvt6MG"}},{"cell_type":"code","source":["class Discriminator(nn.Module):\n","    def __init__(self, d_input_dim, num_classes):\n","        super().__init__()\n","        self.fc1 = ##\n","        self.fc2 = ##\n","        self.fc3 = ##\n","        self.fc4 = ##\n","        self.activation = ##\n","        self.dropout = ##\n","\n","    # forward method\n","    def forward(self, x, label):\n","        ##\n","        return ##"],"metadata":{"id":"eba6YkvTuDTg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Which we can now test:"],"metadata":{"id":"hLAg31RCvGlc"}},{"cell_type":"code","source":["# Build model\n","z_dim = ##\n","x_dim = ##\n","num_classes = ##\n","\n","D = ##\n","\n","# Test output\n","D(torch.randn(10, x_dim).to(device), torch.randn(10, num_classes).to(device)).shape"],"metadata":{"id":"_hpwoPD7uPO0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **1.3. Create the dataloader, optimisers and criterion**\n","\n","Now that we have created a model, we can prepare our dataset. We will use a batch size of 64 and normalise the MNIST dataset to the range [-1,1].\n","\n"],"metadata":{"id":"4AljwMjse40v"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yYJypw4w8xu7"},"outputs":[],"source":["# MNIST Dataset\n","batch_size=64\n","transform = transforms.Compose([\n","    ##,\n","    ##])\n","\n","train_dataset = ##\n","\n","# Data Loader\n","train_loader = ##"]},{"cell_type":"markdown","source":["We will also create our optimiser. In this case, we will use ``Adam`` with ``learning_rate = 2e-4``."],"metadata":{"id":"alHJN77ovYce"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oibE4-xq9BWy"},"outputs":[],"source":["# Optimisers\n","lr = ##\n","G_optimiser = ##\n","D_optimiser = ##"]},{"cell_type":"markdown","source":["And, finally, we will use BCE as the criterion:"],"metadata":{"id":"y3ZRLPzAvjoh"}},{"cell_type":"code","source":["# Loss function\n","criterion = ##"],"metadata":{"id":"ZT_auDkQvrug"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **1.3. Adapt the ``D_train`` and ``G_train`` functions**\n","\n","To finalise our setup, we will need to adapt the `D_train` and `G_train` functions we built this morning to include the conditioning.\n","\n","**Consider**: Think about how you need to format the class labels from the data loader to the class vectors ``(batch_size, num_classes)`` required for conditioning.\n","\n","Let's start with `D_train`:"],"metadata":{"id":"WhKJHoeLfSXl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"EuVC3m9G9C3j"},"outputs":[],"source":["def D_train(x, cond):                  ## take in the labels\n","    #-------------- Function of the discriminator training -------------------#\n","    D.train()\n","    D_optimiser.zero_grad()\n","\n","    bs = x.shape[0]\n","\n","    # Create conditioning vector for real data of size (bs, num_classes)\n","    c = ##   ## create an empty tensor that will contain the one-hot encoded conditioning\n","    ##       ## use scatter to fill the tensor using the class labels\n","    c = ##   ## send the tensor to the device\n","\n","    # Real data reshaped -- label for all real samples is 1\n","    x_real, y_real = ##\n","    x_real, y_real = ##\n","\n","    # Training loss for real data\n","    D_output = ##           ## ensure the conditioning is passed to the network\n","    D_real_loss = ##\n","\n","    # Create conditioning vector for fake data of size (bs, num_classes)\n","    c = ##  ## create an empty tensor that will contain the one-hot encoded conditioning\n","    ##      ## fill in the tensor using random class labels\n","    c = ##  ## send the tensor to the device\n","\n","    # Sample vector and generation of fake data -- label for all fake samples is 0\n","    z = ##\n","    x_fake, y_fake = ##   ## ensure the conditioning is passed to the network\n","\n","    # Training loss for fake fata\n","    D_output = ##  ## ensure the conditioning is passed to the network\n","    D_fake_loss = ##\n","\n","    # Combine the losses\n","    D_loss = ##\n","\n","    # Backpropagate and model update\n","    D_loss.backward()\n","    D_optimiser.step()\n","\n","\n","    return  D_loss.data.item()\n"]},{"cell_type":"markdown","source":["Next, we apply the changes to `G_train`:"],"metadata":{"id":"FfSka8wBwiex"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0wdYzttu8ybC"},"outputs":[],"source":["def G_train(x):\n","    #-------------- Function of the generator training -------------------#\n","    G.train()\n","    G_optimiser.zero_grad()\n","\n","    bs = x.shape[0]\n","\n","    # Create conditioning vector for fake data of size (bs, num_classes)\n","    c = ##  ## create an empty tensor that will contain the one-hot encoded conditioning\n","    ##      ## fill in the tensor using random class labels\n","    c = ##  ## send the tensor to the device\n","\n","    # Sample vector and generation of fake data\n","    z = ##\n","    x_fake = ##                  ## ensure the conditioning is passed to the network\n","\n","    # To \"fool\" the discriminator, fake data labels are set to 1\n","    y = ##\n","\n","    # Training loss of the generator\n","    D_output = ##           ## ensure the conditioning is passed to the network\n","    G_loss = ##\n","\n","    # Backpropagate and model update\n","    G_loss.backward()\n","    G_optimiser.step()\n","\n","    return G_loss.data.item()\n"]},{"cell_type":"markdown","source":["### **1.4. Train the network**\n","\n","We are now ready to train our model. We will need to make a small change to the training loop from this morning to include the labels."],"metadata":{"id":"pxAIvMDdfxzg"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fWoIQBns9HQT"},"outputs":[],"source":["n_epoch = 100\n","groups = {'Loss': ['D_Loss', 'G_Loss']}\n","liveloss = PlotLosses(groups=groups)\n","\n","for epoch in range(1, n_epoch+1):\n","    d_loss_epoch, g_loss_epoch = 0, 0\n","    logs = {}\n","    for batch_idx, (x, labels) in enumerate(train_loader):\n","        d_loss_epoch += ##  ## pass in the labels\n","        g_loss_epoch += ##\n","    logs['D_Loss'] = d_loss_epoch / len(train_loader)\n","    logs['G_Loss'] = g_loss_epoch / len(train_loader)\n","    liveloss.update(logs)\n","    liveloss.draw()\n","\n","\n","    # save every 20th epochs\n","    if(np.mod(epoch, 20) == 0):\n","        torch.save(G.state_dict(), \"./Generator_{:03d}.pth\".format(epoch))"]},{"cell_type":"markdown","source":["**Consider:**\n","\n","1. Does the training behave as expected?\n","2. How do would you know if the training has completed successfully or not?"],"metadata":{"id":"zaDPdBUOxGEi"}},{"cell_type":"markdown","source":["### **1.5. Use the trained generators**\n","\n","We will now use the trained generator to create 10 images for every class in our MNIST dataset."],"metadata":{"id":"ihKBU8dgf8KC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9p7B3P8QLD20"},"outputs":[],"source":["# You can load the trained generator if needed\n","# epoch = 100\n","# G.load_state_dict(torch.load(\"./Generator_{:03d}.pth\".format(epoch)))\n","\n","with torch.no_grad():\n","    ##\n","\n","# Plot\n","##"]},{"cell_type":"markdown","source":["<br>\n","\n","---\n","\n","<br>"],"metadata":{"id":"6XA_YkBLsjqF"}},{"cell_type":"code","source":[],"metadata":{"id":"_8NwyFLYKFCy"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}