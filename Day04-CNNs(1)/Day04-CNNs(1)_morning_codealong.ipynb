{"cells":[{"cell_type":"markdown","id":"dd232acb-4c37-485f-8c08-a7c91f6edc3b","metadata":{"id":"dd232acb-4c37-485f-8c08-a7c91f6edc3b"},"source":["<img src=\"https://drive.google.com/uc?id=1cXtXdAcwedVDbapmz1pj_hULsQrhEcff\" width=\"500\"/>\n","\n","---"]},{"cell_type":"markdown","id":"d23721f9-5f09-4fec-a44f-96c83e508066","metadata":{"id":"d23721f9-5f09-4fec-a44f-96c83e508066"},"source":["# Introduction to convolutional neural networks (CNNs)"]},{"cell_type":"code","execution_count":null,"id":"9908e623-6228-48d3-9dd1-3c0ab20a2a59","metadata":{"id":"9908e623-6228-48d3-9dd1-3c0ab20a2a59"},"outputs":[],"source":["import requests\n","from io import BytesIO\n","from PIL import Image\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import ConfusionMatrixDisplay"]},{"cell_type":"markdown","id":"5d559b53-b373-4cb5-b93f-d4e602879839","metadata":{"id":"5d559b53-b373-4cb5-b93f-d4e602879839"},"source":["<img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/slides/Slide1.png\" width=\"100%\"/>"]},{"cell_type":"markdown","id":"4e2df9b4-59d3-414a-ac49-136cb0e4f630","metadata":{"id":"4e2df9b4-59d3-414a-ac49-136cb0e4f630"},"source":["<img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/slides/Slide2.png\" width=\"100%\"/>"]},{"cell_type":"markdown","id":"3476b425-711e-4c75-b87e-e22f74faaa89","metadata":{"id":"3476b425-711e-4c75-b87e-e22f74faaa89"},"source":["## Excercise:\n","\n","Let's download an image and look at the raw pixel values."]},{"cell_type":"code","execution_count":null,"id":"56b5169a-5b31-42ef-b16e-f67403d478ae","metadata":{"id":"56b5169a-5b31-42ef-b16e-f67403d478ae"},"outputs":[],"source":["response = requests.get(\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/camera.tif\")\n","x = torch.tensor(np.array(Image.open(BytesIO(response.content))), dtype=torch.float32)\n","\n","# TODO: look at raw pixel values and plot image\n"]},{"cell_type":"markdown","id":"9d3264cd-d144-4bd2-97ac-2125ffaf5513","metadata":{"id":"9d3264cd-d144-4bd2-97ac-2125ffaf5513"},"source":["<img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/slides/Slide3.png\" width=\"100%\"/>"]},{"cell_type":"markdown","id":"c5f7575a-0f5a-429f-b049-9dbaa09f57d1","metadata":{"id":"c5f7575a-0f5a-429f-b049-9dbaa09f57d1"},"source":["<img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/slides/Slide4.png\" width=\"100%\"/>"]},{"cell_type":"markdown","id":"d70f36ee-7819-4455-a414-48699c6a47bf","metadata":{"id":"d70f36ee-7819-4455-a414-48699c6a47bf"},"source":["<img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/slides/Slide5.png\" width=\"100%\"/>"]},{"cell_type":"markdown","id":"e8facb0c-7500-4697-98ff-3caa74f330b9","metadata":{"id":"e8facb0c-7500-4697-98ff-3caa74f330b9"},"source":["# How would you identify these images?\n","\n","<table>\n","    <tr>\n","        <td>\n","            <img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/cat.png\" width=\"1000\">\n","        </td>\n","        <td>\n","            <img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/lena.png\" width=\"1000\">\n","        </td>\n","        <td>\n","            <img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/rocket.jpg\" width=\"1000\">\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","id":"e5018ae7-057f-4532-a852-a62811b7c8b6","metadata":{"id":"e5018ae7-057f-4532-a852-a62811b7c8b6"},"source":["<img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/scanning-filter.gif\" width=\"80%\"/>"]},{"cell_type":"markdown","id":"6a2eb0a1-5a79-4038-b08d-50116f5c6833","metadata":{"id":"6a2eb0a1-5a79-4038-b08d-50116f5c6833"},"source":["<img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/slides/Slide7.png\" width=\"100%\"/>"]},{"cell_type":"markdown","id":"c612fad3-cd68-47ab-b535-e0240a879129","metadata":{"id":"c612fad3-cd68-47ab-b535-e0240a879129"},"source":["<img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/slides/Slide8.png\" width=\"100%\"/>"]},{"cell_type":"markdown","id":"5f11e407-84fb-4d2f-9602-778f344bb57a","metadata":{"id":"5f11e407-84fb-4d2f-9602-778f344bb57a"},"source":["<img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/slides/Slide9.png\" width=\"100%\"/>"]},{"cell_type":"markdown","id":"4b97a675-80eb-4623-b706-85a5188de7ad","metadata":{"id":"4b97a675-80eb-4623-b706-85a5188de7ad"},"source":["<img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/slides/Slide10.png\" width=\"100%\"/>"]},{"cell_type":"markdown","id":"0eda0566-aaf3-4e12-89a9-281e65fce72c","metadata":{"id":"0eda0566-aaf3-4e12-89a9-281e65fce72c"},"source":["<img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/slides/Slide11.png\" width=\"100%\"/>"]},{"cell_type":"markdown","id":"ef770328-f68d-48f4-bfb7-a54e4cc81805","metadata":{"id":"ef770328-f68d-48f4-bfb7-a54e4cc81805"},"source":["## Excercise:\n","\n","Let's compute a simple convolution on the camera image above, using PyTorch."]},{"cell_type":"code","execution_count":null,"id":"d54d35bf-c679-42af-aa96-893756e71e42","metadata":{"id":"d54d35bf-c679-42af-aa96-893756e71e42"},"outputs":[],"source":["# TODO: define a convolutional layer with kernel size = 3, no bias, and show the filter weights\n"]},{"cell_type":"markdown","id":"7884b6d3-8bf7-43fa-beae-f73c898cb167","metadata":{"id":"7884b6d3-8bf7-43fa-beae-f73c898cb167"},"source":["Now, let's change the weights so that the filter is an \"edge detector\", and then apply the filter."]},{"cell_type":"code","execution_count":null,"id":"28b4326f-1ecf-4b8f-9a58-4e3925ac2890","metadata":{"id":"28b4326f-1ecf-4b8f-9a58-4e3925ac2890"},"outputs":[],"source":["# TODO: change the filter weights so that we detect horizontal edges in the image, and apply the filter to the image\n"]},{"cell_type":"markdown","id":"74141546-84a4-4e71-b86b-6f9ecbf13e56","metadata":{"id":"74141546-84a4-4e71-b86b-6f9ecbf13e56"},"source":["<img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/slides/Slide12.png\" width=\"100%\"/>"]},{"cell_type":"markdown","id":"98a2deda-0684-428a-bbb2-b4ffbb351a1f","metadata":{"id":"98a2deda-0684-428a-bbb2-b4ffbb351a1f"},"source":["<img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/conv-dimensions.png\" width=\"100%\"/>"]},{"cell_type":"markdown","id":"c6c1109f-1ead-4ab6-b14b-beecc4b82441","metadata":{"id":"c6c1109f-1ead-4ab6-b14b-beecc4b82441"},"source":["## Excercise\n","\n","Let's extend the convolutional layer above so that we output 3 channels (apply 3 filters) instead of 1."]},{"cell_type":"code","execution_count":null,"id":"42d351f3-d8fb-4bc2-96a1-333dbda3d527","metadata":{"id":"42d351f3-d8fb-4bc2-96a1-333dbda3d527"},"outputs":[],"source":["# TODO: change the layer above so that we output 3 channels, and print the shape of the input, filter, and output\n"]},{"cell_type":"markdown","id":"974446b2-48b3-4bcb-b34e-c2112808a084","metadata":{"id":"974446b2-48b3-4bcb-b34e-c2112808a084"},"source":["# Convolutions - extensions\n","<table>\n","    <tr>\n","        <td>\n","            <img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/no_padding_no_strides.gif\" width=\"500\">\n","            <figcaption>Vanilla convolution</figcaption>\n","        </td>\n","        <td>\n","            <img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/no_padding_strides.gif\" width=\"500\">\n","            <figcaption>Stride = 2</figcaption>\n","        </td>\n","        <td>\n","            <img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/same_padding_no_strides.gif\" width=\"500\">\n","            <figcaption>Padding = 1</figcaption>\n","        </td>\n","        <td>\n","            <img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/dilation.gif\" width=\"500\">\n","            <figcaption>Dilation = 2</figcaption>\n","        </td>\n","    </tr>\n","</table>\n","\n","\n","source: https://github.com/vdumoulin/conv_arithmetic"]},{"cell_type":"markdown","id":"22dc3e8e-d243-4fce-8506-67be33e001be","metadata":{"id":"22dc3e8e-d243-4fce-8506-67be33e001be"},"source":["## Excercise\n","\n","Next, let's experiment with different settings for the convolutional layer, and see how that changes the output shape."]},{"cell_type":"code","execution_count":null,"id":"3b10b492-b06b-4ff8-a16f-a64d70e816ea","metadata":{"id":"3b10b492-b06b-4ff8-a16f-a64d70e816ea"},"outputs":[],"source":["# TODO: change the settings for the convolutional layer above and predict how the output shape will change\n","# How does the output shape change with `kernel_size`, `stride`, `padding`, and `dilation`?"]},{"cell_type":"markdown","id":"1f3f332e-7e24-434e-81aa-9e01056f0042","metadata":{"id":"1f3f332e-7e24-434e-81aa-9e01056f0042"},"source":["<img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/slides/Slide14.png\" width=\"100%\"/>"]},{"cell_type":"markdown","id":"1cb9d767-99ee-40ad-8ee6-7398be3fe274","metadata":{"id":"1cb9d767-99ee-40ad-8ee6-7398be3fe274"},"source":["<img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/slides/Slide15.png\" width=\"100%\"/>"]},{"cell_type":"markdown","id":"6bbae469-d6d5-4fc3-bad3-3555e48ffe29","metadata":{"id":"6bbae469-d6d5-4fc3-bad3-3555e48ffe29"},"source":["## Excercise:\n","\n","Now, we are ready to define a convolutional neural network in PyTorch. We will train it to classify MNIST digits.\n","\n","Step 1: first, we need to download the MNIST dataset."]},{"cell_type":"code","execution_count":null,"id":"ca91d628-4548-476b-a72e-ac9af2e6b1d4","metadata":{"id":"ca91d628-4548-476b-a72e-ac9af2e6b1d4"},"outputs":[],"source":["transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.1307,), (0.3081,))\n","    ])\n","train_dataset = datasets.MNIST('./', train=True, download=True, transform=transform)\n","test_dataset = datasets.MNIST('./', train=False, download=True, transform=transform)\n","\n","# TODO: plot an image from the training dataset, and print the size of the two datasets\n"]},{"cell_type":"markdown","id":"1d81e8bb-19e0-4d82-a607-8dada2e35a90","metadata":{"id":"1d81e8bb-19e0-4d82-a607-8dada2e35a90"},"source":["Step 2: next, we need to define a convolutional neural network in PyTorch. We will use the LeNet5 architecture:\n","\n","(Lecun et al., (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE)"]},{"cell_type":"markdown","id":"48aa2152-adcc-40f6-b3a7-caaf8503c1b8","metadata":{"id":"48aa2152-adcc-40f6-b3a7-caaf8503c1b8"},"source":["# LeNet5\n","<img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/lenet5.png\" width=\"100%\"/>"]},{"cell_type":"markdown","id":"51c552b3-4acf-483b-9d4d-04e6f1764f89","metadata":{"id":"51c552b3-4acf-483b-9d4d-04e6f1764f89"},"source":["Here is the PyTorch machinery we can use to help us:\n","\n","- [`nn.Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html): Defines and initialises convolutional layers\n","- [`nn.MaxPool2d`](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html): Defines and initialises max pooling layers\n","- [`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html): Defines and initialises linear (fully connected) layers\n","- [`torch.nn.LogSoftmax`](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html): Defines log softmax operation for output classification layer"]},{"cell_type":"code","execution_count":null,"id":"499af375-0492-4f74-b411-d9ae7310d162","metadata":{"id":"499af375-0492-4f74-b411-d9ae7310d162"},"outputs":[],"source":["class LeNet5(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        #TODO: define the LeNet5 layers\n","\n","\n","    def forward(self, x):\n","\n","        # TODO: define the forward network function\n","\n","        return x\n","\n","    def middle_feature(self, x):\n","        x = self.act(self.c1(x))\n","        x = self.act(self.s2(x))\n","        x = self.c3(x)\n","        return x\n","\n","torch.manual_seed(123)\n","model = LeNet5()\n","\n","# TODO: check the output dimensions makes sense, and print the shapes of the hidden layer outputs\n"]},{"cell_type":"markdown","id":"b66cc51b-6796-47f2-a20a-9777972d8916","metadata":{"id":"b66cc51b-6796-47f2-a20a-9777972d8916"},"source":["Step 3: now, we can train the network using a cross entropy (negative log likelihood) loss function, and evaluate its performance on the test set."]},{"cell_type":"code","execution_count":null,"id":"1359de89-a5ec-47de-b00b-41d69520034a","metadata":{"id":"1359de89-a5ec-47de-b00b-41d69520034a"},"outputs":[],"source":["train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=64)\n","test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=256)\n","\n","def train(model, train_loader, epochs=1):\n","    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","    criterion = torch.nn.NLLLoss()# negative log likelihood loss\n","    model.train()\n","    for epoch in range(epochs):\n","        for batch_idx, (data, target) in enumerate(train_loader):\n","\n","            # TODO: optimise model\n","\n","\n","            if batch_idx % 100 == 0:\n","                print('Train Epoch: {} [{}/{}]\\tLoss: {:.6f}'.format(\n","                    epoch, batch_idx * len(data), len(train_loader.dataset), loss.item()))\n","\n","train(model, train_loader)"]},{"cell_type":"code","execution_count":null,"id":"e76d7f62-7b12-416f-acc3-f35fc5c16351","metadata":{"id":"e76d7f62-7b12-416f-acc3-f35fc5c16351"},"outputs":[],"source":["def evaluate(model, test_loader):\n","    \"Get model predictions on test dataset\"\n","    model.eval()\n","    preds, targets = [],[]\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","\n","            # TODO: compute the class probability predictions of the model, and get the most likely class\n","\n","\n","            preds.append(pred)\n","            targets.append(target)\n","    preds = torch.concat(preds)\n","    targets = torch.concat(targets)\n","\n","    # TODO: compute the average accuracy on the test set\n","\n","\n","    plt.figure()\n","    ConfusionMatrixDisplay.from_predictions(targets, preds, ax=plt.gca(), colorbar=False, cmap='bone_r')\n","    plt.show()\n","\n","evaluate(model, test_loader)"]},{"cell_type":"markdown","id":"9303413f-2698-4168-ba3d-05a19888353b","metadata":{"id":"9303413f-2698-4168-ba3d-05a19888353b"},"source":["<img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/slides/Slide16.png\" width=\"100%\"/>"]},{"cell_type":"markdown","id":"e85bafc7-1f47-4254-863e-4d086bb2b277","metadata":{"id":"e85bafc7-1f47-4254-863e-4d086bb2b277"},"source":["<img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/slides/Slide17.png\" width=\"100%\"/>"]},{"cell_type":"markdown","id":"ada9ef04-6e79-472c-bc84-544462011fb5","metadata":{"id":"ada9ef04-6e79-472c-bc84-544462011fb5"},"source":["<img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/slides/Slide18.png\" width=\"100%\"/>"]},{"cell_type":"markdown","id":"8de6947e-1eb6-4002-8b3b-650d8c53c8a4","metadata":{"id":"8de6947e-1eb6-4002-8b3b-650d8c53c8a4"},"source":["## Excercise\n","Let's try to interpret what the convolutional neural network has learned.\n","\n","We can do this by using many different visualisation techniques.\n","\n","#### Visualising feature maps\n","A first and easy approach is to visualise the feature maps (outputs) of each convolutional layer."]},{"cell_type":"code","execution_count":null,"id":"ca387178-8518-4e21-bb9e-31cea610cc67","metadata":{"id":"ca387178-8518-4e21-bb9e-31cea610cc67"},"outputs":[],"source":["x,target = test_dataset[1]\n","\n","# TODO: plot the feature maps (outputs) for first convolutional layer (model.c1).\n","# What is each feature sensitive to in the input image?\n"]},{"cell_type":"markdown","id":"6ece270a-ce3e-449c-9e80-d11ca0f49ea4","metadata":{"id":"6ece270a-ce3e-449c-9e80-d11ca0f49ea4"},"source":["#### Maximising feature activations\n","Another approach is to change the input image pixel values such that specific feature is maximised.\n","This gives us an image which shows us what \"maximally activates\" that feature."]},{"cell_type":"code","execution_count":null,"id":"5c7a07c6-eaed-441e-bc5b-cea9cef04a4b","metadata":{"id":"5c7a07c6-eaed-441e-bc5b-cea9cef04a4b"},"outputs":[],"source":["def total_variation(image):\n","    diff_h = torch.abs(image[:, :, :-1, :] - image[:, :, 1:, :])\n","    diff_w = torch.abs(image[:, :, :, :-1] - image[:, :, :, 1:])\n","    total_variation_loss = torch.sum(diff_h) + torch.sum(diff_w)\n","    return total_variation_loss\n","\n","plt.figure(figsize=(11,8))\n","for fi in range(16):\n","    print(f\"Maximising feature {fi+1} of {16}..\")\n","    torch.manual_seed(123)\n","\n","    # TODO: optimise a randomly initialised input image to maximally activate the central pixel in the fi feature map of model.c3\n","    # use TV regularisation with a coefficient of 1.5e-2 to help smoooth the image and adam with lrate=1e-2\n","    # plot the maximised image and feature value\n","\n","    plt.subplot(4,4,fi+1)\n","    plt.imshow(x[0,0].detach())\n","    plt.title(f\"Feature value: {f[:,fi,4,4].item():.2f}\")\n","    plt.axis(\"off\")\n","    plt.colorbar()\n","plt.show()"]},{"cell_type":"markdown","id":"40636359-4ad9-4495-86ed-fa91695e45f8","metadata":{"id":"40636359-4ad9-4495-86ed-fa91695e45f8"},"source":["#### Saliency map\n","\n","Another approach is to generate a \"saliency map\": this shows which pixels in the image the class prediction is most sensitive to.\n","More precisely, we compute and plot the gradient of the class prediction with respect to each pixel in the input image."]},{"cell_type":"code","execution_count":null,"id":"86072d04-352c-41d0-86d2-63af27ecc23a","metadata":{"id":"86072d04-352c-41d0-86d2-63af27ecc23a"},"outputs":[],"source":["x,target = test_dataset[1]\n","\n","# TODO: compute the gradient of the class prediction wrt the input image pixel values, and plot the saliency map\n"]},{"cell_type":"markdown","id":"ffac0ea6-bff0-4988-a41d-a8a70d0181d7","metadata":{"id":"ffac0ea6-bff0-4988-a41d-a8a70d0181d7"},"source":["<img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/slides/Slide19.png\" width=\"100%\"/>"]},{"cell_type":"markdown","id":"75900d1a-6591-4be6-908e-56056824c5ac","metadata":{"id":"75900d1a-6591-4be6-908e-56056824c5ac"},"source":["<img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/slides/Slide20.png\" width=\"100%\"/>"]},{"cell_type":"markdown","id":"9e76023e-c388-4edb-ab57-024cedf93b04","metadata":{"id":"9e76023e-c388-4edb-ab57-024cedf93b04"},"source":["<img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/slides/Slide21.png\" width=\"100%\"/>"]},{"cell_type":"markdown","id":"3dddce3d-7183-47dc-9208-7ff4e11955c0","metadata":{"id":"3dddce3d-7183-47dc-9208-7ff4e11955c0"},"source":["# Summary\n","- Convolutional layers learn to recognise **spatial features** in images\n","- Deep convolutional networks learn **hierarchical** feature representations\n","- CNNs are used as **common framework** for many computer vision tasks\n","\n","\n","More materials:\n","- [But, what is a convolution? 3Blue1Brown video](https://www.youtube.com/watch?v=KuXjwB4LzSA)\n","- [Introtodeeplearning.com Deep Computer Vision lecture](https://introtodeeplearning.com/) (which this notebook draws inspiration from)"]},{"cell_type":"code","execution_count":null,"id":"f06023b1-4a45-454f-9611-9526eea2f011","metadata":{"id":"f06023b1-4a45-454f-9611-9526eea2f011"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}