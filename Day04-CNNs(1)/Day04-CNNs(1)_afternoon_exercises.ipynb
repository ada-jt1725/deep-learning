{"cells":[{"cell_type":"markdown","id":"e17f210e-07d5-4e60-9f9f-004de1640838","metadata":{"id":"e17f210e-07d5-4e60-9f9f-004de1640838"},"source":["<img src=\"https://drive.google.com/uc?id=1cXtXdAcwedVDbapmz1pj_hULsQrhEcff\" width=\"500\"/>\n","\n","---"]},{"cell_type":"markdown","id":"af55da1b-8261-4f34-815d-a318d88edaaa","metadata":{"id":"af55da1b-8261-4f34-815d-a318d88edaaa"},"source":["## Goal of afternoon session\n","\n","The goal of this session is to\n","\n","> Excercise 1: Write your own convolutional layer from scratch.\n","\n","> Excercise 2: Add batch normalisation and dropout to the `LeNet5` architecture shown in this morning's lecture, and train it to classify `FashionMNIST`.\n","\n","> Excercise 3: Perform data augmentation and understand its effects.\n","\n","> Excercise 4 (optional extension): interpret what the network has learned using the techniques from this morning's lecture."]},{"cell_type":"code","execution_count":null,"id":"9ded64c1-9d45-4ece-a92b-271e0e088655","metadata":{"id":"9ded64c1-9d45-4ece-a92b-271e0e088655"},"outputs":[],"source":["import requests\n","from io import BytesIO\n","from PIL import Image\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import ConfusionMatrixDisplay"]},{"cell_type":"markdown","id":"6a719bc6-43a6-404e-837e-3effea9751bd","metadata":{"id":"6a719bc6-43a6-404e-837e-3effea9751bd"},"source":["# Excercise 1: Write your own convolutional layer from scratch\n","\n","<img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/conv-definition.png\" width=\"90%\"/>\n","\n","Given the image below, and **without using PyTorch**:\n","1) define two filters: a horizontal edge detector and a vertical edge detector ([see here for help on the filter weights](https://homepages.inf.ed.ac.uk/rbf/HIPR2/sobel.htm))\n","2) convolve these two filters with the image\n","3) plot the convolved output images\n","4) check your function matches `torch.nn.functional.conv2d`"]},{"cell_type":"code","execution_count":null,"id":"6083d3c3-378b-4355-aeeb-be4bb93f22b9","metadata":{"id":"6083d3c3-378b-4355-aeeb-be4bb93f22b9"},"outputs":[],"source":["response = requests.get(\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/camera.tif\")\n","x = np.array(Image.open(BytesIO(response.content)), dtype=np.float32)\n","print(x.shape)\n","\n","# TODO: define edge detector filters\n","\n","\n","def conv2d(x, w):\n","    \"\"\"2D convolution.\n","    x: Input image of shape (height, width)\n","    w: Filter/kernel of shape (filter_height, filter_width)\n","    returns h: Output image\n","    \"\"\"\n","    # TODO: define conv2d function\n","\n","\n","\n","    return h\n","\n","\n","# TODO: plot the convolved output images\n","\n","\n","# TODO: check your function matches `torch.nn.functional.conv2d`\n"]},{"cell_type":"markdown","id":"74510b36-0b74-4c9a-8832-494f0c287187","metadata":{"id":"74510b36-0b74-4c9a-8832-494f0c287187"},"source":["# Excercise 2: Add batch normalisation and dropout to the `LeNet5` architecture shown in this morning's lecture, and train it to classify `FashionMNIST`\n","\n","<img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/lenet5.png\" width=\"100%\"/>"]},{"cell_type":"code","execution_count":null,"id":"c4361432-af23-449f-b5eb-a3bae2527fce","metadata":{"id":"c4361432-af23-449f-b5eb-a3bae2527fce"},"outputs":[],"source":["# download the FashionMNIST dataset\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","    ])\n","train_dataset = datasets.FashionMNIST('./', train=True, download=True, transform=transform)\n","test_dataset = datasets.FashionMNIST('./', train=False, download=True, transform=transform)\n","classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n","\n","# plot some example images\n","print(f\"{len(train_dataset)} images in training dataset\")\n","print(f\"{len(test_dataset)} images in test dataset\")\n","plt.figure(figsize=(12,4))\n","for i in range(5):\n","    plt.subplot(1,5,i+1)\n","    plt.title(classes[train_dataset[i][1]])\n","    plt.imshow(train_dataset[i][0][0], cmap=\"grey\")\n","plt.show()"]},{"cell_type":"markdown","id":"0019385a-43b5-484e-be45-315866f3fac9","metadata":{"id":"0019385a-43b5-484e-be45-315866f3fac9"},"source":["1. First, train the same LeNet5 shown in this morning's lecture to classify the FashionMNIST dataset. Use the same hyperparameters (batch size, optimizer, learning rate etc) as the lecture.\n","2. Then, add batch normalisation and dropout after each convolutional layer. Retrain the network, and compare performance with the standard LeNet5.\n","3. What other changes you could make to the architecture / hyperparameters to improve performance?"]},{"cell_type":"code","execution_count":null,"id":"6e0f22ab-847c-44d3-8c16-08bad348f270","metadata":{"id":"6e0f22ab-847c-44d3-8c16-08bad348f270"},"outputs":[],"source":["# TODO: train the same LeNet5 shown in this morning's lecture to classify the FashionMNIST dataset.\n","# Use the same hyperparameters (batch size, optimizer, learning rate etc) as the lecture.\n","\n","\n","# TODO: Add batch normalisation and dropout after each convolutional layer.\n","# Retrain the network, and compare performance with the standard LeNet5.\n"]},{"cell_type":"markdown","id":"12360fc3-451b-4ae1-9192-95bfdfd06a61","metadata":{"id":"12360fc3-451b-4ae1-9192-95bfdfd06a61"},"source":["# Excercise 3: Perform data augmentation and understand its effects\n","\n","Using data augmentation can help improve the accuracy of the model and reduce the likelihood of overfitting.\n","\n","We can easily perform data augmentations by using PyTorch **[transforms](https://pytorch.org/vision/stable/transforms.html#v1-api-reference)** in our training `Dataset` class.\n","\n","Tasks:\n","1. Add a transform to `train_dataset` which randomly rotates training images by 10 degrees below ([hint](https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomRotation.html#torchvision.transforms.RandomRotation)), and plot some images to check the rotation is being applied\n","2. Retrain your models with this transformation. What differences do you observe?\n","3. Try adding other transformations too (e.g. random cropping, flipping, and gaussian blur)"]},{"cell_type":"code","execution_count":null,"id":"fff1979b-7952-4eaf-ae59-4275fe78ac5f","metadata":{"id":"fff1979b-7952-4eaf-ae59-4275fe78ac5f"},"outputs":[],"source":["# TODO: Add a transform to `train_dataset` which randomly rotates training images by 10 degrees below,\n","# and plot some images to check the rotation is being applied\n","\n","\n","# TODO: Retrain your models with this transformation. What differences do you observe?\n","\n","\n","# TODO: Try adding other transformations too (e.g. random cropping, flipping, and gaussian blur)\n"]},{"cell_type":"markdown","id":"144b50d4-f44e-46dd-9b42-218d2c8467b2","metadata":{"id":"144b50d4-f44e-46dd-9b42-218d2c8467b2"},"source":["# Excercise 4 (optional extension): interpret what the network has learned using the techniques from this morning's lecture\n","\n","Select a test image, and then plot 1) the feature maps (outputs) of the first convolutional layer and 2) the saliency of the image to its class prediction."]},{"cell_type":"code","execution_count":null,"id":"ed2beb64-d7fc-4b9c-bd7a-5b2b8faf016f","metadata":{"id":"ed2beb64-d7fc-4b9c-bd7a-5b2b8faf016f"},"outputs":[],"source":["# TODO: Select a test image, and then plot\n","# 1) the feature maps (outputs) of the first convolutional layer and\n","# 2) the saliency of the image to its class prediction\n","\n"]},{"cell_type":"code","execution_count":null,"id":"f1fd6a32-8663-4645-9589-7cb3472128ae","metadata":{"id":"f1fd6a32-8663-4645-9589-7cb3472128ae"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}