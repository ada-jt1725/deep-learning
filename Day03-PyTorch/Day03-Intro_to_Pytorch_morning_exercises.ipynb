{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oHbTEGZ_rL0"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1cXtXdAcwedVDbapmz1pj_hULsQrhEcff\" width=\"500\"/>\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://pytorch.org/assets/images/pytorch-logo.png\" alt=\"drawing\" width=\"100\"/>\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "<h1 style=\"text-align: center;\"> Introduction to Pytorch for Deep Learning\n",
        "  – Exercises</h1>\n",
        "\n",
        "\n",
        "#### **Morning contents/agenda**\n",
        "\n",
        "1. Understanding the basics:\n",
        "- [But what is a convolution?](https://www.youtube.com/watch?v=KuXjwB4LzSA&ab_channel=3Blue1Brown)\n",
        "\n",
        "- [But what is a neural network?](https://www.youtube.com/watch?v=aircAruvnKk&t=1s&ab_channel=3Blue1Brown)\n",
        "\n",
        "- [What is backpropagation really doing?](https://www.youtube.com/watch?v=Ilg3gGewQ5U&t=2s&ab_channel=3Blue1Brown)\n",
        "\n",
        "2. In this exercise we will work with a chest x-ray dataset from [MedMnist](https://github.com/MedMNIST/MedMNIST) to tackle a reconstruction problem. Often, bio-engineering datasets have sparse or missing information which are difficult to avoid due to poor design unexpected failures or restricitions in acquisition times. Interpolation is a common method to pre-process the data to simulate missing data, but fails when the amount of information is large. Here we will use a neural network to predict missing values by learning the distribution of the dataset as opposed to localised operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20Pb9we5_rL1"
      },
      "source": [
        "### 2.0 Some imports and utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "id": "EAsd9PdiApgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UXZ7QY4L_rL1",
        "outputId": "9f0cad12-4f2d-4440-d6e6-542f0a8b0a7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/2.7 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCuda installed! Running on GPU 0 Tesla T4!\n"
          ]
        }
      ],
      "source": [
        "!pip install torchsummary progressbar2 livelossplot monai -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from torchsummary import summary\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "from livelossplot import PlotLosses\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = True  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
        "    torch.backends.cudnn.enabled   = True\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def set_device(device=\"cpu\", idx=0):\n",
        "    if device != \"cpu\":\n",
        "        if torch.cuda.device_count() > idx and torch.cuda.is_available():\n",
        "            print(\"Cuda installed! Running on GPU {} {}!\".format(idx, torch.cuda.get_device_name(idx)))\n",
        "            device=\"cuda:{}\".format(idx)\n",
        "        elif torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
        "            print(\"Cuda installed but only {} GPU(s) available! Running on GPU 0 {}!\".format(torch.cuda.device_count(), torch.cuda.get_device_name()))\n",
        "            device=\"cuda:0\"\n",
        "        else:\n",
        "            device=\"cpu\"\n",
        "            print(\"No GPU available! Running on CPU\")\n",
        "    return device\n",
        "\n",
        "device = set_device(\"cuda\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJEiTopU_rL2"
      },
      "source": [
        "### 2.1  Download and inspect the data using the commands below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "J-VOPyWQ_rL2",
        "outputId": "216afccb-f292-4e44-f6b6-8382db8107fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-26 11:45:21--  https://zenodo.org/record/6496656/files/chestmnist.npz\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.43.153, 137.138.52.235, 188.185.48.75, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.43.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
            "Location: /records/6496656/files/chestmnist.npz [following]\n",
            "--2025-11-26 11:45:22--  https://zenodo.org/records/6496656/files/chestmnist.npz\n",
            "Reusing existing connection to zenodo.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 82802576 (79M) [application/octet-stream]\n",
            "Saving to: ‘chestmnist.npz’\n",
            "\n",
            "chestmnist.npz      100%[===================>]  78.97M  3.06MB/s    in 24s     \n",
            "\n",
            "2025-11-26 11:45:47 (3.23 MB/s) - ‘chestmnist.npz’ saved [82802576/82802576]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://zenodo.org/record/6496656/files/chestmnist.npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B7MTP18-_rL2",
        "outputId": "aa0c5e86-abd6-4f2a-cf9a-82a2dae5d39d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['train_images', 'val_images', 'test_images', 'train_labels', 'val_labels', 'test_labels']\n"
          ]
        }
      ],
      "source": [
        "data = np.load(\"./chestmnist.npz\")\n",
        "print(data.files)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['train_images'].shape\n",
        "type(data)"
      ],
      "metadata": {
        "id": "20fEjbNkFUAi",
        "outputId": "03aa643e-5e69-498d-c01b-b9e4e33a7c56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.lib.npyio.NpzFile"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>numpy.lib.npyio.NpzFile</b><br/>def __init__(fid, own_fid=False, allow_pickle=False, pickle_kwargs=None, *, max_header_size=format._MAX_HEADER_SIZE)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/numpy/lib/npyio.py</a>NpzFile(fid)\n",
              "\n",
              "A dictionary-like object with lazy-loading of files in the zipped\n",
              "archive provided on construction.\n",
              "\n",
              "`NpzFile` is used to load files in the NumPy ``.npz`` data archive\n",
              "format. It assumes that files in the archive have a ``.npy`` extension,\n",
              "other files are ignored.\n",
              "\n",
              "The arrays and file strings are lazily loaded on either\n",
              "getitem access using ``obj[&#x27;key&#x27;]`` or attribute lookup using\n",
              "``obj.f.key``. A list of all files (without ``.npy`` extensions) can\n",
              "be obtained with ``obj.files`` and the ZipFile object itself using\n",
              "``obj.zip``.\n",
              "\n",
              "Attributes\n",
              "----------\n",
              "files : list of str\n",
              "    List of all files in the archive with a ``.npy`` extension.\n",
              "zip : ZipFile instance\n",
              "    The ZipFile object initialized with the zipped archive.\n",
              "f : BagObj instance\n",
              "    An object on which attribute can be performed as an alternative\n",
              "    to getitem access on the `NpzFile` instance itself.\n",
              "allow_pickle : bool, optional\n",
              "    Allow loading pickled data. Default: False\n",
              "\n",
              "    .. versionchanged:: 1.16.3\n",
              "        Made default False in response to CVE-2019-6446.\n",
              "\n",
              "pickle_kwargs : dict, optional\n",
              "    Additional keyword arguments to pass on to pickle.load.\n",
              "    These are only useful when loading object arrays saved on\n",
              "    Python 2 when using Python 3.\n",
              "max_header_size : int, optional\n",
              "    Maximum allowed size of the header.  Large headers may not be safe\n",
              "    to load securely and thus require explicitly passing a larger value.\n",
              "    See :py:func:`ast.literal_eval()` for details.\n",
              "    This option is ignored when `allow_pickle` is passed.  In that case\n",
              "    the file is by definition trusted and the limit is unnecessary.\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "fid : file, str, or pathlib.Path\n",
              "    The zipped archive to open. This is either a file-like object\n",
              "    or a string containing the path to the archive.\n",
              "own_fid : bool, optional\n",
              "    Whether NpzFile should close the file handle.\n",
              "    Requires that `fid` is a file-like object.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "&gt;&gt;&gt; from tempfile import TemporaryFile\n",
              "&gt;&gt;&gt; outfile = TemporaryFile()\n",
              "&gt;&gt;&gt; x = np.arange(10)\n",
              "&gt;&gt;&gt; y = np.sin(x)\n",
              "&gt;&gt;&gt; np.savez(outfile, x=x, y=y)\n",
              "&gt;&gt;&gt; _ = outfile.seek(0)\n",
              "\n",
              "&gt;&gt;&gt; npz = np.load(outfile)\n",
              "&gt;&gt;&gt; isinstance(npz, np.lib.npyio.NpzFile)\n",
              "True\n",
              "&gt;&gt;&gt; npz\n",
              "NpzFile &#x27;object&#x27; with keys: x, y\n",
              "&gt;&gt;&gt; sorted(npz.files)\n",
              "[&#x27;x&#x27;, &#x27;y&#x27;]\n",
              "&gt;&gt;&gt; npz[&#x27;x&#x27;]  # getitem access\n",
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
              "&gt;&gt;&gt; npz.f.x  # attribute lookup\n",
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, None);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCdhFXC4_rL2"
      },
      "source": [
        "### 2.3 Create a custom dataset\n",
        "\n",
        "* Create your own ``Dataset`` derived class that takes as initialisation arguments:\n",
        "  - ``data_path``, the path to the data\n",
        "  - a probability for a random mask ``p``,\n",
        "  - a ``transform`` to be applied to the data,\n",
        "  - and a ``split`` argument to dictate what part of the data to load (train, validation, test)\n",
        "\n",
        "* Load the data into an argument ``self.data`` inside the initialisation\n",
        "\n",
        "* Create a method for your class ``_get_mask``, that generates a binary mask of the size of the sample to randomly erase some data points based on the probability ``p``\n",
        "\n",
        "* Customise the  ``__getitem__`` class so that it loads a sample from ``self.data`` and returns a masked version of the sample, and the original sample (the former will be input to our network and the later the target)\n",
        "\n",
        "* Don't forget to set the built-in method ``__len__`` to the correct size\n",
        "\n",
        "* Instantiate the class for a training set and a validation set. Plot one input and output for each of these sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BN4muwLu_rL3"
      },
      "outputs": [],
      "source": [
        "class ChestMNIST(Dataset):\n",
        "    def __init__(self, data_path, split=\"train\", p=0.5, transform=None):\n",
        "      self.data_path = data_path\n",
        "      self.p = p\n",
        "      self.transform = transform\n",
        "      self.split = split\n",
        "      if split=='train':\n",
        "        self.data = np.load(data_path)['train_images']\n",
        "      elif split=='test':\n",
        "        self.data = np.load(data_path)['test_images']\n",
        "      elif split=='validation':\n",
        "        self.data = np.load(data_path)['val_images']\n",
        "      else:\n",
        "        raise ValueError('split parameter must be one of train, test, validation')\n",
        "      self.data = torch.from_numpy(self.data).unsqueeze(1) # add channel dimension\n",
        "\n",
        "    def _get_mask(self, img_shape):\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      pass\n",
        "\n",
        "    def __len__(self):\n",
        "      pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1TfPg8K_rL3"
      },
      "outputs": [],
      "source": [
        "## Instantiate datasets\n",
        "\n",
        "## Plots\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ew068BZ_rL3"
      },
      "source": [
        "### 2.4 Modify our ``simpleFFN`` model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHSUsJap_rL3"
      },
      "source": [
        "* Add two more hidden layers to the model\n",
        "\n",
        "* Change the size of the output to match the size of the input\n",
        "\n",
        "* Change the activation of the model to [``Mish``](https://arxiv.org/abs/1908.08681)\n",
        "\n",
        "* Change the activation of last layer, what should it be?\n",
        "\n",
        "* Instantiate the model and print a summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LkxXZQc_rL3"
      },
      "outputs": [],
      "source": [
        "# Modify model\n",
        "class simpleFFN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size_1=100, hidden_size_2=50, output_size=10):\n",
        "    super(simpleFFN, self).__init__()\n",
        "    self.hidden_1 = nn.Linear(input_size, hidden_size_1, bias=False)\n",
        "    self.hidden_2 = nn.Linear(hidden_size_1, hidden_size_2, bias=False)\n",
        "    self.output = nn.Linear(hidden_size_2, 10, bias=False)\n",
        "    self.activation = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, X):\n",
        "    z1 = self.hidden_1(X.flatten(start_dim=1))\n",
        "    a1 = self.activation(z1)\n",
        "    z2 = self.hidden_2(a1)\n",
        "    a2 = self.activation(z2)\n",
        "    z3 = self.output(a2)\n",
        "    a3 = self.activation(z3)\n",
        "    return a3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EofNzK3Z_rL4"
      },
      "outputs": [],
      "source": [
        "# Instantiate and print summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Inc4U5fG_rL4"
      },
      "source": [
        "### 2.5 Prepare parameters and hyperparameters for training\n",
        "\n",
        "* Set your hyperparameters:\n",
        "    - seed: 42\n",
        "    - mask probability: 0.6 (this is a heavy damaged imputation problem! We are only keeping 60% of the information)\n",
        "    - learning rate: 1e-2\n",
        "    - weight decay = 1e-6 (applied in optimiser)\n",
        "    - batch size: 128\n",
        "    - number of epochs: 30\n",
        "\n",
        "\n",
        "* Instantiate ``simpleFFN`` as our model with hidden sizes: 150, 50, 50, 150\n",
        "\n",
        "* Instantiate ``Adam`` as the optimiser\n",
        "\n",
        "* Instantiate ``MSELoss`` as a criterion\n",
        "\n",
        "* Collect any list of transformations you think are appropriate for this problem\n",
        "\n",
        "* Instantiate the training and validation dataset and create the dataloader for each\n",
        "\n",
        "* Visualise an input and target batch using ``make_grid``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pewVAL9_rL4"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "\n",
        "\n",
        "# Training set up: model, optimiser, criterion\n",
        "\n",
        "\n",
        "# Transforms, Dataset and dataloader\n",
        "\n",
        "\n",
        "# Visualise a batch sample\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufNOcBVs_rL4"
      },
      "source": [
        "### 2.6 Modify training and validation functions\n",
        "\n",
        "* Make the necessary modifications to the ``train`` and ``valid`` functions from the lecture to adapt to our reconstruction problem\n",
        "\n",
        "* Does prediction play a role in this problem?\n",
        "\n",
        "* Is accuracy a suitable metric?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GXp7Hgi_rL4"
      },
      "outputs": [],
      "source": [
        "# Modify functions\n",
        "def train(model, optimizer, criterion, data_loader):\n",
        "    model.train()\n",
        "    train_loss, train_accuracy = 0, 0\n",
        "    for input, target in data_loader:\n",
        "        input, target = input.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(input)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "\n",
        "        train_loss += loss*input.size(0)\n",
        "        pred = output.softmax(dim=1).max(dim=1)[1]\n",
        "        train_accuracy += accuracy_score(target.cpu().numpy(), pred.detach().cpu().numpy())*input.size(0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss = train_loss / len(data_loader.dataset)\n",
        "    train_accuracy = train_accuracy/len(data_loader.dataset)\n",
        "    return train_loss, train_accuracy\n",
        "\n",
        "\n",
        "def valid(model, criterion, data_loader):\n",
        "    \" Equivalent to the training function without any backpropagation or optimisation steps\"\n",
        "    model.eval()\n",
        "    valid_loss, valid_accuracy = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for input, target in data_loader:\n",
        "            input, target = input.to(device), target.to(device)\n",
        "\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            valid_loss += loss*input.size(0)\n",
        "\n",
        "            pred = output.softmax(dim=1).max(dim=1)[1]\n",
        "\n",
        "            valid_accuracy += accuracy_score(target.cpu().numpy(), pred.detach().cpu().numpy())*input.size(0)\n",
        "\n",
        "        valid_loss = valid_loss / len(data_loader.dataset)\n",
        "        valid_accuracy = valid_accuracy/len(data_loader.dataset)\n",
        "        return valid_loss, valid_accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMiovebI_rL4"
      },
      "source": [
        "### 2.7 Train and validate the model\n",
        "\n",
        "* Train your model\n",
        "\n",
        "* Visualise the output of a validation sample along training\n",
        "\n",
        "* At the end of training, plot the 32 reconstructed and target samples from a validation batch\n",
        "\n",
        "* What do you observe?\n",
        "\n",
        "* Are the results as expected?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s82at41L_rL4"
      },
      "outputs": [],
      "source": [
        "# Train model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot recon and target from valid batch"
      ],
      "metadata": {
        "id": "aQ1EnssDjJs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH8LPdae_rL4"
      },
      "source": [
        "### 2.8 Save model to disk and load\n",
        "\n",
        "* ``Pytorch`` stores all the parameters of models and optimizers, their weights and biases in an easy to read dictionary called a \"state-dict\".\n",
        "\n",
        "* When we store models and optimizers, we store the state-dict.  \n",
        "\n",
        "* Together with the model definition we can then restore the model to it's state when we stored it to disk.\n",
        "\n",
        "* Let's look at the contents of the state-dict of both our optimizer and our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-xx9IMh_rL4"
      },
      "outputs": [],
      "source": [
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "# Print optimiser's state_dict\n",
        "print(\"Optimiser's state_dict:\")\n",
        "for var_name in optimiser.state_dict():\n",
        "    print(var_name, \"\\t\", optimiser.state_dict()[var_name])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTJyPrbV_rL5"
      },
      "source": [
        "From colab (and locally) we can store models to disk using ```torch.save``` and passing both a models state_dict() and a path where to store it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0UVJSmK_rL5"
      },
      "outputs": [],
      "source": [
        "#!mkdir '/content/gdrive/My Drive/models'  ## create the director for storing the model in Google Drive\n",
        "\n",
        "model_save_name = 'chestmnist_simpleFFN_model.pt'           # .pt and .pth are common file extensions for saving models in pytorch\n",
        "path = F\"/content/gdrive/My Drive/models/{model_save_name}\" # use this to store in your Google Drive storage\n",
        "path = F'./{model_save_name}'                               # use this to store locally (it will be erased once the colab session is over)\n",
        "torch.save(model.state_dict(), path)\n",
        "\n",
        "optimiser_save_name = 'chestmnist_simpleFFN_optimiser.pt'\n",
        "path = F\"/content/gdrive/My Drive/models/{optimiser_save_name}\"\n",
        "path = F\"./{optimiser_save_name}\"\n",
        "torch.save(model.state_dict(), path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz4ZRrXq_rL5"
      },
      "source": [
        "Finally, we can restore models from the saved ```state_dict```'s and do a number of things such as:\n",
        "1. Use it as a checkpoint and continue training (given we stored the optimizer as well)\n",
        "2. Make predictions from our model\n",
        "3. Perform inspections of our model\n",
        "4. Use our model in ensembles\n",
        "5. ...\n",
        "\n",
        "By default a loaded model is put into ```.train()``` mode. So be careful when using networks that behave different depending on training and test time e.g. dropout regularized networks or batch-normalized networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cG_13t8i_rL5"
      },
      "outputs": [],
      "source": [
        "model_save_name = 'chestmnist_simpleFFN_model.pt'           # .pt and .pth are common file extensions for saving models in pytorch\n",
        "path = F\"/content/gdrive/My Drive/models/{model_save_name}\" # use this to store in your Google Drive storage\n",
        "path = F'./{model_save_name}'\n",
        "\n",
        "model = simpleFFN(input_size= 1 * 28 * 28, hidden_size_1=150, hidden_size_2=50, hidden_size_3=50, hidden_size_4=150).to(device) ## creates an instance of the model\n",
        "model.load_state_dict(torch.load(path)) ## loads the parameters of the model in path. state_dict is a dictionary object that maps each layer in the model to its trainable parameters (weights and biases).\n",
        "model.eval()\n",
        "\n",
        "valid_loss = valid(model, mseloss, valid_loader)\n",
        "print(\"Avg. Valid Loss: %1.3f\" % valid_loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uADu0Udo_rL5"
      },
      "source": [
        "### 2.8 Training with Unet\n",
        "\n",
        "* Instantiate a ``U-net`` using the snipped below.\n",
        "\n",
        "* For now you do not need to understand what a ``U-net`` is or how it works. This will be explored later in the course.\n",
        "\n",
        "* Print the summary of the model and have a look at what kind of layers it includes. Search for these layers in the ``Pytorch`` documentation to gain a general understanding of their operations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRRtG8aM_rL5"
      },
      "outputs": [],
      "source": [
        "from monai.networks.nets import UNet\n",
        "set_seed(42)\n",
        "model = UNet(spatial_dims=2, in_channels=1, out_channels=1, channels=(8, 8, 8), strides=(1, 1,), act=\"mish\").to(device)\n",
        "summ = summary(model, input_size=(1, 28, 28))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Train the model with the same hyperparameters from before. Don't forget to re-initialise the optimiser with the correct model parameters\n",
        "\n",
        "* As before, visualise some validation samples along training.\n",
        "\n",
        "* Plot 32 reconstructed and target samples from the validation batch\n",
        "\n",
        "* Save your final model\n",
        "\n",
        "* What differences do you observe from training with a simple feed-forward network? Why do you think that is?"
      ],
      "metadata": {
        "id": "wvzwwRlWhwFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate optimiser\n"
      ],
      "metadata": {
        "id": "ekM5VEbsmn-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyZggtJq_rL5"
      },
      "outputs": [],
      "source": [
        "# Train model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot recon and target from valid batch"
      ],
      "metadata": {
        "id": "TqzSyfiRjMT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model"
      ],
      "metadata": {
        "id": "8Jg5novaU7b0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.7 ('mltorch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b996dd704eec390d394e4eff83c8c73a7c6e40d054c583352c9aa3265aab441b"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}