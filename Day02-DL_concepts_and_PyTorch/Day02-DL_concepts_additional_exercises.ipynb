{"cells":[{"cell_type":"markdown","metadata":{"id":"9YehS8enAmDn"},"source":["<img src=\"https://drive.google.com/uc?id=1cXtXdAcwedVDbapmz1pj_hULsQrhEcff\" width=\"500\"/>\n","\n","---\n"]},{"cell_type":"markdown","source":["# **Deep learning concepts**\n","\n","\n","#### **Morning contents/agenda**\n","\n","1. Supervised VS unsupervised learning\n","\n","2. Parameters and hyperparameters of a network\n","\n","  &ensp;&ensp;&ensp;&ensp;2.1 Activation functions\n","\n","  &ensp;&ensp;&ensp;&ensp;2.2 Losses\n","\n","3. **Training is an optimisation problem**: gradient descent and backpropagation\n","\n","4. Batch, mini-batch, and stochastic gradient descent\n","\n","5. Bias and variance, and regularisers\n","\n","#### **Learning outcomes**\n","\n","1. Understand the difference between parameters and hyperparameters of a network\n","\n","2. Understand how networks are trained using gradient descent and backpropagation\n","\n","3. Understand how batch size works and the effect of regularisers in the training process\n","\n","\n","<br>\n","\n","#### **Afternoon contents/agenda**\n","\n","*Afternoon exercises are provided as additional content and will not be taught. We will instead focus on learning about PyTorch.*\n","\n","1. Half-moon classifier\n","\n","2. L2 regularisation on MNIST\n","\n","#### **Learning outcomes**\n","\n","1. Be able to build a simple classifier for simple datasets\n","\n","2. Understand the effect of explicit regularisation during training\n","\n","<br/>\n","\n","---\n","\n","<br/>"],"metadata":{"id":"koBaqXTQrby5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_kYki018eTFJ"},"outputs":[],"source":["!pip install pycm livelossplot\n","%pylab inline\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","from livelossplot import PlotLosses\n","from pycm import *\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","import torchvision.transforms as transforms\n","from torchvision.datasets import MNIST\n","from torchsummary import summary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LI8sNA9feT3H"},"outputs":[],"source":["def set_seed(seed):\n","    \"\"\"\n","    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n","    \"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n","    torch.backends.cudnn.enabled   = False\n","\n","    return True\n","\n","device = 'cpu'\n","if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n","    print(\"Cuda installed! Running on GPU!\")\n","    device = 'cuda'\n","else:\n","    print(\"No GPU available!\")"]},{"cell_type":"markdown","metadata":{"id":"EirUJEpBVvU3"},"source":["## 1. Half-moon classifier\n","\n","Our goal this this first part is to write a network with 2 hidden layers to perform a binary classification task on the provided dataset.\n","\n","Before we begin, let's fix some hyperparameters:"]},{"cell_type":"code","source":["# Define hyperparameters\n","set_seed(42)             # random seed\n","epochs = 1000            # number of loops through whole dataset\n","batch_size = 1000        # size of a single batch\n","batch_num = 1            # use full batch training\n","test_size = 100          # examples in test set\n","lr = ##                  # find a good learning rate\n","momentum = ##            # play with this value of momentum\n","\n","#Define the size of the input, hidden, and output layers\n","I, H1, H2, O = 2, 3, 5, 1"],"metadata":{"id":"bmkyJg8sWgTa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **1.1. Dataset**\n","\n","To make the dataset, we will use the following utility function:"],"metadata":{"id":"F0RETSWTWgiG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"urlFMaVtY-Rg"},"outputs":[],"source":["from sklearn.datasets import make_moons\n","from sklearn.metrics import accuracy_score\n","\n","\n","def make_train_test(batch_size, batch_num, test_size, noise=0.01):\n","    \"\"\"\n","    Makes a two-moon train-test dataset with fixed batch size, number and noise level\n","    \"\"\"\n","    X_train, y_train = make_moons(n_samples=batch_size*batch_num, noise=noise)\n","    y_train = y_train.reshape(batch_num, batch_size, 1)\n","    X_train = X_train.reshape(batch_num, batch_size, 2)\n","\n","\n","    X_test, y_test = make_moons(noise=noise)\n","    y_test = y_test.reshape(test_size, 1)\n","    return X_train, y_train, X_test, y_test"]},{"cell_type":"markdown","source":["Using the function above, a train and test `TensorDataset`, with an associated `DataLoader`:"],"metadata":{"id":"2OhY0SxmWZis"}},{"cell_type":"code","source":["# Use the function 'make_train_test' defined above to create two-moons + noise\n","X_train, y_train, X_test, y_test = ##  # call the function defined above\n","\n","# Define Train Set in Pytorch\n","X_train = torch.from_numpy(X_train).float()[0] # Convert to torch tensor, single batch\n","y_train = torch.from_numpy(y_train).float()[0] # Convert to torch tensor, single batch\n","\n","train_dataset = ##  # create a train TensorDataset\n","\n","# Define Test Set in Pytorch\n","X_test = torch.from_numpy(X_test).float() # Convert to torch tensor, already single batch\n","y_test = torch.from_numpy(y_test).float() # Convert to torch tensor, already single batch\n","\n","test_dataset = ##  # create a test TensorDataset\n","\n","# Use Pytorch's functionality to load data in batches. Here we use full-batch training again.\n","train_loader = ##  # create a train DataLoader\n","test_loader = ##   # create a test DataLoader"],"metadata":{"id":"E71zBhSvWZpt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **1.2. Network**\n","\n","Define a simple FFN with two hidden layers and sigmoid activations:"],"metadata":{"id":"6iCp7Wb95fL8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"R3X5OAGRZT-4"},"outputs":[],"source":["class SingleHiddenLayerNetwork(nn.Module):\n","    def __init__(self, I, H1, H2, O):\n","        super(SingleHiddenLayerNetwork, self).__init__()\n","        self.hidden_1 = ##\n","        self.hidden_2 = ##\n","        self.output = ##\n","        self.activation = ##\n","\n","    def forward(self, X):\n","        z1 = ##\n","        a1 = ##\n","        z2 = ##\n","        a2 = ##\n","        z3 = ##\n","        a3 = ##\n","        return a3"]},{"cell_type":"markdown","source":["### **1.3. Loss and training functions**\n","\n","Let's, for the sake of learning, definte the BCE loss manually:"],"metadata":{"id":"ioSh8G3U5vte"}},{"cell_type":"code","source":["# define the loss (criterion)\n","def bce_loss(y, a3):\n","    return ##"],"metadata":{"id":"GArLqJ205uDH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, we can define our train and evaluate functions:"],"metadata":{"id":"7B84P2WyWLFc"}},{"cell_type":"code","source":["# train function\n","def train(model, optimizer, data_loader):\n","    ## # set your model to train\n","    for X, y in data_loader:\n","        ## # zero the gradients\n","        ## # forward pass\n","        ## # loss calculation\n","        ## #Â backprop\n","        ## # update the model\n","\n","    y_pred = np.where(a3[:, 0].detach().numpy()>0.5, 1, 0) # you can use this code\n","    accuracy = accuracy_score(y, y_pred)                   # but try to understand what it does\n","    return loss, accuracy                                  # and return the loss and the accuracy\n","\n","# evaluate function\n","def evaluate(model, data_loader):\n","    ## # set your model to evaluate\n","    for X, y in data_loader:\n","        with torch.no_grad(): # we don't need gradients now\n","            ## # forward pass\n","            ## # loss calculation\n","    y_pred = np.where(a3[:, 0].numpy()>0.5, 1, 0)  # you can use this code\n","    accuracy = accuracy_score(y, y_pred)           # but try to understand what it does\n","    return loss, accuracy                          # and return the loss and the accuracy"],"metadata":{"id":"enyTC3OiWLoU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **1.4. Training**\n","\n","Instantiate your network and train it for 1000 epochs:"],"metadata":{"id":"yEucAuH46LKm"}},{"cell_type":"code","source":["network = SingleHiddenLayerNetwork(I, H1, H2, O)\n","optim = torch.optim.SGD(network.parameters(), lr=lr, momentum=0.9)\n","for i in range(1000):\n","    train_loss, train_accuracy = train(network, optim, train_loader)\n","    test_loss, test_accuracy = evaluate(network, test_loader)\n","\n","    if i % 100 == 0:\n","        print(\"Training Loss in epoch \"+str(i)+\": %1.2f\" % train_loss.item())\n","        print(\"Training accuracy in epoch \"+str(i)+\": %1.2f\" % train_accuracy)\n","        print(\"Test Loss in epoch \"+str(i)+\": %1.2f\" % test_loss.item())\n","        print(\"Test accuracy in epoch \"+str(i)+\": %1.2f\" % test_accuracy, \"\\n\")\n"],"metadata":{"id":"iBkACnTB6KjO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Use the code snippets provided to plot the training and set classified points of the half-moon."],"metadata":{"id":"pIFdPO9pA5CA"}},{"cell_type":"code","source":["network.eval()\n","with torch.no_grad():\n","    a_train = network(X_train)\n","    a_test = network(X_test)\n","print(\"Test set accuracy: \", accuracy_score(y_test, np.where(a_test[:, 0].numpy()>0.5, 1, 0)))\n","fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n","ax[0].scatter(X_train[:, 0], X_train[:, 1], c=np.where(a_train[:, 0].numpy()>0.5, 1, 0))\n","ax[1].scatter(X_test[:, 0], X_test[:, 1], c=np.where(a_test[:, 0].numpy()>0.5, 1, 0))\n","\n"],"metadata":{"id":"x_rpHldG7ZLy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cI6I6bPCZxdV"},"source":["<br>\n","\n","---\n","\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"DWzJXSJSZroz"},"source":["## 2. L2 regularisation on MNIST\n","\n","Add L2 regularisation to this morning's training with MNIST\n","\n","**HINT**: there is a way to do it that only involves changing one line of code, check the arguments of SGD [here](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)\n","\n","You can copy paste the code blocks you need from this morning's session."]},{"cell_type":"code","source":["# create a simple network\n","class simpleFFN(nn.Module):\n","  def __init__(self):\n","    super(simpleFFN, self).__init__()\n","    self.hidden_1 = nn.Linear(784, 200, bias=True)\n","    self.hidden_2 = nn.Linear(200, 50, bias=True)\n","    self.hidden_3 = nn.Linear(50,200, bias=True)\n","    self.output = nn.Linear(200, 10, bias=False)\n","    self.activation = nn.Mish()  ## Debbie loves this one ;-)\n","\n","  def forward(self, X):\n","    z1 = self.hidden_1(X)\n","    a1 = self.activation(z1)\n","    z2 = self.hidden_2(a1)\n","    a2 = self.activation(z2)\n","    z3 = self.hidden_3(a2)\n","    a3 = self.activation(z3)\n","    z4 = self.output(a3)\n","    a4 = self.activation(z4)\n","    return a4\n","\n","# test that it runs\n","x = torch.randn((1, 1, 784))\n","model = simpleFFN()\n","y = model(x)\n","print(y)\n","print(model)"],"metadata":{"id":"WNlT1-zQ05ox"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# download MNIST\n","mnist_train = MNIST(\"./\", download=True, train=True)\n","mnist_test = MNIST(\"./\", download=True, train=False)"],"metadata":{"id":"u9m0Kg3QEzxI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# split the data\n","shuffler = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42).split(mnist_train.data, mnist_train.targets)\n","indices = [(train_idx, validation_idx) for train_idx, validation_idx in shuffler][0]\n","\n","# define an standardisation function\n","def apply_standardization(X):\n","  X /= 255.\n","  X -= 0.1307\n","  X /= 0.3081\n","  return X\n","\n","# standardise the data\n","X_train, y_train = apply_standardization(mnist_train.data[indices[0]].float()), mnist_train.targets[indices[0]]\n","X_val, y_val = apply_standardization(mnist_train.data[indices[1]].float()), mnist_train.targets[indices[1]]\n","X_test, y_test =  apply_standardization(mnist_test.data.float()), mnist_test.targets"],"metadata":{"id":"HYqjkRTPE4tO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create the TensorDatasets containing mnist_train, mnist_validate, and mnist_test\n","mnist_train = TensorDataset(X_train, y_train.long())\n","mnist_validate = TensorDataset(X_val, y_val.long())\n","mnist_test = TensorDataset(X_test, y_test.long())"],"metadata":{"id":"l39A5Ml1FCft"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# implement training and validation functions\n","\n","def train(model, optimizer, criterion, data_loader):\n","    model.train()                         # the model is in the training mode so the parameters(weights)to be optimised will be updated\n","    train_loss, train_accuracy = 0, 0     # initialise loss and accuracy to 0 for training\n","    for X, y in data_loader:              # iterate over the mini-batches defined in the data loader\n","        X, y = X.to(device), y.to(device) # send data to the device (GPU in our case)\n","        optimizer.zero_grad()             # resetting optimiser info\n","        a2 = model(X.view(-1, 28*28))     # forward pass\n","        loss = criterion(a2, y)           # compute loss\n","        loss.backward()                   # backpropagation to calculate the gradients\n","        train_loss += loss*X.size(0)      # # add it up for different mini-batches and undo loss normalisation\n","        y_pred = F.log_softmax(a2, dim=1).max(1)[1]  # get predictions\n","        train_accuracy += accuracy_score(y.cpu().numpy(), y_pred.detach().cpu().numpy())*X.size(0) # compute accuracy\n","        optimizer.step()                  # perform a step of gradient descent\n","\n","    return train_loss/len(data_loader.dataset), train_accuracy/len(data_loader.dataset)  # here we can average over the whole dataset\n","\n","\n","def validate(model, criterion, data_loader):      # does not need optimiser\n","    model.eval()                                  # model is set to evaluation mode so no dropout or any other funny stuff here\n","    validation_loss, validation_accuracy = 0., 0. # initialise loss and accuracy to 0 for training\n","    for X, y in data_loader:                      # iterate over the mini-batches defined in the data loader\n","        with torch.no_grad():                     # deactivates autograd engine\n","            X, y = X.to(device), y.to(device)     # send data to the device (GPU in our case)\n","            a2 = model(X.view(-1, 28*28))         # forward pass\n","            loss = criterion(a2, y)               # evaluate loss\n","            validation_loss += loss*X.size(0)     # add it up for different mini-batches and undo loss normalisation\n","            y_pred = F.log_softmax(a2, dim=1).max(1)[1]  # get predictions\n","            validation_accuracy += accuracy_score(y.cpu().numpy(), y_pred.cpu().numpy())*X.size(0) # compute accuracy\n","\n","    return validation_loss/len(data_loader.dataset), validation_accuracy/len(data_loader.dataset)  # here we can average over the whole dataset"],"metadata":{"id":"KbWyTwASFDPv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# set hyperparameters\n","\n","seed = 42\n","lr = 1e-2\n","momentum = 0.9\n","batch_size = 64  # here batch_size really refers to what we have described as mini-batch\n","test_batch_size = 1000\n","n_epochs = 30\n","\n","set_seed(seed)\n","model = simpleFFN().to(device)                                              # instantiate model and send it to the GPU\n","optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)   # instantiate the optimizer\n","criterion = nn.CrossEntropyLoss()"],"metadata":{"id":"ovYwQ3ZBFOsj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We have defined a new parameter in the optimiser called `weight_decay`. This parameter is equivalent to adding L2 regularisation.\n","\n","Check the documentation of the `SGD` optimiser in `PyTorch` [here](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD)"],"metadata":{"id":"NdhZsz2qsNvl"}},{"cell_type":"code","source":["# create DataLoaders\n","\n","train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=0) ## num_workers=0 means that the main process will retrieve the data.\n","validation_loader = DataLoader(mnist_validate, batch_size=test_batch_size, shuffle=False, num_workers=0)\n","test_loader = DataLoader(mnist_test, batch_size=test_batch_size, shuffle=False, num_workers=0)"],"metadata":{"id":"Db_8ocXSFaQG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train for the number of epochs specified\n","\n","set_seed(seed)\n","liveloss = PlotLosses()    # plots evolution of loss and accuracy\n","for epoch in range(n_epochs):\n","    logs = {}\n","    train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n","\n","    logs['' + 'log loss'] = train_loss.item()\n","    logs['' + 'accuracy'] = train_accuracy\n","\n","    validation_loss, validation_accuracy = validate(model, criterion, validation_loader)\n","    logs['val_' + 'log loss'] = validation_loss.item()\n","    logs['val_' + 'accuracy'] = validation_accuracy\n","\n","    liveloss.update(logs)\n","    liveloss.draw()\n","    print(validation_loss.item())"],"metadata":{"id":"NPO7na2IFkNp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<br>\n","\n","---\n","\n","<br>\n"],"metadata":{"id":"sS9v-Ib_DVoZ"}},{"cell_type":"code","source":[],"metadata":{"id":"UR3ncx7otXXg"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}