{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/ese-ada-lovelace-2024/DL_Module_2024_Teaching/blob/main/Day05-CNNs(2)_and_Probability/Day05-CNNs(2)_morning_solutions.ipynb","timestamp":1732721812374}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1YLNtm8gNsviTEnVXzfiby2VMKrc0XzLP\" width=\"500\"/>\n","\n","---\n"],"metadata":{"id":"9YehS8enAmDn"}},{"cell_type":"markdown","source":["# **Recap (1)**\n","\n","#### **Morning contents/agenda**\n","\n","1. Overview of contents covered\n","\n","2. A design and training guide\n","\n","3. Key network operations\n","\n","4. U-Net\n","\n","5. Backpropagation\n","\n","<br>\n","\n","#### **Afternoon contents/agenda**\n","\n","1. Building a U-Net from scratch\n","\n","<br/>\n","\n","---\n","\n","<br/>"],"metadata":{"id":"9CY6juJtSkmD"}},{"cell_type":"markdown","source":["## 1. Overview of contents covered\n","\n","The basic ingredients of Deep Learning are what we covered during Week 1:\n","\n","1. Fully-connected/Dense/Feed-forward layers (you might also see these called Multi-Layer Perceptron or MLP)\n","2. Convolutional layers - always preferred to other types of layers when images are involved\n","3. Max Pooling layers\n","4. Transposed convolution layers (and other un-pooling methods)\n","\n","<br>\n","\n","To these, we add specific regularisation methods:\n","\n","1. Dropout\n","2. Batch normalisation\n","3. Layer normalisation\n","\n","<br>\n","\n","Every model is then composed using these operations, depending on the type of data used and the task being solved:\n","\n","1. Classification problems - usually solved with FFNs and CNNs\n","2. Domain-mapping problems (e.g. segmentation) - usually solved using architectures like U-Nets\n","3. Generative problems - which we will see in Week 2\n","4. Sequential-data processing/generation - which we will see in Week 3\n","\n","<br>\n","\n","---\n","\n","<br>"],"metadata":{"id":"CJnJKuttVH0d"}},{"cell_type":"markdown","source":["## 2. A design and training guide\n","\n","There are a huge number of hyperparameters in Deep Learning, and it can be daunting to know where to start to tune them appropriately. It is impossible to give you hard and fast rules to decide which parameters to tune, because they are always task and dataset dependent, and they largely require trial and error. However, I will try to give you some ideas to guide you in your designs.\n","\n","<br>\n","\n","### **Before starting**\n","\n","**Before** starting to design any networks or testing any code, we should always do the following:\n","\n","1. **Explore your dataset** to figure out the type of data you have, how it is organised, and how many data samples it contains. This is a good time to familiarise yourself with the dataset, look for outliers like corrupted images, and understand the statistical properties of your samples. Larger, more diverse datasets will require deeper, more feature-rich networks. Very small datasets might not be suitable for Deep Learning or might require using transfer learning.\n","\n","2. **Understand the problem** that you are trying to solve. What will be the inputs of your model? What will your model output? Or, what is the same, what type of problem is it: classification, regression, dimensionality reduction, generative modelling? Write these on a piece of paper even before attempting to write any code.\n","\n","3. **Research existing solutions** to similar tasks. I would recommend exploring blogposts, the latest literature, GitHub repositories, and examples/tutorials in PyTorch, Tensorflow, and Keras. Based on this research, you should find and keep notes of: 1) how have the authors prepared or processed their dataset? 2) what evaluation metrics have the authors used to measure model performance? 3) what model architecture (and hyperparameters in general) have the authors used? If possible, rank the models you decide to be suitable in levels of complexity.\n","\n","4. **Write the pipeline code** to: 1) load and prepare the dataset and the training, validation and test sets; 2) run the training, validation and test loops for any possible model; 3) prepare a series of metrics that you will use to evaluation your models (and that you will not use for training).\n","\n","5. **Establish a procedure to keep track of results**. You will likely be running many different tests with your models, so having a streamlined and reliable way to keep track of these will be critical. This could be by using a notebook, a spreadsheet or something more advanced like [Weights & Biases](https://wandb.ai/site/).\n","\n","<br>\n","\n","#### *Determining network inputs and outputs*\n","\n","Network inputs are determined by the task and architecture:\n","\n","1. FFN-based architectures require flat vectors as inputs `(batch, input_dim)`\n","2. CNN-based architectures require image-like inputs, including a channel dimension `(batch, channel, H, W)`\n","\n","<br>\n","\n","If the network deals with sequential data, an additional dimension will be required (we will see this in Week 3):\n","\n","1. For FFN-based architectures, including Transformers `(batch, seq_length, input_dim)`\n","2. For CNN-based architectures `(batch, seq_length, channel, H, W)`\n","\n","<br>\n","\n","Network outputs are determined by the specific task (we will see some of these in Week 2 and Week 3):\n","\n","1. For **classification tasks**, the output should be a vector of probabilities with as many dimensions as classes exist\n","\n","2. For **generative or regression tasks**, the output should have dimensions determined by the data type: `(batch, output_dim)` for vectorised data and `(batch, channel, H, W)` for image-like data\n","\n","3. For **discrete generative tasks**, such as text generation, the output should be a vector of probabilities with as many dimensions as possible discrete outputs exist (possible words in text generation)\n","\n","4. A `seq_length` dimension can be added for **sequential outputs**: `(batch, seq_length, output_dim)` for vectorised data and `(batch, channel, seq_length, H, W)` for image-like data\n","\n","<br>\n","\n","<center>\n","<img src=\"https://drive.google.com/uc?id=1nisb0AaclwvxsyFDWvrV6U89Z_9sBeIZ\" width=\"800\"/>\n","</center>\n","\n","<br><br>\n","\n","\n","### **Bootstrapping the design**\n","\n","There are a number of choices to make in order to bootstrap your design:\n","\n","1. The first step is always to **choose a base model** from which to work. To do this, use the research that you have conducted before. **Always start with the simplest model** before trying the latest state-of-the-art (even if that simple model is a CNN with 3 layers). Once the first model on the list works, you can then start going down your ranking to understand how each of the models performs. This way you will have a baseline against which to benchmark any improvements you make.\n","\n","2. To **select an optimiser**, start with a simple, popular optimiser: I would always recommed starting with Adam with default parameters. If the paper you are reproducing has used a different optimiser, you might also want to try the optimiser they suggest with the parameters that they recommend.\n","\n","3. The **loss function** can be initially chosen based on the problem you're trying to solve: MSE for regression problems and Cross-Entropy for classification problems. If the paper you are reproducing suggests a different loss function, and this loss function is already implemented or easily implemented, you can also start using that instead.\n","\n","4. The **batch size** should often be chosen as the largest batch size supported by the available hardware. We generally choose a batch size in the range 32-512 as a general rule of thumb. However, large batch sizes can also produce more unstable training loops as suggested [here](https://arxiv.org/abs/1804.07612), so you should watch out for that. The batch size governs the training speed and shouldn't be used to directly tune the validation set performance.\n","\n","5. The **learning rate** is critical to training the model. A very small learning rate can be seen through slow progression in the loss over epochs. A very large learning rate can be seen when the loss function becomes unstable, erratic, or it diverges. A reasonable procedure to find a learning rate is to start with a small number like `1e-6` and progressively increase the rate by multiplying it by `10` until the training diverges. You can use the last learning rate value before it started diverging.\n","\n","6. Choose the **number of epochs** by balancing starting to see some convergence of the training with computational efficiency. You will likely be running a significant amount of tests. Therefore, you don't wont each test to take hours to run. Always try to fail fast and leave the long training until after you have found a good set of hyperparameters. We often start with epochs in the range 5-100 depending on the problem.\n","\n","<br>\n","\n","### **Testing your setup**\n","\n","Once you have bootstrapped your design, I suggest that you thoroughly test it before proceding with the actual training or hyperparameter optimisation.\n","\n","To do this, I recommend that you take one single batch from your dataset and try to overfit your model to this batch only. If you cannot overfit to this single batch, you should troubleshoot your model and code, as it is very likely that you have made a mistake somewhere (even if you don't get an error message).\n","\n","Use this chance to also print the shape of every output of every significant operation to ensure they are as expected.\n","\n","I also recommend extensive plotting across your training, validation and testing procedures to ensure that they make sense. Plot everything: the images in your dataset, the batches in your dataloader, the outputs of your network, the gradients in your layers, the filters in your `Conv2d`s, etc. And familiarise yourself with every plot that you do and what they mean.\n","\n","At this point, you can introduce the full dataset (or a larger portion of it) and train your model.\n","\n","<br>"],"metadata":{"id":"jNciq44i45fz"}},{"cell_type":"markdown","source":["### **Fine tune your design**\n","\n","Once you have a baseline run, we will start exploring the behaviour of the model as it responds to different hyperparameters and fine tuning its performance. We will do this by devising experiments and always following concrete evidence.\n","\n","We could try to use an automated algorithm to explore the whole space of possible hyperparameters. Unfortunately, that is not possible in Deep Learning: there are way too many hyperparameters over too large a range to do this effectively.\n","\n","Instead, we will use well-defined sets of experiments to understand what hyperparameters are important and what ranges for those hyperparameters are relevant for our problem. We can later use this information to run automated hyperparameter tuning (if time allows).\n","\n","Our focus during the experiments, especially at first, should **not** be on obtaining the best possible performance, but to explore and understand the importance and effect of different hyperparameters for our specific problem. We need to develop an intuition for our problem, and running these experiments will allow us to do this.\n","\n","Our procedure should then be:\n","\n","1. Look the current results of our model: this includes training curves, performance metrics, and actual quality of the outputs (for example, by looking plotting output images of our network).\n","\n","2. Identify a hypothesis for the next round of experiments based on the evidence provided by the current results of our model. For example, a hypothesis could be that the network is overfitting.\n","\n","2. Design a set of experiments that are determined by the hypothesis. The experiments should have a clear goal and be sufficiently narrow in scope: if we try to add multiple features, test multiple hyperparameters or answer multiple questions at once, we may not be able to disentangle the separate effects on the results. For example, if our hypothesis is that the network is overfitting, we could design experiments to explore the impact of a regulariser like BatchNorm.\n","\n","3. Use the results of the model after each experiment to learn as much as possible about your hypothesis and the impact of different hyperparameters. Make sure to keep track of your experiments, including notes of what you learn from them. You will likely want to revisit these notes later on.\n","\n","4. Consider whether to update your current network design to a new best configuration.\n","\n","5. Go back to step 1. Repeat this process as much as needed or for as long as you can.\n","\n","<br>\n","\n","At this point, you can perform a formal hyperparameter tuning (using grid or random search) for those parameters that you feel will have the largest impact on the results.\n","\n","Finally, you can use this final set of hyperparameters to train your model for a larger number of epochs. The total number of epochs will be determined by the observed loss and accuracy: you want to train for as long as these values keep improving both in the training and validation sets. You also want to ensure that you check the outputs of your network often to see if you can observe the improvement.\n","\n","<br><br>\n","\n","Here are some examples of loss and accuracy outputs you might obtain, and what hypotheses you might extract from them:\n","\n","<br>\n","\n","<p align = \"center\"><img src=\"https://drive.google.com/uc?id=14OxdJTla4FMPS7pe-seG8JVsaETU0oyW\" width=\"600\"/></p><p align = \"center\">\n","<i>The model seems to be training but with instability, its a risky training. We could address this by using a smaller learning rate, changing the batch size, or adding some regularisation.</i>\n","</p>\n","\n","<br>\n","\n","<p align = \"center\"><img src=\"https://drive.google.com/uc?id=1-ZGoWxDpIWGs7YcK3ZeljFHrbfTu-ho6\" width=\"600\"/></p><p align = \"center\">\n","<i>The model does not have enough capacity and is underfitting.We could address this by increasing the number of epochs, trying to increase the learning rate, or increasing the model capacity.</i>\n","</p>\n","\n","<br>\n","\n","<p align = \"center\"><img src=\"https://drive.google.com/uc?id=1pYHiHoOr-HDs6GLIQovkC4zvCAmkYvrS\" width=\"600\"/></p><p align = \"center\">\n","<i>The model has too much flexibility and is overfitting. We could address this by adding regularisers, or decreasing model capacity.</i>\n","</p>\n","\n","<br><br>\n","\n","Here are some things to consider in this process:\n","\n","- If underfitting, you should increase the complexity of the network. Usually, you will get more of a performance boost from adding more layers than adding more neurons/channels in each layer.\n","- If overfitting, you should start by using regularisation approaches: Dropout, BatchNorm, LayerNorm, data augmentation, and explicit regularisers. If none of these are enough, you should reduce the complexity of your network.\n","- Overfit first and then regularize. I would recommend starting with a model large enough that it can overfit (i.e. focus on training loss) and then regularize it appropriately (give up some training loss to improve the validation loss).\n","- At this stage, the loss function is determined by the problem being solved. You should use MSE for regression-like problems and cross-entropy for classification problems (with added KL terms in the case of VAEs). Once you feel comfortable with designing networks, you can explore other types of loss functions.\n","- I would recommend considering [learning-rate scheduling](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate), but only later on in the training process.\n","- Adam is a great optimiser, so I would use that as a default.\n","- If you are struggling to diagnose what is causing your model to perform poorly, I recommend accessing the gradients of every layer in your network and plotting their norm over the training process. This will help you understand if your network is suffering from vanishing and exploding gradients. If you are suffering from either of the two, you should consider: (1) change your activation functions to a flavour of ReLU; (2) add BatchNorm; (3) add skip connections; (4) although not covered, gradient clipping is another possible approach.\n","- Only test one change at a time!\n","\n","<br><br>\n","\n","For a more in-depth discussion, I would recommend this [tuning playbook](https://github.com/google-research/tuning_playbook). If you want to get a pro's take on this process, [this blog](https://karpathy.github.io/2019/04/25/recipe/) is very good.\n","\n","<br>\n","\n","\n","### **PyTorch implementation steps**\n","\n","Here is a high-level overview of the steps that you always need to take:\n","\n","1. Create a `Dataset` object(s) that will contain your complete dataset (either a custom one by creating a class that inherits from Dataset or one extracted directly from torch or torchvision).\n","2. Create a `Dataloader` object(s) to assemble batches from the Dataset and send them to the device. At this point, you might want to use a `StratifiedShuffleSplit` to separate between validation and training loaders.\n","3. Create a model, either by using a new `nn.Module` class or by downloading a predefined network like Alexnet.\n","4. Instantiate an appropriate criterion like MSE or CrossEntropy.\n","5. Instantiate an optimizer like SGD or Adam.\n","6. Create the train, validation, and test loops.\n","7. Run over all epochs using alternatively train and validation.\n","8. Run the test loop on the trained model.\n","\n","<br>\n","\n","The train loop always has the same steps as well:\n","\n","1. Set the model to `model.train()` to ensure operations like dropout are correctly set up.\n","2. Iterate through every batch.\n","3. Send the batch to the device.\n","4. Zero out the gradients by calling `optimiser.zero_grad()`.\n","5. Apply the model to the batch.\n","6. Compare the output of the model and the expected target using the criterion.\n","7. Calculate any other relevant metrics like accuracy.\n","8. Calculate the gradients using backpropagation by calling `loss.backward()`.\n","9. Update the parameters with the gradient using `optimizer.step()`.\n","\n","<br>\n","\n","Validation and testing follow a similar procedure:\n","\n","1. Set the model to `model.eval()` to ensure operations like dropout are correctly set up.\n","2. Iterate through every batch.\n","3. Send the batch to the device.\n","4. Apply the model to the batch.\n","5. Compare the output of the model and the expected target using the criterion if there is a target available.\n","6. Calculate any other relevant metrics like accuracy.\n","7. For testing, save the outputs of the model for verification.\n","\n","<br>\n","\n","---\n","\n","<br>"],"metadata":{"id":"UZBdUWVUfPYU"}},{"cell_type":"markdown","source":["## 3. Key network operations\n","\n","### **Linear layers**\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1AtQhgLLuGKiLi8neVLNo1xQ91rfW7XLj\" width=\"800\"/></center>\n","\n","<img src=\"https://drive.google.com/uc?id=1rAh2U6ejO54rptSXbHTYqiTYg78mvOTe\" width=\"800\"/>\n","\n","<br>\n","\n","### **Convolutional layers**\n","\n","<center><img src=\"https://drive.google.com/uc?id=1ME4bFACe5hE9pSYyIvclE1lE542YbEL3\" width=\"800\"/></center>\n","\n","<br>\n","\n","<p align = \"center\"><img src=\"https://drive.google.com/uc?id=1jyD_4d3HvulHQ5obeKJ57ANt6-U66din\" width=\"800\"/></p><p align = \"center\">\n","<i>3-channel input (RGB image) and 1-channel output example</i>\n","</p>\n","\n","<br>\n","\n","<p align = \"center\"><img src=\"https://drive.google.com/uc?id=1400AvvRTZkRH06-13XHDR19GILqBxwTP\" width=\"800\"/></p><p align = \"center\">\n","<i>3-channel input (RGB image) and 2-channel output example</i>\n","</p>\n","\n","<br>\n","\n","### **Max pooling**\n","\n","<img src=\"https://benmoseley.blog/uploads/teaching/2024-ESE-DL/images/slides/Slide15.png\" width=\"600\"/>\n","\n","<br>\n","\n","### **Transposed convolutional layers**\n","\n","Transposed convolutions can be computed by following an easy recipe:\n","\n","<center><img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*54-7typHLLXhdvAhlku9SQ.png\" width=\"900\"/></center>\n","\n","\n","where we know that:\n","- `s`: stride\n","- `p`: padding\n","- `k`: kernel size\n","\n","and we use this hyperparameters to calculate:\n","- `z`: how many zeros to insert in between pixels of my input\n","- `p'`: how much padding do I add around the image\n","\n","But with the added caveat that, **as the name indicates**, we need to **transpose the kernel** before using it to convolve with the input. Transposing the kernel in this case implies flipping the kernel along each of its axes.\n","\n","<br>\n","\n","### **Activation functions**\n","\n","Historically, the three most commonly used are:\n","\n","- tanh\n","- sigmoid\n","- ReLU (Rectified Linear Unit)\n","\n","\n","<img src=\"https://miro.medium.com/max/1190/1*f9erByySVjTjohfFdNkJYQ.jpeg\" width=\"210\"/>\n","\n","<img src=\"https://miro.medium.com/max/4800/1*XxxiA0jJvPrHEJHD4z893g.png\" width=\"400\"/>\n","\n","But over time, many more have been developed:\n","\n","<center><img src=\"https://drive.google.com/uc?id=1H6XZ-jCIbXZpff00CwqL0TcXbW57nlGb\" width=\"800\"/></center>\n","\n","A special case of activation functions are those involving outputs from several neurons in each layer:\n","\n","<center><img src=\"https://drive.google.com/uc?id=1xjF1HkWG3pQhZ0OFCmiHCPUDMSrwaji3\" width=\"400\"/></center>\n","\n","[Link to wikipedia entry](https://en.wikipedia.org/wiki/Activation_function) about activation functions.\n","\n","\n","<br>\n","\n","### **Skip connections**\n","\n","<img src=\"https://www.researchgate.net/publication/348555917/figure/fig4/AS:991956234694659@1613512204672/Conceptualized-architecture-of-the-skip-connection-51-The-sub-figure-on-the-left-shows.ppm\" width=\"600\"/>\n","\n","<br><br>\n","\n","---\n","\n","<br>"],"metadata":{"id":"w8lkqosQu1iW"}},{"cell_type":"markdown","source":["## 4. U-Net\n","\n","<p align = \"center\"><img src=\"https://drive.google.com/uc?id=1glxl06_zsq-2Off0E21VOQmG5k22R6_z\" width=\"800\"/></p><p align = \"center\">\n","<i> sources: <a href=\"https://arxiv.org/pdf/1505.04597.pdf\">original unet</a>, <a href=\"https://www.kaggle.com/c/tgs-salt-identification-challenge\"> seismic segmentation</a></i>\n","</p>\n","\n","<br>\n","\n","---\n","\n","<br>"],"metadata":{"id":"rPEqPDJuxTIN"}},{"cell_type":"markdown","source":["## 5. Backpropagation\n","\n","<center><p>Step 1</p><img src=\"https://drive.google.com/uc?id=1OO0M9ZBPHle0XwMsGiYMmspe17Lb2T-4\" width=\"800\"/></center>\n","\n","<br>\n","\n","Here it is worth noting that $a_4$ depends on all the model parameters $w_i,b_i$.\n","\n","<br>\n","\n","<center><p>Step 2</p><img src=\"https://drive.google.com/uc?id=1GdVX-e8Jn70is2m2j06SFdbz2EYR2CLa\" width=\"800\"/></center>\n","\n","<br>\n","\n","where here we add a $\\frac{1}{2}$ in front of the loss to simplify calculations:\n","\n","$$\\require{cancel}$$\n","$$\\frac{\\partial C}{\\partial a_4} = \\frac{1}{\\cancel{2}} \\cancel{2} (a_4 - y)$$\n","\n","and then we continue with the chain rule:\n","\n","<br>\n","\n","<center><p>Step 3</p><img src=\"https://drive.google.com/uc?id=13SS5XgL-BXsEy37gsbsP_PvxO_bD6mi7\" width=\"800\"/></center>\n","\n","<br>\n","\n","---\n","\n","<br>"],"metadata":{"id":"1qo5pnE5wKa4"}},{"cell_type":"code","source":[],"metadata":{"id":"pGw7UWv98vcG"},"execution_count":null,"outputs":[]}]}